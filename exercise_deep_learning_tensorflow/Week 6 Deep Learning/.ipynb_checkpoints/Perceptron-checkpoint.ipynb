{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2455f690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "# Adapted from the code on https://www.tensorflow.org/tutorials/images/cnn \n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models, regularizers\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fcced03",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd558fc",
   "metadata": {},
   "source": [
    "1- Multi Layer Perceptron (MLP). Modify the provided script 'perceptron.py' to build a MLP. Use architectures \n",
    "with 0, 1 and 2 hidden layers. Keep the complexity of the model bounded so runs do not take much more\n",
    "than 1 hour to reach the maximum of testing accuracy. Notice that the input needs to be \"flattened\" since there is no spatial structure \n",
    "in this fully connected design.  This can be achieved by adding a dummy layer with no free parameters with \"layers.Flatten()\"\n",
    "as the first layer in the constructor \"model.Sequential()\". Obtain the learning curves and discuss the results.\n",
    "Report the optimizer in use, initialization parameters, the learning rate, etc. Is early stopping convenient\n",
    "in this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c908543d",
   "metadata": {},
   "source": [
    "YOUR ANWSER HERE: \n",
    "\n",
    "* item Optimizer: Adam \n",
    "* item Initialization parameters: Epochs = 50\n",
    "* item Learning rate: Defaults to 0.001, this learning rate however is tweaked by the adam optimizer.\n",
    "\n",
    "Is early stopping convenientin this model? </br>\n",
    "In the given example model early stopping has no use, because we get a results that is quite bad already and early stopping it would only worsen our result. we should only stop early if we get within a certain very low loss range, which we clearly aren't in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6cb1d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original\n",
    "model1 = models.Sequential([\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(10, activation='softmax') # end with softmax for classification\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3daa84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e7e110a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.9633 - accuracy: 0.3154 - val_loss: 1.9403 - val_accuracy: 0.3495\n",
      "Epoch 2/50\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 1.8807 - accuracy: 0.3516 - val_loss: 1.9325 - val_accuracy: 0.3309\n",
      "Epoch 3/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.8549 - accuracy: 0.3633 - val_loss: 2.0048 - val_accuracy: 0.3578\n",
      "Epoch 4/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.8515 - accuracy: 0.3685 - val_loss: 1.9024 - val_accuracy: 0.3535\n",
      "Epoch 5/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.8280 - accuracy: 0.3735 - val_loss: 1.8070 - val_accuracy: 0.3724\n",
      "Epoch 6/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.8183 - accuracy: 0.3788 - val_loss: 1.9229 - val_accuracy: 0.3435\n",
      "Epoch 7/50\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.8233 - accuracy: 0.3765 - val_loss: 1.8950 - val_accuracy: 0.3565\n",
      "Epoch 8/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.8104 - accuracy: 0.3830 - val_loss: 1.9101 - val_accuracy: 0.3538\n",
      "Epoch 9/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7967 - accuracy: 0.3877 - val_loss: 1.8916 - val_accuracy: 0.3546\n",
      "Epoch 10/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.8111 - accuracy: 0.3849 - val_loss: 1.8683 - val_accuracy: 0.3606\n",
      "Epoch 11/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7937 - accuracy: 0.3867 - val_loss: 1.9026 - val_accuracy: 0.3504\n",
      "Epoch 12/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7876 - accuracy: 0.3904 - val_loss: 2.0165 - val_accuracy: 0.3229\n",
      "Epoch 13/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7917 - accuracy: 0.3884 - val_loss: 1.9265 - val_accuracy: 0.3380\n",
      "Epoch 14/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7865 - accuracy: 0.3900 - val_loss: 1.9771 - val_accuracy: 0.3254\n",
      "Epoch 15/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7912 - accuracy: 0.3923 - val_loss: 1.8446 - val_accuracy: 0.3614\n",
      "Epoch 16/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7809 - accuracy: 0.3911 - val_loss: 2.0319 - val_accuracy: 0.3277\n",
      "Epoch 17/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7909 - accuracy: 0.3927 - val_loss: 1.9066 - val_accuracy: 0.3425\n",
      "Epoch 18/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7841 - accuracy: 0.3967 - val_loss: 1.7855 - val_accuracy: 0.3748\n",
      "Epoch 19/50\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 1.7771 - accuracy: 0.3956 - val_loss: 1.7767 - val_accuracy: 0.3852\n",
      "Epoch 20/50\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 1.7743 - accuracy: 0.3992 - val_loss: 1.8297 - val_accuracy: 0.3670\n",
      "Epoch 21/50\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 1.7683 - accuracy: 0.4005 - val_loss: 1.9699 - val_accuracy: 0.3389\n",
      "Epoch 22/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7743 - accuracy: 0.3976 - val_loss: 1.9668 - val_accuracy: 0.3446\n",
      "Epoch 23/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7704 - accuracy: 0.3988 - val_loss: 1.8968 - val_accuracy: 0.3589\n",
      "Epoch 24/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7679 - accuracy: 0.3979 - val_loss: 1.9194 - val_accuracy: 0.3432\n",
      "Epoch 25/50\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 1.7646 - accuracy: 0.3998 - val_loss: 1.8932 - val_accuracy: 0.3666\n",
      "Epoch 26/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7587 - accuracy: 0.4015 - val_loss: 2.0045 - val_accuracy: 0.3326\n",
      "Epoch 27/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7587 - accuracy: 0.4009 - val_loss: 1.8997 - val_accuracy: 0.3526\n",
      "Epoch 28/50\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 1.7631 - accuracy: 0.4025 - val_loss: 1.8834 - val_accuracy: 0.3665\n",
      "Epoch 29/50\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 1.7601 - accuracy: 0.4021 - val_loss: 2.1223 - val_accuracy: 0.3201\n",
      "Epoch 30/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7586 - accuracy: 0.4029 - val_loss: 1.8970 - val_accuracy: 0.3630\n",
      "Epoch 31/50\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 1.7548 - accuracy: 0.4042 - val_loss: 1.8985 - val_accuracy: 0.3516\n",
      "Epoch 32/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7589 - accuracy: 0.4026 - val_loss: 1.8294 - val_accuracy: 0.3756\n",
      "Epoch 33/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7485 - accuracy: 0.4052 - val_loss: 1.9364 - val_accuracy: 0.3560\n",
      "Epoch 34/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7534 - accuracy: 0.4051 - val_loss: 1.8671 - val_accuracy: 0.3607\n",
      "Epoch 35/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7481 - accuracy: 0.4022 - val_loss: 2.0531 - val_accuracy: 0.3251\n",
      "Epoch 36/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7562 - accuracy: 0.4073 - val_loss: 1.9111 - val_accuracy: 0.3538\n",
      "Epoch 37/50\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.7478 - accuracy: 0.4076 - val_loss: 1.8690 - val_accuracy: 0.3571\n",
      "Epoch 38/50\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.7518 - accuracy: 0.4035 - val_loss: 1.8609 - val_accuracy: 0.3613\n",
      "Epoch 39/50\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.7559 - accuracy: 0.4046 - val_loss: 2.0041 - val_accuracy: 0.3301\n",
      "Epoch 40/50\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.7460 - accuracy: 0.4085 - val_loss: 1.9644 - val_accuracy: 0.3454\n",
      "Epoch 41/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7598 - accuracy: 0.4036 - val_loss: 1.8763 - val_accuracy: 0.3565\n",
      "Epoch 42/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7430 - accuracy: 0.4088 - val_loss: 1.8619 - val_accuracy: 0.3594\n",
      "Epoch 43/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7472 - accuracy: 0.4074 - val_loss: 2.0144 - val_accuracy: 0.3468\n",
      "Epoch 44/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7459 - accuracy: 0.4071 - val_loss: 1.8718 - val_accuracy: 0.3609\n",
      "Epoch 45/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7427 - accuracy: 0.4080 - val_loss: 1.9017 - val_accuracy: 0.3543\n",
      "Epoch 46/50\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 1.7428 - accuracy: 0.4067 - val_loss: 1.8509 - val_accuracy: 0.3637\n",
      "Epoch 47/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7463 - accuracy: 0.4091 - val_loss: 1.9570 - val_accuracy: 0.3402\n",
      "Epoch 48/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7470 - accuracy: 0.4076 - val_loss: 1.8238 - val_accuracy: 0.3775\n",
      "Epoch 49/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7461 - accuracy: 0.4083 - val_loss: 1.8717 - val_accuracy: 0.3690\n",
      "Epoch 50/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7477 - accuracy: 0.4083 - val_loss: 1.9488 - val_accuracy: 0.3320\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.9488 - accuracy: 0.3320\n"
     ]
    }
   ],
   "source": [
    "history1 = model1.fit(train_images, train_labels, epochs=50,\n",
    "                    validation_data=(test_images, test_labels))\n",
    "\n",
    "test_loss1, test_acc1 = model1.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c510e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Do this with 0,1,2 Hidden layers\n",
    "model2 = models.Sequential([\n",
    "    layers.Flatten(), # Input layer doesn't count\n",
    "    layers.Dense(512, activation='relu'), # 1 Hidden layer\n",
    "    layers.Dense(10, activation='softmax') # end with softmax for classification also the final layer isn't part of a hidden layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9ea3b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4bb5b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.8858 - accuracy: 0.3276 - val_loss: 1.7161 - val_accuracy: 0.3785\n",
      "Epoch 2/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.7052 - accuracy: 0.3891 - val_loss: 1.6681 - val_accuracy: 0.4013\n",
      "Epoch 3/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.6402 - accuracy: 0.4179 - val_loss: 1.6043 - val_accuracy: 0.4310\n",
      "Epoch 4/50\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.5904 - accuracy: 0.4363 - val_loss: 1.5902 - val_accuracy: 0.4298\n",
      "Epoch 5/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.5622 - accuracy: 0.4455 - val_loss: 1.5727 - val_accuracy: 0.4370\n",
      "Epoch 6/50\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.5382 - accuracy: 0.4551 - val_loss: 1.5379 - val_accuracy: 0.4570\n",
      "Epoch 7/50\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.5164 - accuracy: 0.4613 - val_loss: 1.5500 - val_accuracy: 0.4499\n",
      "Epoch 8/50\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 1.5007 - accuracy: 0.4666 - val_loss: 1.5441 - val_accuracy: 0.4513\n",
      "Epoch 9/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.4879 - accuracy: 0.4748 - val_loss: 1.5516 - val_accuracy: 0.4517\n",
      "Epoch 10/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.4725 - accuracy: 0.4761 - val_loss: 1.5216 - val_accuracy: 0.4537\n",
      "Epoch 11/50\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 1.4696 - accuracy: 0.4773 - val_loss: 1.5319 - val_accuracy: 0.4583\n",
      "Epoch 12/50\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 1.4519 - accuracy: 0.4838 - val_loss: 1.5581 - val_accuracy: 0.4571\n",
      "Epoch 13/50\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.4489 - accuracy: 0.4855 - val_loss: 1.5229 - val_accuracy: 0.4591\n",
      "Epoch 14/50\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 1.4361 - accuracy: 0.4892 - val_loss: 1.4967 - val_accuracy: 0.4753\n",
      "Epoch 15/50\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.4285 - accuracy: 0.4929 - val_loss: 1.4941 - val_accuracy: 0.4668\n",
      "Epoch 16/50\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.4208 - accuracy: 0.4941 - val_loss: 1.4705 - val_accuracy: 0.4863\n",
      "Epoch 17/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.4171 - accuracy: 0.4958 - val_loss: 1.4798 - val_accuracy: 0.4770\n",
      "Epoch 18/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.4077 - accuracy: 0.5000 - val_loss: 1.5272 - val_accuracy: 0.4688\n",
      "Epoch 19/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.4004 - accuracy: 0.5019 - val_loss: 1.4936 - val_accuracy: 0.4672\n",
      "Epoch 20/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3969 - accuracy: 0.5043 - val_loss: 1.5013 - val_accuracy: 0.4751\n",
      "Epoch 21/50\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 1.3938 - accuracy: 0.5030 - val_loss: 1.4892 - val_accuracy: 0.4762\n",
      "Epoch 22/50\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3890 - accuracy: 0.5045 - val_loss: 1.5129 - val_accuracy: 0.4665\n",
      "Epoch 23/50\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 1.3823 - accuracy: 0.5087 - val_loss: 1.5016 - val_accuracy: 0.4711\n",
      "Epoch 24/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3752 - accuracy: 0.5114 - val_loss: 1.4769 - val_accuracy: 0.4835\n",
      "Epoch 25/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3702 - accuracy: 0.5112 - val_loss: 1.4740 - val_accuracy: 0.4780\n",
      "Epoch 26/50\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3728 - accuracy: 0.5124 - val_loss: 1.5024 - val_accuracy: 0.4671\n",
      "Epoch 27/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3634 - accuracy: 0.5156 - val_loss: 1.4966 - val_accuracy: 0.4765\n",
      "Epoch 28/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3609 - accuracy: 0.5154 - val_loss: 1.5060 - val_accuracy: 0.4704\n",
      "Epoch 29/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3553 - accuracy: 0.5187 - val_loss: 1.4812 - val_accuracy: 0.4782\n",
      "Epoch 30/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3516 - accuracy: 0.5196 - val_loss: 1.5311 - val_accuracy: 0.4667\n",
      "Epoch 31/50\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3473 - accuracy: 0.5229 - val_loss: 1.4850 - val_accuracy: 0.4825\n",
      "Epoch 32/50\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 1.3442 - accuracy: 0.5221 - val_loss: 1.5150 - val_accuracy: 0.4644\n",
      "Epoch 33/50\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.3417 - accuracy: 0.5229 - val_loss: 1.4945 - val_accuracy: 0.4681\n",
      "Epoch 34/50\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 1.3369 - accuracy: 0.5230 - val_loss: 1.4739 - val_accuracy: 0.4810\n",
      "Epoch 35/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3321 - accuracy: 0.5238 - val_loss: 1.4849 - val_accuracy: 0.4774\n",
      "Epoch 36/50\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 1.3363 - accuracy: 0.5229 - val_loss: 1.5234 - val_accuracy: 0.4654\n",
      "Epoch 37/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3282 - accuracy: 0.5258 - val_loss: 1.4833 - val_accuracy: 0.4749\n",
      "Epoch 38/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3248 - accuracy: 0.5302 - val_loss: 1.4946 - val_accuracy: 0.4725\n",
      "Epoch 39/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3210 - accuracy: 0.5288 - val_loss: 1.5195 - val_accuracy: 0.4702\n",
      "Epoch 40/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3165 - accuracy: 0.5321 - val_loss: 1.5222 - val_accuracy: 0.4701\n",
      "Epoch 41/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3144 - accuracy: 0.5317 - val_loss: 1.4894 - val_accuracy: 0.4776\n",
      "Epoch 42/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3171 - accuracy: 0.5289 - val_loss: 1.5031 - val_accuracy: 0.4728\n",
      "Epoch 43/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3098 - accuracy: 0.5336 - val_loss: 1.5044 - val_accuracy: 0.4761\n",
      "Epoch 44/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3118 - accuracy: 0.5323 - val_loss: 1.5164 - val_accuracy: 0.4724\n",
      "Epoch 45/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3034 - accuracy: 0.5338 - val_loss: 1.5013 - val_accuracy: 0.4744\n",
      "Epoch 46/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3006 - accuracy: 0.5362 - val_loss: 1.4854 - val_accuracy: 0.4749\n",
      "Epoch 47/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3043 - accuracy: 0.5361 - val_loss: 1.5531 - val_accuracy: 0.4619\n",
      "Epoch 48/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.2981 - accuracy: 0.5367 - val_loss: 1.5189 - val_accuracy: 0.4722\n",
      "Epoch 49/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.2931 - accuracy: 0.5391 - val_loss: 1.5237 - val_accuracy: 0.4693\n",
      "Epoch 50/50\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2952 - accuracy: 0.5357 - val_loss: 1.5278 - val_accuracy: 0.4737\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 1.5278 - accuracy: 0.4737\n"
     ]
    }
   ],
   "source": [
    "history2 = model2.fit(train_images, train_labels, epochs=50,\n",
    "                    validation_data=(test_images, test_labels))\n",
    "\n",
    "test_loss2, test_acc2 = model2.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39efca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = models.Sequential([\n",
    "    layers.Flatten(), # Input layer doesn't count\n",
    "    layers.Dense(512, activation='relu'), # Hidden layer 1\n",
    "    layers.Dense(256, activation='relu'), # Hidden layer 2\n",
    "    layers.Dense(10, activation='softmax') # end with softmax for classification also the final layer isn't part of a hidden layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3edc6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0339ea9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1563/1563 [==============================] - 14s 8ms/step - loss: 1.8565 - accuracy: 0.3307 - val_loss: 1.7119 - val_accuracy: 0.3808\n",
      "Epoch 2/50\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.6696 - accuracy: 0.4035 - val_loss: 1.6338 - val_accuracy: 0.4186\n",
      "Epoch 3/50\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.5896 - accuracy: 0.4336 - val_loss: 1.5865 - val_accuracy: 0.4352\n",
      "Epoch 4/50\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.5400 - accuracy: 0.4492 - val_loss: 1.5165 - val_accuracy: 0.4566\n",
      "Epoch 5/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.5016 - accuracy: 0.4627 - val_loss: 1.5279 - val_accuracy: 0.4545\n",
      "Epoch 6/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.4734 - accuracy: 0.4741 - val_loss: 1.4916 - val_accuracy: 0.4682\n",
      "Epoch 7/50\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.4389 - accuracy: 0.4849 - val_loss: 1.5019 - val_accuracy: 0.4693\n",
      "Epoch 8/50\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.4127 - accuracy: 0.4938 - val_loss: 1.5061 - val_accuracy: 0.4643\n",
      "Epoch 9/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3989 - accuracy: 0.4997 - val_loss: 1.4653 - val_accuracy: 0.4782\n",
      "Epoch 10/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3804 - accuracy: 0.5054 - val_loss: 1.4377 - val_accuracy: 0.4887\n",
      "Epoch 11/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3621 - accuracy: 0.5136 - val_loss: 1.4496 - val_accuracy: 0.4849\n",
      "Epoch 12/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3425 - accuracy: 0.5173 - val_loss: 1.4525 - val_accuracy: 0.4800\n",
      "Epoch 13/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3307 - accuracy: 0.5244 - val_loss: 1.4869 - val_accuracy: 0.4769\n",
      "Epoch 14/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3192 - accuracy: 0.5258 - val_loss: 1.4638 - val_accuracy: 0.4790\n",
      "Epoch 15/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3092 - accuracy: 0.5308 - val_loss: 1.4595 - val_accuracy: 0.4862\n",
      "Epoch 16/50\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2947 - accuracy: 0.5364 - val_loss: 1.4584 - val_accuracy: 0.4975\n",
      "Epoch 17/50\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2886 - accuracy: 0.5376 - val_loss: 1.4502 - val_accuracy: 0.4961\n",
      "Epoch 18/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.2768 - accuracy: 0.5425 - val_loss: 1.4359 - val_accuracy: 0.4971\n",
      "Epoch 19/50\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2688 - accuracy: 0.5450 - val_loss: 1.4475 - val_accuracy: 0.4964\n",
      "Epoch 20/50\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2598 - accuracy: 0.5471 - val_loss: 1.4458 - val_accuracy: 0.4944\n",
      "Epoch 21/50\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 1.2485 - accuracy: 0.5527 - val_loss: 1.4472 - val_accuracy: 0.4996\n",
      "Epoch 22/50\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2431 - accuracy: 0.5530 - val_loss: 1.4546 - val_accuracy: 0.4931\n",
      "Epoch 23/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.2305 - accuracy: 0.5593 - val_loss: 1.4550 - val_accuracy: 0.4983\n",
      "Epoch 24/50\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 1.2255 - accuracy: 0.5607 - val_loss: 1.4627 - val_accuracy: 0.4920\n",
      "Epoch 25/50\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 1.2203 - accuracy: 0.5632 - val_loss: 1.5038 - val_accuracy: 0.4919\n",
      "Epoch 26/50\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2112 - accuracy: 0.5646 - val_loss: 1.4781 - val_accuracy: 0.4946\n",
      "Epoch 27/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.2118 - accuracy: 0.5648 - val_loss: 1.4504 - val_accuracy: 0.4995\n",
      "Epoch 28/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1993 - accuracy: 0.5700 - val_loss: 1.4648 - val_accuracy: 0.4959\n",
      "Epoch 29/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1965 - accuracy: 0.5730 - val_loss: 1.4783 - val_accuracy: 0.4973\n",
      "Epoch 30/50\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1893 - accuracy: 0.5726 - val_loss: 1.4904 - val_accuracy: 0.4900\n",
      "Epoch 31/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1846 - accuracy: 0.5753 - val_loss: 1.4656 - val_accuracy: 0.4991\n",
      "Epoch 32/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1817 - accuracy: 0.5759 - val_loss: 1.4774 - val_accuracy: 0.5045\n",
      "Epoch 33/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1754 - accuracy: 0.5757 - val_loss: 1.4735 - val_accuracy: 0.4984\n",
      "Epoch 34/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1705 - accuracy: 0.5788 - val_loss: 1.4906 - val_accuracy: 0.4988\n",
      "Epoch 35/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1665 - accuracy: 0.5792 - val_loss: 1.4861 - val_accuracy: 0.4983\n",
      "Epoch 36/50\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1581 - accuracy: 0.5863 - val_loss: 1.4661 - val_accuracy: 0.4994\n",
      "Epoch 37/50\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1528 - accuracy: 0.5857 - val_loss: 1.4924 - val_accuracy: 0.5008\n",
      "Epoch 38/50\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1493 - accuracy: 0.5863 - val_loss: 1.5153 - val_accuracy: 0.4983\n",
      "Epoch 39/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1488 - accuracy: 0.5868 - val_loss: 1.5187 - val_accuracy: 0.4909\n",
      "Epoch 40/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1398 - accuracy: 0.5888 - val_loss: 1.5137 - val_accuracy: 0.4977\n",
      "Epoch 41/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1383 - accuracy: 0.5922 - val_loss: 1.5129 - val_accuracy: 0.4984\n",
      "Epoch 42/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1282 - accuracy: 0.5935 - val_loss: 1.5290 - val_accuracy: 0.4899\n",
      "Epoch 43/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1272 - accuracy: 0.5949 - val_loss: 1.5335 - val_accuracy: 0.5011\n",
      "Epoch 44/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1239 - accuracy: 0.5937 - val_loss: 1.6201 - val_accuracy: 0.4851\n",
      "Epoch 45/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1245 - accuracy: 0.5953 - val_loss: 1.5491 - val_accuracy: 0.4929\n",
      "Epoch 46/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1176 - accuracy: 0.5989 - val_loss: 1.5552 - val_accuracy: 0.4949\n",
      "Epoch 47/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1147 - accuracy: 0.5990 - val_loss: 1.5605 - val_accuracy: 0.4904\n",
      "Epoch 48/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1111 - accuracy: 0.6014 - val_loss: 1.5262 - val_accuracy: 0.5022\n",
      "Epoch 49/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1066 - accuracy: 0.6026 - val_loss: 1.6075 - val_accuracy: 0.4831\n",
      "Epoch 50/50\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1039 - accuracy: 0.6025 - val_loss: 1.5897 - val_accuracy: 0.4909\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.5897 - accuracy: 0.4909\n"
     ]
    }
   ],
   "source": [
    "history3 = model3.fit(train_images, train_labels, epochs=50,\n",
    "                    validation_data=(test_images, test_labels))\n",
    "\n",
    "test_loss3, test_acc3 = model3.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39110700",
   "metadata": {},
   "source": [
    "2- Reuse the code from part 1 to build and run a MLP with one hidden layer as big a you can. \n",
    "Compare the performance of your design with the results appearing in Table 1 of [https://arxiv.org/pdf/1611.03530.pdf] for a MLP of 512 units in a single \n",
    "hidden layer. Report the best result found for a maximum of 1000 epochs or 2 hrs CPU running time.\n",
    "The best accuracy amongst all teams will be awarded extra points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea1ebb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do they mean with as big as you can\n",
    "# Add dropout, batchnorm?, minibatches?, regurilazation and data augment!\n",
    "model_big = models.Sequential([\n",
    "    layers.Flatten(), # Input layer doesn't count\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1024, activation=\"relu\"), # 1 Hidden layer\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(10, activation='softmax') # end with softmax for classification also the final layer isn't part of a hidden layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bd5e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_big.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0999afd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.9242 - accuracy: 0.3280 - val_loss: 1.7704 - val_accuracy: 0.3880\n",
      "Epoch 2/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.8098 - accuracy: 0.3606 - val_loss: 1.7885 - val_accuracy: 0.3752\n",
      "Epoch 3/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.7498 - accuracy: 0.3842 - val_loss: 1.7576 - val_accuracy: 0.3776\n",
      "Epoch 4/1000\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 1.7083 - accuracy: 0.3969 - val_loss: 1.6607 - val_accuracy: 0.4059\n",
      "Epoch 5/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.6701 - accuracy: 0.4123 - val_loss: 1.6758 - val_accuracy: 0.4118\n",
      "Epoch 6/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.6387 - accuracy: 0.4240 - val_loss: 1.6356 - val_accuracy: 0.4256\n",
      "Epoch 7/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.6182 - accuracy: 0.4292 - val_loss: 1.6176 - val_accuracy: 0.4229\n",
      "Epoch 8/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.6013 - accuracy: 0.4348 - val_loss: 1.5468 - val_accuracy: 0.4566\n",
      "Epoch 9/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.5805 - accuracy: 0.4408 - val_loss: 1.5447 - val_accuracy: 0.4589\n",
      "Epoch 10/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.5680 - accuracy: 0.4454 - val_loss: 1.5427 - val_accuracy: 0.4521\n",
      "Epoch 11/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.5571 - accuracy: 0.4510 - val_loss: 1.5414 - val_accuracy: 0.4533\n",
      "Epoch 12/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.5455 - accuracy: 0.4522 - val_loss: 1.5164 - val_accuracy: 0.4696\n",
      "Epoch 13/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.5366 - accuracy: 0.4555 - val_loss: 1.5107 - val_accuracy: 0.4748\n",
      "Epoch 14/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.5334 - accuracy: 0.4579 - val_loss: 1.5089 - val_accuracy: 0.4638\n",
      "Epoch 15/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.5260 - accuracy: 0.4606 - val_loss: 1.5320 - val_accuracy: 0.4621\n",
      "Epoch 16/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.5241 - accuracy: 0.4592 - val_loss: 1.5224 - val_accuracy: 0.4609\n",
      "Epoch 17/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.5147 - accuracy: 0.4657 - val_loss: 1.4929 - val_accuracy: 0.4741\n",
      "Epoch 18/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.5033 - accuracy: 0.4678 - val_loss: 1.5130 - val_accuracy: 0.4705\n",
      "Epoch 19/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.4940 - accuracy: 0.4746 - val_loss: 1.5549 - val_accuracy: 0.4594\n",
      "Epoch 20/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.4871 - accuracy: 0.4736 - val_loss: 1.4743 - val_accuracy: 0.4859\n",
      "Epoch 21/1000\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 1.4843 - accuracy: 0.4741 - val_loss: 1.7443 - val_accuracy: 0.4096\n",
      "Epoch 22/1000\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 1.4796 - accuracy: 0.4742 - val_loss: 1.4726 - val_accuracy: 0.4877\n",
      "Epoch 23/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.4686 - accuracy: 0.4819 - val_loss: 1.4664 - val_accuracy: 0.4859\n",
      "Epoch 24/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.4591 - accuracy: 0.4833 - val_loss: 1.4576 - val_accuracy: 0.4886\n",
      "Epoch 25/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.4520 - accuracy: 0.4874 - val_loss: 1.4871 - val_accuracy: 0.4863\n",
      "Epoch 26/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.4536 - accuracy: 0.4853 - val_loss: 1.4545 - val_accuracy: 0.4899\n",
      "Epoch 27/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.4510 - accuracy: 0.4862 - val_loss: 1.4362 - val_accuracy: 0.4932\n",
      "Epoch 28/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.4425 - accuracy: 0.4895 - val_loss: 1.4603 - val_accuracy: 0.4868\n",
      "Epoch 29/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.4442 - accuracy: 0.4923 - val_loss: 1.4642 - val_accuracy: 0.4854\n",
      "Epoch 30/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.4379 - accuracy: 0.4922 - val_loss: 1.4688 - val_accuracy: 0.4870\n",
      "Epoch 31/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.4292 - accuracy: 0.4964 - val_loss: 1.4694 - val_accuracy: 0.4865\n",
      "Epoch 32/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.4247 - accuracy: 0.5001 - val_loss: 1.4611 - val_accuracy: 0.4819\n",
      "Epoch 33/1000\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 1.4227 - accuracy: 0.4981 - val_loss: 1.4724 - val_accuracy: 0.4996\n",
      "Epoch 34/1000\n",
      "1563/1563 [==============================] - 21s 14ms/step - loss: 1.4176 - accuracy: 0.4993 - val_loss: 1.4346 - val_accuracy: 0.4989\n",
      "Epoch 35/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.4115 - accuracy: 0.5008 - val_loss: 1.4372 - val_accuracy: 0.5006\n",
      "Epoch 36/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.4067 - accuracy: 0.5019 - val_loss: 1.4877 - val_accuracy: 0.4900\n",
      "Epoch 37/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.4067 - accuracy: 0.5034 - val_loss: 1.4170 - val_accuracy: 0.4990\n",
      "Epoch 38/1000\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 1.4010 - accuracy: 0.5051 - val_loss: 1.4157 - val_accuracy: 0.5018\n",
      "Epoch 39/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.3992 - accuracy: 0.5071 - val_loss: 1.4159 - val_accuracy: 0.4967\n",
      "Epoch 40/1000\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 1.3956 - accuracy: 0.5085 - val_loss: 1.4748 - val_accuracy: 0.4788\n",
      "Epoch 41/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3988 - accuracy: 0.5041 - val_loss: 1.4409 - val_accuracy: 0.4997\n",
      "Epoch 42/1000\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 1.3997 - accuracy: 0.5066 - val_loss: 1.4378 - val_accuracy: 0.4987\n",
      "Epoch 43/1000\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.3974 - accuracy: 0.5079 - val_loss: 1.4434 - val_accuracy: 0.4924\n",
      "Epoch 44/1000\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 1.3887 - accuracy: 0.5093 - val_loss: 1.4289 - val_accuracy: 0.4971\n",
      "Epoch 45/1000\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 1.3830 - accuracy: 0.5099 - val_loss: 1.4302 - val_accuracy: 0.4931\n",
      "Epoch 46/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.3846 - accuracy: 0.5100 - val_loss: 1.3965 - val_accuracy: 0.5178\n",
      "Epoch 47/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.3814 - accuracy: 0.5127 - val_loss: 1.4573 - val_accuracy: 0.4869\n",
      "Epoch 48/1000\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.3781 - accuracy: 0.5138 - val_loss: 1.4237 - val_accuracy: 0.5044\n",
      "Epoch 49/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3738 - accuracy: 0.5146 - val_loss: 1.4011 - val_accuracy: 0.5109\n",
      "Epoch 50/1000\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.3713 - accuracy: 0.5165 - val_loss: 1.4281 - val_accuracy: 0.5037\n",
      "Epoch 51/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.3709 - accuracy: 0.5155 - val_loss: 1.4279 - val_accuracy: 0.5023\n",
      "Epoch 52/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.3686 - accuracy: 0.5158 - val_loss: 1.4012 - val_accuracy: 0.5135\n",
      "Epoch 53/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.3699 - accuracy: 0.5155 - val_loss: 1.4153 - val_accuracy: 0.5042\n",
      "Epoch 54/1000\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.3638 - accuracy: 0.5170 - val_loss: 1.4262 - val_accuracy: 0.5114\n",
      "Epoch 55/1000\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.3590 - accuracy: 0.5208 - val_loss: 1.4100 - val_accuracy: 0.5072\n",
      "Epoch 56/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.3548 - accuracy: 0.5215 - val_loss: 1.4461 - val_accuracy: 0.4964\n",
      "Epoch 57/1000\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.3549 - accuracy: 0.5210 - val_loss: 1.4131 - val_accuracy: 0.5059\n",
      "Epoch 58/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3610 - accuracy: 0.5174 - val_loss: 1.4006 - val_accuracy: 0.5167\n",
      "Epoch 59/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3596 - accuracy: 0.5200 - val_loss: 1.3770 - val_accuracy: 0.5142\n",
      "Epoch 60/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3450 - accuracy: 0.5242 - val_loss: 1.3794 - val_accuracy: 0.5172\n",
      "Epoch 61/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3480 - accuracy: 0.5249 - val_loss: 1.3795 - val_accuracy: 0.5148\n",
      "Epoch 62/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3516 - accuracy: 0.5216 - val_loss: 1.3903 - val_accuracy: 0.5093\n",
      "Epoch 63/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3502 - accuracy: 0.5218 - val_loss: 1.4179 - val_accuracy: 0.5000\n",
      "Epoch 64/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3451 - accuracy: 0.5262 - val_loss: 1.3793 - val_accuracy: 0.5177\n",
      "Epoch 65/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3470 - accuracy: 0.5247 - val_loss: 1.3853 - val_accuracy: 0.5170\n",
      "Epoch 66/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3436 - accuracy: 0.5274 - val_loss: 1.4147 - val_accuracy: 0.5027\n",
      "Epoch 67/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3391 - accuracy: 0.5289 - val_loss: 1.4067 - val_accuracy: 0.5108\n",
      "Epoch 68/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3394 - accuracy: 0.5264 - val_loss: 1.4212 - val_accuracy: 0.5041\n",
      "Epoch 69/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3383 - accuracy: 0.5258 - val_loss: 1.4006 - val_accuracy: 0.5108\n",
      "Epoch 70/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3353 - accuracy: 0.5290 - val_loss: 1.4094 - val_accuracy: 0.5102\n",
      "Epoch 71/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3358 - accuracy: 0.5306 - val_loss: 1.3967 - val_accuracy: 0.5129\n",
      "Epoch 72/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3331 - accuracy: 0.5295 - val_loss: 1.4245 - val_accuracy: 0.5022\n",
      "Epoch 73/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3285 - accuracy: 0.5314 - val_loss: 1.3953 - val_accuracy: 0.5097\n",
      "Epoch 74/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3280 - accuracy: 0.5302 - val_loss: 1.3958 - val_accuracy: 0.5071\n",
      "Epoch 75/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3298 - accuracy: 0.5322 - val_loss: 1.4027 - val_accuracy: 0.5012\n",
      "Epoch 76/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3248 - accuracy: 0.5332 - val_loss: 1.4179 - val_accuracy: 0.5016\n",
      "Epoch 77/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3268 - accuracy: 0.5328 - val_loss: 1.3761 - val_accuracy: 0.5138\n",
      "Epoch 78/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3260 - accuracy: 0.5331 - val_loss: 1.4168 - val_accuracy: 0.5019\n",
      "Epoch 79/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3277 - accuracy: 0.5334 - val_loss: 1.4190 - val_accuracy: 0.5054\n",
      "Epoch 80/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3236 - accuracy: 0.5329 - val_loss: 1.3804 - val_accuracy: 0.5151\n",
      "Epoch 81/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3295 - accuracy: 0.5304 - val_loss: 1.3845 - val_accuracy: 0.5211\n",
      "Epoch 82/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3256 - accuracy: 0.5324 - val_loss: 1.3929 - val_accuracy: 0.5074\n",
      "Epoch 83/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3272 - accuracy: 0.5305 - val_loss: 1.3995 - val_accuracy: 0.5128\n",
      "Epoch 84/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3248 - accuracy: 0.5313 - val_loss: 1.4186 - val_accuracy: 0.5085\n",
      "Epoch 85/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3180 - accuracy: 0.5355 - val_loss: 1.4064 - val_accuracy: 0.4987\n",
      "Epoch 86/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3273 - accuracy: 0.5333 - val_loss: 1.3755 - val_accuracy: 0.5184\n",
      "Epoch 87/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3222 - accuracy: 0.5319 - val_loss: 1.4028 - val_accuracy: 0.5063\n",
      "Epoch 88/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3235 - accuracy: 0.5317 - val_loss: 1.4047 - val_accuracy: 0.5060\n",
      "Epoch 89/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3195 - accuracy: 0.5345 - val_loss: 1.3787 - val_accuracy: 0.5132\n",
      "Epoch 90/1000\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 1.3182 - accuracy: 0.5344 - val_loss: 1.4147 - val_accuracy: 0.5047\n",
      "Epoch 91/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.3198 - accuracy: 0.5328 - val_loss: 1.3990 - val_accuracy: 0.5080\n",
      "Epoch 92/1000\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.3160 - accuracy: 0.5352 - val_loss: 1.3728 - val_accuracy: 0.5166\n",
      "Epoch 93/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3134 - accuracy: 0.5357 - val_loss: 1.3895 - val_accuracy: 0.5093\n",
      "Epoch 94/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3179 - accuracy: 0.5324 - val_loss: 1.3847 - val_accuracy: 0.5122\n",
      "Epoch 95/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3159 - accuracy: 0.5332 - val_loss: 1.3824 - val_accuracy: 0.5146\n",
      "Epoch 96/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3151 - accuracy: 0.5347 - val_loss: 1.3795 - val_accuracy: 0.5180\n",
      "Epoch 97/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3081 - accuracy: 0.5358 - val_loss: 1.3716 - val_accuracy: 0.5210\n",
      "Epoch 98/1000\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 1.3060 - accuracy: 0.5386 - val_loss: 1.3944 - val_accuracy: 0.5125\n",
      "Epoch 99/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.3073 - accuracy: 0.5363 - val_loss: 1.3862 - val_accuracy: 0.5173\n",
      "Epoch 100/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.3090 - accuracy: 0.5377 - val_loss: 1.4249 - val_accuracy: 0.5111\n",
      "Epoch 101/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.3050 - accuracy: 0.5373 - val_loss: 1.4037 - val_accuracy: 0.5107\n",
      "Epoch 102/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.3093 - accuracy: 0.5395 - val_loss: 1.3759 - val_accuracy: 0.5204\n",
      "Epoch 103/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.3128 - accuracy: 0.5362 - val_loss: 1.3724 - val_accuracy: 0.5146\n",
      "Epoch 104/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.3193 - accuracy: 0.5315 - val_loss: 1.3948 - val_accuracy: 0.5185\n",
      "Epoch 105/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.3139 - accuracy: 0.5366 - val_loss: 1.4240 - val_accuracy: 0.5082\n",
      "Epoch 106/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.3072 - accuracy: 0.5349 - val_loss: 1.3970 - val_accuracy: 0.5163\n",
      "Epoch 107/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.3101 - accuracy: 0.5357 - val_loss: 1.4039 - val_accuracy: 0.5101\n",
      "Epoch 108/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.2972 - accuracy: 0.5398 - val_loss: 1.4039 - val_accuracy: 0.5111\n",
      "Epoch 109/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.3025 - accuracy: 0.5381 - val_loss: 1.4284 - val_accuracy: 0.5009\n",
      "Epoch 110/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.2921 - accuracy: 0.5432 - val_loss: 1.3903 - val_accuracy: 0.5173\n",
      "Epoch 111/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.2934 - accuracy: 0.5439 - val_loss: 1.3715 - val_accuracy: 0.5195\n",
      "Epoch 112/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.2967 - accuracy: 0.5426 - val_loss: 1.3597 - val_accuracy: 0.5223\n",
      "Epoch 113/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2914 - accuracy: 0.5444 - val_loss: 1.3615 - val_accuracy: 0.5242\n",
      "Epoch 114/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2923 - accuracy: 0.5423 - val_loss: 1.3755 - val_accuracy: 0.5202\n",
      "Epoch 115/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2953 - accuracy: 0.5431 - val_loss: 1.3624 - val_accuracy: 0.5236\n",
      "Epoch 116/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.2902 - accuracy: 0.5442 - val_loss: 1.3863 - val_accuracy: 0.5196\n",
      "Epoch 117/1000\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 1.2901 - accuracy: 0.5455 - val_loss: 1.3826 - val_accuracy: 0.5163\n",
      "Epoch 118/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2897 - accuracy: 0.5436 - val_loss: 1.3724 - val_accuracy: 0.5196\n",
      "Epoch 119/1000\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.2890 - accuracy: 0.5464 - val_loss: 1.3740 - val_accuracy: 0.5290\n",
      "Epoch 120/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2860 - accuracy: 0.5446 - val_loss: 1.3657 - val_accuracy: 0.5179\n",
      "Epoch 121/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2827 - accuracy: 0.5454 - val_loss: 1.4391 - val_accuracy: 0.5006\n",
      "Epoch 122/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2852 - accuracy: 0.5427 - val_loss: 1.3789 - val_accuracy: 0.5161\n",
      "Epoch 123/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2813 - accuracy: 0.5469 - val_loss: 1.3652 - val_accuracy: 0.5186\n",
      "Epoch 124/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2795 - accuracy: 0.5463 - val_loss: 1.3782 - val_accuracy: 0.5218\n",
      "Epoch 125/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2806 - accuracy: 0.5465 - val_loss: 1.3556 - val_accuracy: 0.5252\n",
      "Epoch 126/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.2864 - accuracy: 0.5461 - val_loss: 1.3830 - val_accuracy: 0.5138\n",
      "Epoch 127/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2872 - accuracy: 0.5452 - val_loss: 1.3800 - val_accuracy: 0.5202\n",
      "Epoch 128/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2835 - accuracy: 0.5454 - val_loss: 1.3830 - val_accuracy: 0.5259\n",
      "Epoch 129/1000\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.2805 - accuracy: 0.5491 - val_loss: 1.3806 - val_accuracy: 0.5198\n",
      "Epoch 130/1000\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.2817 - accuracy: 0.5467 - val_loss: 1.3611 - val_accuracy: 0.5230\n",
      "Epoch 131/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2844 - accuracy: 0.5464 - val_loss: 1.3921 - val_accuracy: 0.5170\n",
      "Epoch 132/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2756 - accuracy: 0.5486 - val_loss: 1.3827 - val_accuracy: 0.5210\n",
      "Epoch 133/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2770 - accuracy: 0.5484 - val_loss: 1.4157 - val_accuracy: 0.4959\n",
      "Epoch 134/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2791 - accuracy: 0.5471 - val_loss: 1.4270 - val_accuracy: 0.5009\n",
      "Epoch 135/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2798 - accuracy: 0.5462 - val_loss: 1.3971 - val_accuracy: 0.5164\n",
      "Epoch 136/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2757 - accuracy: 0.5490 - val_loss: 1.3636 - val_accuracy: 0.5243\n",
      "Epoch 137/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2800 - accuracy: 0.5487 - val_loss: 1.3749 - val_accuracy: 0.5237\n",
      "Epoch 138/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2775 - accuracy: 0.5490 - val_loss: 1.4057 - val_accuracy: 0.5141\n",
      "Epoch 139/1000\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.2715 - accuracy: 0.5520 - val_loss: 1.3876 - val_accuracy: 0.5169\n",
      "Epoch 140/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2779 - accuracy: 0.5461 - val_loss: 1.3629 - val_accuracy: 0.5220\n",
      "Epoch 141/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2680 - accuracy: 0.5487 - val_loss: 1.3748 - val_accuracy: 0.5271\n",
      "Epoch 142/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2711 - accuracy: 0.5499 - val_loss: 1.4473 - val_accuracy: 0.5053\n",
      "Epoch 143/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2687 - accuracy: 0.5519 - val_loss: 1.3684 - val_accuracy: 0.5243\n",
      "Epoch 144/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2695 - accuracy: 0.5506 - val_loss: 1.3605 - val_accuracy: 0.5207\n",
      "Epoch 145/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2712 - accuracy: 0.5515 - val_loss: 1.3667 - val_accuracy: 0.5212\n",
      "Epoch 146/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2659 - accuracy: 0.5520 - val_loss: 1.4080 - val_accuracy: 0.5144\n",
      "Epoch 147/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2687 - accuracy: 0.5503 - val_loss: 1.3721 - val_accuracy: 0.5199\n",
      "Epoch 148/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2707 - accuracy: 0.5496 - val_loss: 1.3680 - val_accuracy: 0.5254\n",
      "Epoch 149/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2696 - accuracy: 0.5502 - val_loss: 1.4018 - val_accuracy: 0.5167\n",
      "Epoch 150/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2667 - accuracy: 0.5525 - val_loss: 1.3961 - val_accuracy: 0.5181\n",
      "Epoch 151/1000\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.2698 - accuracy: 0.5509 - val_loss: 1.3695 - val_accuracy: 0.5230\n",
      "Epoch 152/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2612 - accuracy: 0.5523 - val_loss: 1.3703 - val_accuracy: 0.5152\n",
      "Epoch 153/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2644 - accuracy: 0.5520 - val_loss: 1.3643 - val_accuracy: 0.5258\n",
      "Epoch 154/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2655 - accuracy: 0.5529 - val_loss: 1.3664 - val_accuracy: 0.5289\n",
      "Epoch 155/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2646 - accuracy: 0.5544 - val_loss: 1.3901 - val_accuracy: 0.5119\n",
      "Epoch 156/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2686 - accuracy: 0.5512 - val_loss: 1.3805 - val_accuracy: 0.5197\n",
      "Epoch 157/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2589 - accuracy: 0.5541 - val_loss: 1.3735 - val_accuracy: 0.5289\n",
      "Epoch 158/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2636 - accuracy: 0.5554 - val_loss: 1.4039 - val_accuracy: 0.5243\n",
      "Epoch 159/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2614 - accuracy: 0.5534 - val_loss: 1.4053 - val_accuracy: 0.5203\n",
      "Epoch 160/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2595 - accuracy: 0.5544 - val_loss: 1.4019 - val_accuracy: 0.5112\n",
      "Epoch 161/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2599 - accuracy: 0.5543 - val_loss: 1.3936 - val_accuracy: 0.5195\n",
      "Epoch 162/1000\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.2554 - accuracy: 0.5535 - val_loss: 1.3626 - val_accuracy: 0.5245\n",
      "Epoch 163/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.2542 - accuracy: 0.5579 - val_loss: 1.3949 - val_accuracy: 0.5233\n",
      "Epoch 164/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2543 - accuracy: 0.5572 - val_loss: 1.3758 - val_accuracy: 0.5253\n",
      "Epoch 165/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.2575 - accuracy: 0.5544 - val_loss: 1.3997 - val_accuracy: 0.5127\n",
      "Epoch 166/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.2572 - accuracy: 0.5561 - val_loss: 1.3551 - val_accuracy: 0.5302\n",
      "Epoch 167/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.2583 - accuracy: 0.5546 - val_loss: 1.3683 - val_accuracy: 0.5217\n",
      "Epoch 168/1000\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.2521 - accuracy: 0.5544 - val_loss: 1.3618 - val_accuracy: 0.5240\n",
      "Epoch 169/1000\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.2533 - accuracy: 0.5543 - val_loss: 1.3506 - val_accuracy: 0.5227\n",
      "Epoch 170/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2490 - accuracy: 0.5589 - val_loss: 1.3692 - val_accuracy: 0.5223\n",
      "Epoch 171/1000\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 1.2541 - accuracy: 0.5572 - val_loss: 1.3590 - val_accuracy: 0.5250\n",
      "Epoch 172/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.2549 - accuracy: 0.5570 - val_loss: 1.3618 - val_accuracy: 0.5249\n",
      "Epoch 173/1000\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 1.2551 - accuracy: 0.5559 - val_loss: 1.3843 - val_accuracy: 0.5207\n",
      "Epoch 174/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2522 - accuracy: 0.5560 - val_loss: 1.3636 - val_accuracy: 0.5299\n",
      "Epoch 175/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2588 - accuracy: 0.5517 - val_loss: 1.3807 - val_accuracy: 0.5230\n",
      "Epoch 176/1000\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.2533 - accuracy: 0.5586 - val_loss: 1.3630 - val_accuracy: 0.5272\n",
      "Epoch 177/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2522 - accuracy: 0.5564 - val_loss: 1.3368 - val_accuracy: 0.5296\n",
      "Epoch 178/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2494 - accuracy: 0.5601 - val_loss: 1.3503 - val_accuracy: 0.5296\n",
      "Epoch 179/1000\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.2531 - accuracy: 0.5580 - val_loss: 1.3951 - val_accuracy: 0.5101\n",
      "Epoch 180/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2527 - accuracy: 0.5566 - val_loss: 1.3577 - val_accuracy: 0.5317\n",
      "Epoch 181/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.2490 - accuracy: 0.5598 - val_loss: 1.3591 - val_accuracy: 0.5235\n",
      "Epoch 182/1000\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.2509 - accuracy: 0.5561 - val_loss: 1.3490 - val_accuracy: 0.5275\n",
      "Epoch 183/1000\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 1.2456 - accuracy: 0.5610 - val_loss: 1.3377 - val_accuracy: 0.5272\n",
      "Epoch 184/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2456 - accuracy: 0.5597 - val_loss: 1.3627 - val_accuracy: 0.5198\n",
      "Epoch 185/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2498 - accuracy: 0.5570 - val_loss: 1.3428 - val_accuracy: 0.5251\n",
      "Epoch 186/1000\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 1.2472 - accuracy: 0.5569 - val_loss: 1.3640 - val_accuracy: 0.5267\n",
      "Epoch 187/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.2431 - accuracy: 0.5572 - val_loss: 1.3726 - val_accuracy: 0.5202\n",
      "Epoch 188/1000\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.2505 - accuracy: 0.5563 - val_loss: 1.3508 - val_accuracy: 0.5264\n",
      "Epoch 189/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.2484 - accuracy: 0.5598 - val_loss: 1.3738 - val_accuracy: 0.5236\n",
      "Epoch 190/1000\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.2416 - accuracy: 0.5629 - val_loss: 1.3570 - val_accuracy: 0.5253\n",
      "Epoch 191/1000\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 1.2410 - accuracy: 0.5627 - val_loss: 1.3749 - val_accuracy: 0.5180\n",
      "Epoch 192/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.2430 - accuracy: 0.5615 - val_loss: 1.3608 - val_accuracy: 0.5257\n",
      "Epoch 193/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2454 - accuracy: 0.5582 - val_loss: 1.3584 - val_accuracy: 0.5227\n",
      "Epoch 194/1000\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.2449 - accuracy: 0.5616 - val_loss: 1.3486 - val_accuracy: 0.5291\n",
      "Epoch 195/1000\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 1.2465 - accuracy: 0.5594 - val_loss: 1.3512 - val_accuracy: 0.5244\n",
      "Epoch 196/1000\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.2434 - accuracy: 0.5609 - val_loss: 1.3511 - val_accuracy: 0.5325\n",
      "Epoch 197/1000\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 1.2459 - accuracy: 0.5611 - val_loss: 1.3826 - val_accuracy: 0.5281\n",
      "Epoch 198/1000\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.2396 - accuracy: 0.5587 - val_loss: 1.3638 - val_accuracy: 0.5249\n",
      "Epoch 199/1000\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.2402 - accuracy: 0.5617 - val_loss: 1.3633 - val_accuracy: 0.5256\n",
      "Epoch 200/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2374 - accuracy: 0.5602 - val_loss: 1.3679 - val_accuracy: 0.5247\n",
      "Epoch 201/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2421 - accuracy: 0.5609 - val_loss: 1.3410 - val_accuracy: 0.5325\n",
      "Epoch 202/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2381 - accuracy: 0.5617 - val_loss: 1.3915 - val_accuracy: 0.5189\n",
      "Epoch 203/1000\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.2450 - accuracy: 0.5596 - val_loss: 1.3742 - val_accuracy: 0.5232\n",
      "Epoch 204/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2376 - accuracy: 0.5617 - val_loss: 1.3710 - val_accuracy: 0.5235\n",
      "Epoch 205/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2387 - accuracy: 0.5614 - val_loss: 1.3749 - val_accuracy: 0.5262\n",
      "Epoch 206/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2404 - accuracy: 0.5586 - val_loss: 1.3868 - val_accuracy: 0.5165\n",
      "Epoch 207/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2408 - accuracy: 0.5599 - val_loss: 1.3856 - val_accuracy: 0.5219\n",
      "Epoch 208/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2387 - accuracy: 0.5614 - val_loss: 1.3688 - val_accuracy: 0.5273\n",
      "Epoch 209/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2394 - accuracy: 0.5615 - val_loss: 1.3755 - val_accuracy: 0.5262\n",
      "Epoch 210/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2365 - accuracy: 0.5610 - val_loss: 1.3588 - val_accuracy: 0.5288\n",
      "Epoch 211/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2353 - accuracy: 0.5630 - val_loss: 1.3894 - val_accuracy: 0.5281\n",
      "Epoch 212/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2374 - accuracy: 0.5628 - val_loss: 1.4021 - val_accuracy: 0.5205\n",
      "Epoch 213/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2376 - accuracy: 0.5618 - val_loss: 1.3527 - val_accuracy: 0.5297\n",
      "Epoch 214/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2353 - accuracy: 0.5606 - val_loss: 1.3923 - val_accuracy: 0.5234\n",
      "Epoch 215/1000\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.2392 - accuracy: 0.5602 - val_loss: 1.3885 - val_accuracy: 0.5216\n",
      "Epoch 216/1000\n",
      " 687/1563 [============>.................] - ETA: 12s - loss: 1.2241 - accuracy: 0.5620"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13156/4106505860.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m history_big = model_big.fit(train_images, train_labels, epochs=1000,\n\u001b[0m\u001b[0;32m      2\u001b[0m                     validation_data=(test_images, test_labels))\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_loss_big\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc_big\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_big\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ylja0\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ylja0\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ylja0\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ylja0\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ylja0\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\users\\ylja0\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ylja0\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history_big = model_big.fit(train_images, train_labels, epochs=1000,\n",
    "                    validation_data=(test_images, test_labels))\n",
    "\n",
    "test_loss_big, test_acc_big = model_big.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "756db38d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history_big' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13156/3250056480.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory_big\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory_big\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history_big' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(history_big.history['accuracy'], label='accuracy')\n",
    "plt.plot(history_big.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e2e370-0bf4-4e9a-90eb-8f4104667fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
