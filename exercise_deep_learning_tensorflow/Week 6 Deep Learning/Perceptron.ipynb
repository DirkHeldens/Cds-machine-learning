{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6029b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from the code on https://www.tensorflow.org/tutorials/images/cnn \n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models, regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c34469e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc381da",
   "metadata": {},
   "source": [
    "1- Multi Layer Perceptron (MLP). Modify the provided script 'perceptron.py' to build a MLP. Use architectures \n",
    "with 0, 1 and 2 hidden layers. Keep the complexity of the model bounded so runs do not take much more\n",
    "than 1 hour to reach the maximum of testing accuracy. Notice that the input needs to be \"flattened\" since there is no spatial structure \n",
    "in this fully connected design.  This can be achieved by adding a dummy layer with no free parameters with \"layers.Flatten()\"\n",
    "as the first layer in the constructor \"model.Sequential()\". Obtain the learning curves and discuss the results.\n",
    "Report the optimizer in use, initialization parameters, the learning rate, etc. Is early stopping convenient\n",
    "in this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28afbf72",
   "metadata": {},
   "source": [
    "YOUR ANWSER HERE: \n",
    "\n",
    "* item Optimizer: Adam \n",
    "* item Initialization parameters: TODO\n",
    "* item Learning rate: Defaults to 0.001, this learning rate however is tweaked by the adam optimizer.\n",
    "\n",
    "Is early stopping convenientin this model? </br>\n",
    "In the given example model early stopping has no use, because we get a results that is quite bad already and early stopping it would only worsen our result. we should only stop early if we get within a certain very low loss range, which we clearly aren't in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f988a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original\n",
    "model1 = models.Sequential([\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(10, activation='softmax') # end with softmax for classification\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74650f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee6e520d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.0489 - accuracy: 0.2773 - val_loss: 1.9155 - val_accuracy: 0.3348\n",
      "Epoch 2/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.8887 - accuracy: 0.3518 - val_loss: 1.8612 - val_accuracy: 0.3577\n",
      "Epoch 3/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.8529 - accuracy: 0.3613 - val_loss: 1.9880 - val_accuracy: 0.3287\n",
      "Epoch 4/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.8428 - accuracy: 0.3726 - val_loss: 1.9051 - val_accuracy: 0.3577\n",
      "Epoch 5/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.8387 - accuracy: 0.3746 - val_loss: 1.8376 - val_accuracy: 0.3622\n",
      "Epoch 6/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.8170 - accuracy: 0.3802 - val_loss: 1.8306 - val_accuracy: 0.3758\n",
      "Epoch 7/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.8057 - accuracy: 0.3830 - val_loss: 1.7995 - val_accuracy: 0.3766\n",
      "Epoch 8/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.8047 - accuracy: 0.3835 - val_loss: 1.8815 - val_accuracy: 0.3561\n",
      "Epoch 9/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7945 - accuracy: 0.3840 - val_loss: 1.8975 - val_accuracy: 0.3616\n",
      "Epoch 10/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7956 - accuracy: 0.3847 - val_loss: 1.8974 - val_accuracy: 0.3405\n",
      "Epoch 11/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.8095 - accuracy: 0.3850 - val_loss: 2.0555 - val_accuracy: 0.3142\n",
      "Epoch 12/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7982 - accuracy: 0.3900 - val_loss: 1.8422 - val_accuracy: 0.3686\n",
      "Epoch 13/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7924 - accuracy: 0.3926 - val_loss: 1.8129 - val_accuracy: 0.3729\n",
      "Epoch 14/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7904 - accuracy: 0.3899 - val_loss: 2.2214 - val_accuracy: 0.3178\n",
      "Epoch 15/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.8046 - accuracy: 0.3881 - val_loss: 1.8627 - val_accuracy: 0.3601\n",
      "Epoch 16/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7728 - accuracy: 0.3905 - val_loss: 1.9220 - val_accuracy: 0.3521\n",
      "Epoch 17/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7729 - accuracy: 0.3954 - val_loss: 2.0107 - val_accuracy: 0.3289\n",
      "Epoch 18/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7751 - accuracy: 0.3934 - val_loss: 1.8745 - val_accuracy: 0.3529\n",
      "Epoch 19/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7658 - accuracy: 0.3988 - val_loss: 1.9565 - val_accuracy: 0.3522\n",
      "Epoch 20/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7750 - accuracy: 0.3973 - val_loss: 1.8992 - val_accuracy: 0.3542\n",
      "Epoch 21/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7741 - accuracy: 0.3941 - val_loss: 1.9212 - val_accuracy: 0.3515\n",
      "Epoch 22/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7762 - accuracy: 0.3943 - val_loss: 1.8584 - val_accuracy: 0.3657\n",
      "Epoch 23/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7720 - accuracy: 0.3993 - val_loss: 1.8234 - val_accuracy: 0.3594\n",
      "Epoch 24/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7518 - accuracy: 0.3996 - val_loss: 1.8493 - val_accuracy: 0.3716\n",
      "Epoch 25/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7757 - accuracy: 0.3982 - val_loss: 1.8483 - val_accuracy: 0.3790\n",
      "Epoch 26/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7530 - accuracy: 0.4048 - val_loss: 1.8808 - val_accuracy: 0.3535\n",
      "Epoch 27/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7757 - accuracy: 0.3929 - val_loss: 1.9137 - val_accuracy: 0.3569\n",
      "Epoch 28/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7453 - accuracy: 0.4033 - val_loss: 1.9109 - val_accuracy: 0.3481\n",
      "Epoch 29/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7622 - accuracy: 0.3992 - val_loss: 1.8394 - val_accuracy: 0.3812\n",
      "Epoch 30/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7477 - accuracy: 0.4111 - val_loss: 1.8400 - val_accuracy: 0.3634\n",
      "Epoch 31/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7448 - accuracy: 0.4049 - val_loss: 1.8708 - val_accuracy: 0.3564\n",
      "Epoch 32/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7571 - accuracy: 0.4039 - val_loss: 1.8362 - val_accuracy: 0.3729\n",
      "Epoch 33/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7477 - accuracy: 0.4044 - val_loss: 1.8578 - val_accuracy: 0.3641\n",
      "Epoch 34/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7503 - accuracy: 0.4026 - val_loss: 1.8678 - val_accuracy: 0.3621\n",
      "Epoch 35/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7474 - accuracy: 0.4024 - val_loss: 1.7968 - val_accuracy: 0.3874\n",
      "Epoch 36/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7405 - accuracy: 0.4075 - val_loss: 1.8648 - val_accuracy: 0.3711\n",
      "Epoch 37/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7586 - accuracy: 0.4037 - val_loss: 1.8781 - val_accuracy: 0.3674\n",
      "Epoch 38/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7678 - accuracy: 0.4067 - val_loss: 1.9220 - val_accuracy: 0.3771\n",
      "Epoch 39/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7528 - accuracy: 0.4063 - val_loss: 1.8848 - val_accuracy: 0.3495\n",
      "Epoch 40/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7294 - accuracy: 0.4111 - val_loss: 1.9015 - val_accuracy: 0.3562\n",
      "Epoch 41/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7507 - accuracy: 0.4073 - val_loss: 1.9222 - val_accuracy: 0.3441\n",
      "Epoch 42/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7395 - accuracy: 0.4099 - val_loss: 1.8814 - val_accuracy: 0.3666\n",
      "Epoch 43/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7345 - accuracy: 0.4106 - val_loss: 1.8228 - val_accuracy: 0.3823\n",
      "Epoch 44/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7444 - accuracy: 0.4086 - val_loss: 1.8312 - val_accuracy: 0.3833\n",
      "Epoch 45/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7506 - accuracy: 0.4044 - val_loss: 1.8212 - val_accuracy: 0.3809\n",
      "Epoch 46/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7500 - accuracy: 0.4053 - val_loss: 1.8523 - val_accuracy: 0.3754\n",
      "Epoch 47/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7366 - accuracy: 0.4122 - val_loss: 1.9702 - val_accuracy: 0.3570\n",
      "Epoch 48/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7377 - accuracy: 0.4092 - val_loss: 2.0739 - val_accuracy: 0.3338\n",
      "Epoch 49/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7450 - accuracy: 0.4097 - val_loss: 1.9676 - val_accuracy: 0.3555\n",
      "Epoch 50/50\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7341 - accuracy: 0.4125 - val_loss: 2.0708 - val_accuracy: 0.3118\n",
      "313/313 [==============================] - 0s 926us/step - loss: 2.0708 - accuracy: 0.3118\n"
     ]
    }
   ],
   "source": [
    "history1 = model1.fit(train_images, train_labels, epochs=50,\n",
    "                    validation_data=(test_images, test_labels))\n",
    "\n",
    "test_loss1, test_acc1 = model1.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1381b2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Do this with 0,1,2 Hidden layers\n",
    "model2 = models.Sequential([\n",
    "    layers.Flatten(), # Input layer doesn't count\n",
    "    layers.Dense(500, activation='relu'), # 1 Hidden layer\n",
    "    layers.Dense(10, activation='softmax') # end with softmax for classification also the final layer isn't part of a hidden layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed7413c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fa392af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.0873 - accuracy: 0.2835 - val_loss: 1.7366 - val_accuracy: 0.3761\n",
      "Epoch 2/50\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.7234 - accuracy: 0.3847 - val_loss: 1.6613 - val_accuracy: 0.4105\n",
      "Epoch 3/50\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.6568 - accuracy: 0.4121 - val_loss: 1.5957 - val_accuracy: 0.4383\n",
      "Epoch 4/50\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.6139 - accuracy: 0.4251 - val_loss: 1.6235 - val_accuracy: 0.4241\n",
      "Epoch 5/50\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.5683 - accuracy: 0.4436 - val_loss: 1.5765 - val_accuracy: 0.4430\n",
      "Epoch 6/50\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.5429 - accuracy: 0.4530 - val_loss: 1.5232 - val_accuracy: 0.4652\n",
      "Epoch 7/50\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.5198 - accuracy: 0.4589 - val_loss: 1.5331 - val_accuracy: 0.4630\n",
      "Epoch 8/50\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.4992 - accuracy: 0.4667 - val_loss: 1.5143 - val_accuracy: 0.4632\n",
      "Epoch 9/50\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4907 - accuracy: 0.4701 - val_loss: 1.5411 - val_accuracy: 0.4566\n",
      "Epoch 10/50\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4822 - accuracy: 0.4748 - val_loss: 1.5340 - val_accuracy: 0.4548\n",
      "Epoch 11/50\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4768 - accuracy: 0.4749 - val_loss: 1.4983 - val_accuracy: 0.4649\n",
      "Epoch 12/50\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4658 - accuracy: 0.4782 - val_loss: 1.5467 - val_accuracy: 0.4572\n",
      "Epoch 13/50\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.4525 - accuracy: 0.4828 - val_loss: 1.5386 - val_accuracy: 0.4580\n",
      "Epoch 14/50\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4436 - accuracy: 0.4876 - val_loss: 1.4987 - val_accuracy: 0.4708\n",
      "Epoch 15/50\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.4417 - accuracy: 0.4873 - val_loss: 1.5547 - val_accuracy: 0.4447\n",
      "Epoch 16/50\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.4350 - accuracy: 0.4932 - val_loss: 1.5164 - val_accuracy: 0.4560\n",
      "Epoch 17/50\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.4257 - accuracy: 0.4931 - val_loss: 1.5389 - val_accuracy: 0.4517\n",
      "Epoch 18/50\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.4179 - accuracy: 0.4929 - val_loss: 1.4931 - val_accuracy: 0.4719\n",
      "Epoch 19/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4107 - accuracy: 0.4969 - val_loss: 1.5114 - val_accuracy: 0.4579\n",
      "Epoch 20/50\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3989 - accuracy: 0.4985 - val_loss: 1.4774 - val_accuracy: 0.4782\n",
      "Epoch 21/50\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3941 - accuracy: 0.5009 - val_loss: 1.4947 - val_accuracy: 0.4739\n",
      "Epoch 22/50\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3855 - accuracy: 0.5028 - val_loss: 1.4904 - val_accuracy: 0.4713\n",
      "Epoch 23/50\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3958 - accuracy: 0.5004 - val_loss: 1.5062 - val_accuracy: 0.4744\n",
      "Epoch 24/50\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3855 - accuracy: 0.5072 - val_loss: 1.5121 - val_accuracy: 0.4591\n",
      "Epoch 25/50\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3743 - accuracy: 0.5085 - val_loss: 1.4729 - val_accuracy: 0.4773\n",
      "Epoch 26/50\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3733 - accuracy: 0.5105 - val_loss: 1.4693 - val_accuracy: 0.4804\n",
      "Epoch 27/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3662 - accuracy: 0.5118 - val_loss: 1.4760 - val_accuracy: 0.4775\n",
      "Epoch 28/50\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3548 - accuracy: 0.5157 - val_loss: 1.5677 - val_accuracy: 0.4467\n",
      "Epoch 29/50\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3602 - accuracy: 0.5150 - val_loss: 1.4930 - val_accuracy: 0.4741\n",
      "Epoch 30/50\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3706 - accuracy: 0.5137 - val_loss: 1.4790 - val_accuracy: 0.4770\n",
      "Epoch 31/50\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3461 - accuracy: 0.5212 - val_loss: 1.4943 - val_accuracy: 0.4710\n",
      "Epoch 32/50\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3403 - accuracy: 0.5206 - val_loss: 1.4846 - val_accuracy: 0.4760\n",
      "Epoch 33/50\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3527 - accuracy: 0.5166 - val_loss: 1.4878 - val_accuracy: 0.4819\n",
      "Epoch 34/50\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3358 - accuracy: 0.5213 - val_loss: 1.6023 - val_accuracy: 0.4341\n",
      "Epoch 35/50\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3401 - accuracy: 0.5255 - val_loss: 1.5289 - val_accuracy: 0.4609\n",
      "Epoch 36/50\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3326 - accuracy: 0.5219 - val_loss: 1.4782 - val_accuracy: 0.4752\n",
      "Epoch 37/50\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3332 - accuracy: 0.5207 - val_loss: 1.5095 - val_accuracy: 0.4619\n",
      "Epoch 38/50\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3264 - accuracy: 0.5263 - val_loss: 1.4912 - val_accuracy: 0.4782\n",
      "Epoch 39/50\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3233 - accuracy: 0.5240 - val_loss: 1.5032 - val_accuracy: 0.4779\n",
      "Epoch 40/50\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3286 - accuracy: 0.5273 - val_loss: 1.5015 - val_accuracy: 0.4752\n",
      "Epoch 41/50\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3325 - accuracy: 0.5224 - val_loss: 1.4932 - val_accuracy: 0.4789\n",
      "Epoch 42/50\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3215 - accuracy: 0.5318 - val_loss: 1.5303 - val_accuracy: 0.4626\n",
      "Epoch 43/50\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3152 - accuracy: 0.5294 - val_loss: 1.4848 - val_accuracy: 0.4760\n",
      "Epoch 44/50\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3186 - accuracy: 0.5324 - val_loss: 1.5372 - val_accuracy: 0.4734\n",
      "Epoch 45/50\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3061 - accuracy: 0.5312 - val_loss: 1.5527 - val_accuracy: 0.4680\n",
      "Epoch 46/50\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3127 - accuracy: 0.5297 - val_loss: 1.5008 - val_accuracy: 0.4745\n",
      "Epoch 47/50\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3080 - accuracy: 0.5312 - val_loss: 1.4979 - val_accuracy: 0.4768\n",
      "Epoch 48/50\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3008 - accuracy: 0.5350 - val_loss: 1.4979 - val_accuracy: 0.4764\n",
      "Epoch 49/50\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3065 - accuracy: 0.5360 - val_loss: 1.4911 - val_accuracy: 0.4804\n",
      "Epoch 50/50\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2999 - accuracy: 0.5387 - val_loss: 1.4930 - val_accuracy: 0.4766\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 1.4930 - accuracy: 0.4766\n"
     ]
    }
   ],
   "source": [
    "history2 = model2.fit(train_images, train_labels, epochs=50,\n",
    "                    validation_data=(test_images, test_labels))\n",
    "\n",
    "test_loss2, test_acc2 = model2.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5157d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = models.Sequential([\n",
    "    layers.Flatten(), # Input layer doesn't count\n",
    "    layers.Dense(500, activation='relu'), # Hidden layer 1\n",
    "    layers.Dense(250, activation='relu'), # Hidden layer 2\n",
    "    layers.Dense(10, activation='softmax') # end with softmax for classification also the final layer isn't part of a hidden layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7a9aadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96fb7e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 1.9962 - accuracy: 0.2866 - val_loss: 1.6984 - val_accuracy: 0.3978\n",
      "Epoch 2/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6806 - accuracy: 0.3963 - val_loss: 1.6290 - val_accuracy: 0.4210\n",
      "Epoch 3/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5900 - accuracy: 0.4312 - val_loss: 1.5503 - val_accuracy: 0.4431\n",
      "Epoch 4/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5319 - accuracy: 0.4486 - val_loss: 1.5350 - val_accuracy: 0.4518\n",
      "Epoch 5/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4921 - accuracy: 0.4644 - val_loss: 1.4893 - val_accuracy: 0.4721\n",
      "Epoch 6/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4598 - accuracy: 0.4741 - val_loss: 1.5240 - val_accuracy: 0.4476\n",
      "Epoch 7/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4322 - accuracy: 0.4864 - val_loss: 1.4687 - val_accuracy: 0.4743\n",
      "Epoch 8/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4134 - accuracy: 0.4913 - val_loss: 1.4706 - val_accuracy: 0.4775\n",
      "Epoch 9/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4003 - accuracy: 0.5007 - val_loss: 1.4624 - val_accuracy: 0.4792\n",
      "Epoch 10/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3737 - accuracy: 0.5075 - val_loss: 1.4606 - val_accuracy: 0.4797\n",
      "Epoch 11/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3668 - accuracy: 0.5111 - val_loss: 1.4516 - val_accuracy: 0.4856\n",
      "Epoch 12/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3341 - accuracy: 0.5225 - val_loss: 1.4917 - val_accuracy: 0.4762\n",
      "Epoch 13/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3304 - accuracy: 0.5256 - val_loss: 1.4299 - val_accuracy: 0.4952\n",
      "Epoch 14/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3134 - accuracy: 0.5297 - val_loss: 1.4550 - val_accuracy: 0.4903\n",
      "Epoch 15/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3017 - accuracy: 0.5325 - val_loss: 1.4664 - val_accuracy: 0.4890\n",
      "Epoch 16/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2855 - accuracy: 0.5372 - val_loss: 1.4439 - val_accuracy: 0.4923\n",
      "Epoch 17/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2882 - accuracy: 0.5395 - val_loss: 1.4482 - val_accuracy: 0.4947\n",
      "Epoch 18/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2610 - accuracy: 0.5471 - val_loss: 1.5358 - val_accuracy: 0.4735\n",
      "Epoch 19/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2715 - accuracy: 0.5443 - val_loss: 1.5035 - val_accuracy: 0.4716\n",
      "Epoch 20/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2527 - accuracy: 0.5513 - val_loss: 1.4617 - val_accuracy: 0.4852\n",
      "Epoch 21/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2422 - accuracy: 0.5558 - val_loss: 1.4974 - val_accuracy: 0.4781\n",
      "Epoch 22/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2397 - accuracy: 0.5571 - val_loss: 1.4552 - val_accuracy: 0.4951\n",
      "Epoch 23/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2221 - accuracy: 0.5590 - val_loss: 1.4902 - val_accuracy: 0.4789\n",
      "Epoch 24/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2183 - accuracy: 0.5668 - val_loss: 1.4466 - val_accuracy: 0.4960\n",
      "Epoch 25/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2139 - accuracy: 0.5620 - val_loss: 1.4494 - val_accuracy: 0.5019\n",
      "Epoch 26/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2000 - accuracy: 0.5700 - val_loss: 1.4632 - val_accuracy: 0.4965\n",
      "Epoch 27/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1933 - accuracy: 0.5733 - val_loss: 1.4588 - val_accuracy: 0.4975\n",
      "Epoch 28/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1869 - accuracy: 0.5726 - val_loss: 1.4813 - val_accuracy: 0.4944\n",
      "Epoch 29/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1895 - accuracy: 0.5734 - val_loss: 1.5006 - val_accuracy: 0.4870\n",
      "Epoch 30/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1783 - accuracy: 0.5759 - val_loss: 1.4549 - val_accuracy: 0.5011\n",
      "Epoch 31/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1784 - accuracy: 0.5758 - val_loss: 1.5131 - val_accuracy: 0.4873\n",
      "Epoch 32/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1773 - accuracy: 0.5788 - val_loss: 1.4970 - val_accuracy: 0.4964\n",
      "Epoch 33/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1615 - accuracy: 0.5843 - val_loss: 1.5506 - val_accuracy: 0.4849\n",
      "Epoch 34/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1709 - accuracy: 0.5776 - val_loss: 1.5276 - val_accuracy: 0.4852\n",
      "Epoch 35/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1512 - accuracy: 0.5848 - val_loss: 1.5192 - val_accuracy: 0.4930\n",
      "Epoch 36/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1386 - accuracy: 0.5909 - val_loss: 1.5081 - val_accuracy: 0.4926\n",
      "Epoch 37/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1499 - accuracy: 0.5884 - val_loss: 1.5412 - val_accuracy: 0.4883\n",
      "Epoch 38/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1369 - accuracy: 0.5920 - val_loss: 1.5298 - val_accuracy: 0.4929\n",
      "Epoch 39/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1388 - accuracy: 0.5913 - val_loss: 1.5341 - val_accuracy: 0.4877\n",
      "Epoch 40/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1306 - accuracy: 0.5949 - val_loss: 1.5784 - val_accuracy: 0.4900\n",
      "Epoch 41/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1162 - accuracy: 0.5944 - val_loss: 1.5924 - val_accuracy: 0.4850\n",
      "Epoch 42/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1219 - accuracy: 0.5988 - val_loss: 1.5109 - val_accuracy: 0.4989\n",
      "Epoch 43/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1053 - accuracy: 0.6056 - val_loss: 1.5409 - val_accuracy: 0.4982\n",
      "Epoch 44/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1141 - accuracy: 0.6024 - val_loss: 1.5464 - val_accuracy: 0.4937\n",
      "Epoch 45/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1201 - accuracy: 0.5957 - val_loss: 1.5205 - val_accuracy: 0.4983\n",
      "Epoch 46/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0968 - accuracy: 0.6050 - val_loss: 1.5235 - val_accuracy: 0.4880\n",
      "Epoch 47/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0908 - accuracy: 0.6086 - val_loss: 1.5405 - val_accuracy: 0.4935\n",
      "Epoch 48/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0921 - accuracy: 0.6093 - val_loss: 1.5329 - val_accuracy: 0.4985\n",
      "Epoch 49/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0947 - accuracy: 0.6058 - val_loss: 1.5949 - val_accuracy: 0.4848\n",
      "Epoch 50/50\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0888 - accuracy: 0.6120 - val_loss: 1.5902 - val_accuracy: 0.4847\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.5902 - accuracy: 0.4847\n"
     ]
    }
   ],
   "source": [
    "history3 = model3.fit(train_images, train_labels, epochs=50,\n",
    "                    validation_data=(test_images, test_labels))\n",
    "\n",
    "test_loss3, test_acc3 = model3.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f3ba29",
   "metadata": {},
   "source": [
    "2- Reuse the code from part 1 to build and run a MLP with one hidden layer as big a you can. \n",
    "Compare the performance of your design with the results appearing in Table 1 of [https://arxiv.org/pdf/1611.03530.pdf] for a MLP of 512 units in a single \n",
    "hidden layer. Report the best result found for a maximum of 1000 epochs or 2 hrs CPU running time.\n",
    "The best accuracy amongst all teams will be awarded extra points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b01d1283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do they mean with as big as you can\n",
    "# Add dropout, batchnorm?, minibatches?, regurilazation and data augment!\n",
    "model_big = models.Sequential([\n",
    "    layers.Flatten(), # Input layer doesn't count\n",
    "    layers.AlphaDropout(0.2),\n",
    "    layers.Dense(512), # 1 Hidden layer\n",
    "    layers.LeakyReLU(),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(10, activation='softmax') # end with softmax for classification also the final layer isn't part of a hidden layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc8cfe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_big.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2818df7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 2.2233 - accuracy: 0.2478 - val_loss: 2.0145 - val_accuracy: 0.3180\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 1.9935 - accuracy: 0.2809 - val_loss: 1.9724 - val_accuracy: 0.3105\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 1.9583 - accuracy: 0.2938 - val_loss: 1.9089 - val_accuracy: 0.3259\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 1.9345 - accuracy: 0.3027 - val_loss: 1.7884 - val_accuracy: 0.3595\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 1.9149 - accuracy: 0.3105 - val_loss: 1.7859 - val_accuracy: 0.3663\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 1.8954 - accuracy: 0.3191 - val_loss: 1.8390 - val_accuracy: 0.3608\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 1.8872 - accuracy: 0.3225 - val_loss: 1.7561 - val_accuracy: 0.3818\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.8788 - accuracy: 0.3244 - val_loss: 1.7840 - val_accuracy: 0.3777\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.8684 - accuracy: 0.3289 - val_loss: 1.7188 - val_accuracy: 0.3949\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.8640 - accuracy: 0.3309 - val_loss: 1.7572 - val_accuracy: 0.3841\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.8579 - accuracy: 0.3342 - val_loss: 1.7238 - val_accuracy: 0.3929\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.8493 - accuracy: 0.3391 - val_loss: 1.7457 - val_accuracy: 0.3800\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.8446 - accuracy: 0.3381 - val_loss: 1.7141 - val_accuracy: 0.4001\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.8373 - accuracy: 0.3409 - val_loss: 1.7086 - val_accuracy: 0.3976\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.8364 - accuracy: 0.3428 - val_loss: 1.6871 - val_accuracy: 0.4083\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.8362 - accuracy: 0.3407 - val_loss: 1.6957 - val_accuracy: 0.4009\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.8281 - accuracy: 0.3453 - val_loss: 1.6977 - val_accuracy: 0.4007\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.8257 - accuracy: 0.3454 - val_loss: 1.6729 - val_accuracy: 0.4063\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.8220 - accuracy: 0.3472 - val_loss: 1.6719 - val_accuracy: 0.4073\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.8217 - accuracy: 0.3484 - val_loss: 1.6901 - val_accuracy: 0.4087\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.8214 - accuracy: 0.3478 - val_loss: 1.7135 - val_accuracy: 0.3993\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.8121 - accuracy: 0.3528 - val_loss: 1.6768 - val_accuracy: 0.4093\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.8157 - accuracy: 0.3504 - val_loss: 1.7064 - val_accuracy: 0.4084\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.8105 - accuracy: 0.3496 - val_loss: 1.6925 - val_accuracy: 0.4102\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 1.8066 - accuracy: 0.3534 - val_loss: 1.6490 - val_accuracy: 0.4192\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 1.8088 - accuracy: 0.3503 - val_loss: 1.6591 - val_accuracy: 0.4133\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 1.8092 - accuracy: 0.3524 - val_loss: 1.6711 - val_accuracy: 0.4169\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 1.8070 - accuracy: 0.3517 - val_loss: 1.6309 - val_accuracy: 0.4276\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 1.7984 - accuracy: 0.3569 - val_loss: 1.6638 - val_accuracy: 0.4229\n",
      "Epoch 30/100\n",
      "1224/1563 [======================>.......] - ETA: 2s - loss: 1.7980 - accuracy: 0.3566"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18028/216565736.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m history_big = model_big.fit(train_images, train_labels, epochs=100,\n\u001b[0m\u001b[0;32m      2\u001b[0m                     validation_data=(test_images, test_labels))\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_loss_big\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc_big\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_big\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ylja0\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ylja0\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ylja0\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ylja0\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ylja0\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\users\\ylja0\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ylja0\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history_big = model_big.fit(train_images, train_labels, epochs=100,\n",
    "                    validation_data=(test_images, test_labels))\n",
    "\n",
    "test_loss_big, test_acc_big = model_big.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e8f90d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
