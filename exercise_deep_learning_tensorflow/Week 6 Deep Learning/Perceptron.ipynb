{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "Perceptron.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2455f690"
      },
      "source": [
        "# Adapted from the code on https://www.tensorflow.org/tutorials/images/cnn \n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models, regularizers"
      ],
      "id": "2455f690",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fcced03"
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0"
      ],
      "id": "0fcced03",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efd558fc"
      },
      "source": [
        "1- Multi Layer Perceptron (MLP). Modify the provided script 'perceptron.py' to build a MLP. Use architectures \n",
        "with 0, 1 and 2 hidden layers. Keep the complexity of the model bounded so runs do not take much more\n",
        "than 1 hour to reach the maximum of testing accuracy. Notice that the input needs to be \"flattened\" since there is no spatial structure \n",
        "in this fully connected design.  This can be achieved by adding a dummy layer with no free parameters with \"layers.Flatten()\"\n",
        "as the first layer in the constructor \"model.Sequential()\". Obtain the learning curves and discuss the results.\n",
        "Report the optimizer in use, initialization parameters, the learning rate, etc. Is early stopping convenient\n",
        "in this model?"
      ],
      "id": "efd558fc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c908543d"
      },
      "source": [
        "YOUR ANWSER HERE: \n",
        "\n",
        "* item Optimizer: Adam \n",
        "* item Initialization parameters: Epochs = 50\n",
        "* item Learning rate: Defaults to 0.001, this learning rate however is tweaked by the adam optimizer.\n",
        "* Is early stopping convenientin this model?: \n",
        "In the case of this model we could say that early stopping could be useful since we don't really learn any information with 0 layers. As can be seen in the results we also dont see any major improvements.\n",
        "* Discuss result: In this model there is no hidden layer to learn on. The final dense layer is the only layer that can influence the result, so it isn't weird to see that the  results here are quite bad.\n"
      ],
      "id": "c908543d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6cb1d6a"
      },
      "source": [
        "# Original\n",
        "model1 = models.Sequential([\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(10, activation='softmax') # end with softmax for classification\n",
        "])"
      ],
      "id": "a6cb1d6a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3daa84e"
      },
      "source": [
        "model1.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "id": "c3daa84e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e7e110a",
        "outputId": "676d5632-be02-4164-f235-35f4de641f30"
      },
      "source": [
        "history1 = model1.fit(train_images, train_labels, epochs=50,\n",
        "                    validation_data=(test_images, test_labels))\n",
        "\n",
        "test_loss1, test_acc1 = model1.evaluate(test_images, test_labels)"
      ],
      "id": "0e7e110a",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.9633 - accuracy: 0.3154 - val_loss: 1.9403 - val_accuracy: 0.3495\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 2s 2ms/step - loss: 1.8807 - accuracy: 0.3516 - val_loss: 1.9325 - val_accuracy: 0.3309\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.8549 - accuracy: 0.3633 - val_loss: 2.0048 - val_accuracy: 0.3578\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.8515 - accuracy: 0.3685 - val_loss: 1.9024 - val_accuracy: 0.3535\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.8280 - accuracy: 0.3735 - val_loss: 1.8070 - val_accuracy: 0.3724\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.8183 - accuracy: 0.3788 - val_loss: 1.9229 - val_accuracy: 0.3435\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.8233 - accuracy: 0.3765 - val_loss: 1.8950 - val_accuracy: 0.3565\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.8104 - accuracy: 0.3830 - val_loss: 1.9101 - val_accuracy: 0.3538\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7967 - accuracy: 0.3877 - val_loss: 1.8916 - val_accuracy: 0.3546\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.8111 - accuracy: 0.3849 - val_loss: 1.8683 - val_accuracy: 0.3606\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7937 - accuracy: 0.3867 - val_loss: 1.9026 - val_accuracy: 0.3504\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7876 - accuracy: 0.3904 - val_loss: 2.0165 - val_accuracy: 0.3229\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7917 - accuracy: 0.3884 - val_loss: 1.9265 - val_accuracy: 0.3380\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7865 - accuracy: 0.3900 - val_loss: 1.9771 - val_accuracy: 0.3254\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7912 - accuracy: 0.3923 - val_loss: 1.8446 - val_accuracy: 0.3614\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7809 - accuracy: 0.3911 - val_loss: 2.0319 - val_accuracy: 0.3277\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7909 - accuracy: 0.3927 - val_loss: 1.9066 - val_accuracy: 0.3425\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7841 - accuracy: 0.3967 - val_loss: 1.7855 - val_accuracy: 0.3748\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 2s 2ms/step - loss: 1.7771 - accuracy: 0.3956 - val_loss: 1.7767 - val_accuracy: 0.3852\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 2s 2ms/step - loss: 1.7743 - accuracy: 0.3992 - val_loss: 1.8297 - val_accuracy: 0.3670\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 2s 2ms/step - loss: 1.7683 - accuracy: 0.4005 - val_loss: 1.9699 - val_accuracy: 0.3389\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7743 - accuracy: 0.3976 - val_loss: 1.9668 - val_accuracy: 0.3446\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7704 - accuracy: 0.3988 - val_loss: 1.8968 - val_accuracy: 0.3589\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7679 - accuracy: 0.3979 - val_loss: 1.9194 - val_accuracy: 0.3432\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 2s 2ms/step - loss: 1.7646 - accuracy: 0.3998 - val_loss: 1.8932 - val_accuracy: 0.3666\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7587 - accuracy: 0.4015 - val_loss: 2.0045 - val_accuracy: 0.3326\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7587 - accuracy: 0.4009 - val_loss: 1.8997 - val_accuracy: 0.3526\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 2s 2ms/step - loss: 1.7631 - accuracy: 0.4025 - val_loss: 1.8834 - val_accuracy: 0.3665\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 2s 2ms/step - loss: 1.7601 - accuracy: 0.4021 - val_loss: 2.1223 - val_accuracy: 0.3201\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7586 - accuracy: 0.4029 - val_loss: 1.8970 - val_accuracy: 0.3630\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 2s 2ms/step - loss: 1.7548 - accuracy: 0.4042 - val_loss: 1.8985 - val_accuracy: 0.3516\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7589 - accuracy: 0.4026 - val_loss: 1.8294 - val_accuracy: 0.3756\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7485 - accuracy: 0.4052 - val_loss: 1.9364 - val_accuracy: 0.3560\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7534 - accuracy: 0.4051 - val_loss: 1.8671 - val_accuracy: 0.3607\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7481 - accuracy: 0.4022 - val_loss: 2.0531 - val_accuracy: 0.3251\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7562 - accuracy: 0.4073 - val_loss: 1.9111 - val_accuracy: 0.3538\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.7478 - accuracy: 0.4076 - val_loss: 1.8690 - val_accuracy: 0.3571\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.7518 - accuracy: 0.4035 - val_loss: 1.8609 - val_accuracy: 0.3613\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.7559 - accuracy: 0.4046 - val_loss: 2.0041 - val_accuracy: 0.3301\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.7460 - accuracy: 0.4085 - val_loss: 1.9644 - val_accuracy: 0.3454\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7598 - accuracy: 0.4036 - val_loss: 1.8763 - val_accuracy: 0.3565\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7430 - accuracy: 0.4088 - val_loss: 1.8619 - val_accuracy: 0.3594\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7472 - accuracy: 0.4074 - val_loss: 2.0144 - val_accuracy: 0.3468\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7459 - accuracy: 0.4071 - val_loss: 1.8718 - val_accuracy: 0.3609\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7427 - accuracy: 0.4080 - val_loss: 1.9017 - val_accuracy: 0.3543\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 2s 2ms/step - loss: 1.7428 - accuracy: 0.4067 - val_loss: 1.8509 - val_accuracy: 0.3637\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7463 - accuracy: 0.4091 - val_loss: 1.9570 - val_accuracy: 0.3402\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7470 - accuracy: 0.4076 - val_loss: 1.8238 - val_accuracy: 0.3775\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7461 - accuracy: 0.4083 - val_loss: 1.8717 - val_accuracy: 0.3690\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 1.7477 - accuracy: 0.4083 - val_loss: 1.9488 - val_accuracy: 0.3320\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 1.9488 - accuracy: 0.3320\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA92reZC2idv"
      },
      "source": [
        "YOUR ANWSER HERE: \n",
        "\n",
        "* item Optimizer: Adam \n",
        "* item Initialization parameters: Epochs = 50\n",
        "* item Learning rate: Defaults to 0.001, this learning rate however is tweaked by the adam optimizer.\n",
        "* Is early stopping convenient in this model?: \n",
        "In this example we can see that the model gets stuck on around 0.47-0.48.  We could say that theoratically the model could improve further but it shows no sign of improving drastically, so early stopping could be  usefull.\n",
        "* Discuss result: We can already see  the difference with this model with one hidden layer and  the model with 0 hidden layers. with this extra layer and no reguralization we alrady get a ~ 12% higher val_accuracy.\n"
      ],
      "id": "kA92reZC2idv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c510e1c"
      },
      "source": [
        "# TODO: Do this with 0,1,2 Hidden layers\n",
        "model2 = models.Sequential([\n",
        "    layers.Flatten(), # Input layer doesn't count\n",
        "    layers.Dense(512, activation='relu'), # 1 Hidden layer\n",
        "    layers.Dense(10, activation='softmax') # end with softmax for classification also the final layer isn't part of a hidden layer\n",
        "])"
      ],
      "id": "7c510e1c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9ea3b02"
      },
      "source": [
        "model2.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "id": "b9ea3b02",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4bb5b24",
        "outputId": "8b4ff2be-7dc2-4979-8433-0c686f14c471"
      },
      "source": [
        "history2 = model2.fit(train_images, train_labels, epochs=50,\n",
        "                    validation_data=(test_images, test_labels))\n",
        "\n",
        "test_loss2, test_acc2 = model2.evaluate(test_images, test_labels)"
      ],
      "id": "d4bb5b24",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.8858 - accuracy: 0.3276 - val_loss: 1.7161 - val_accuracy: 0.3785\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.7052 - accuracy: 0.3891 - val_loss: 1.6681 - val_accuracy: 0.4013\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.6402 - accuracy: 0.4179 - val_loss: 1.6043 - val_accuracy: 0.4310\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.5904 - accuracy: 0.4363 - val_loss: 1.5902 - val_accuracy: 0.4298\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.5622 - accuracy: 0.4455 - val_loss: 1.5727 - val_accuracy: 0.4370\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.5382 - accuracy: 0.4551 - val_loss: 1.5379 - val_accuracy: 0.4570\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.5164 - accuracy: 0.4613 - val_loss: 1.5500 - val_accuracy: 0.4499\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.5007 - accuracy: 0.4666 - val_loss: 1.5441 - val_accuracy: 0.4513\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.4879 - accuracy: 0.4748 - val_loss: 1.5516 - val_accuracy: 0.4517\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.4725 - accuracy: 0.4761 - val_loss: 1.5216 - val_accuracy: 0.4537\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 1.4696 - accuracy: 0.4773 - val_loss: 1.5319 - val_accuracy: 0.4583\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.4519 - accuracy: 0.4838 - val_loss: 1.5581 - val_accuracy: 0.4571\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.4489 - accuracy: 0.4855 - val_loss: 1.5229 - val_accuracy: 0.4591\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 15s 10ms/step - loss: 1.4361 - accuracy: 0.4892 - val_loss: 1.4967 - val_accuracy: 0.4753\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.4285 - accuracy: 0.4929 - val_loss: 1.4941 - val_accuracy: 0.4668\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.4208 - accuracy: 0.4941 - val_loss: 1.4705 - val_accuracy: 0.4863\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.4171 - accuracy: 0.4958 - val_loss: 1.4798 - val_accuracy: 0.4770\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.4077 - accuracy: 0.5000 - val_loss: 1.5272 - val_accuracy: 0.4688\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.4004 - accuracy: 0.5019 - val_loss: 1.4936 - val_accuracy: 0.4672\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3969 - accuracy: 0.5043 - val_loss: 1.5013 - val_accuracy: 0.4751\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 12s 7ms/step - loss: 1.3938 - accuracy: 0.5030 - val_loss: 1.4892 - val_accuracy: 0.4762\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3890 - accuracy: 0.5045 - val_loss: 1.5129 - val_accuracy: 0.4665\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.3823 - accuracy: 0.5087 - val_loss: 1.5016 - val_accuracy: 0.4711\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3752 - accuracy: 0.5114 - val_loss: 1.4769 - val_accuracy: 0.4835\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3702 - accuracy: 0.5112 - val_loss: 1.4740 - val_accuracy: 0.4780\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3728 - accuracy: 0.5124 - val_loss: 1.5024 - val_accuracy: 0.4671\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3634 - accuracy: 0.5156 - val_loss: 1.4966 - val_accuracy: 0.4765\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3609 - accuracy: 0.5154 - val_loss: 1.5060 - val_accuracy: 0.4704\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3553 - accuracy: 0.5187 - val_loss: 1.4812 - val_accuracy: 0.4782\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3516 - accuracy: 0.5196 - val_loss: 1.5311 - val_accuracy: 0.4667\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.3473 - accuracy: 0.5229 - val_loss: 1.4850 - val_accuracy: 0.4825\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.3442 - accuracy: 0.5221 - val_loss: 1.5150 - val_accuracy: 0.4644\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 19s 12ms/step - loss: 1.3417 - accuracy: 0.5229 - val_loss: 1.4945 - val_accuracy: 0.4681\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 1.3369 - accuracy: 0.5230 - val_loss: 1.4739 - val_accuracy: 0.4810\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3321 - accuracy: 0.5238 - val_loss: 1.4849 - val_accuracy: 0.4774\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 12s 7ms/step - loss: 1.3363 - accuracy: 0.5229 - val_loss: 1.5234 - val_accuracy: 0.4654\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3282 - accuracy: 0.5258 - val_loss: 1.4833 - val_accuracy: 0.4749\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3248 - accuracy: 0.5302 - val_loss: 1.4946 - val_accuracy: 0.4725\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3210 - accuracy: 0.5288 - val_loss: 1.5195 - val_accuracy: 0.4702\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3165 - accuracy: 0.5321 - val_loss: 1.5222 - val_accuracy: 0.4701\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3144 - accuracy: 0.5317 - val_loss: 1.4894 - val_accuracy: 0.4776\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3171 - accuracy: 0.5289 - val_loss: 1.5031 - val_accuracy: 0.4728\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3098 - accuracy: 0.5336 - val_loss: 1.5044 - val_accuracy: 0.4761\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3118 - accuracy: 0.5323 - val_loss: 1.5164 - val_accuracy: 0.4724\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3034 - accuracy: 0.5338 - val_loss: 1.5013 - val_accuracy: 0.4744\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3006 - accuracy: 0.5362 - val_loss: 1.4854 - val_accuracy: 0.4749\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3043 - accuracy: 0.5361 - val_loss: 1.5531 - val_accuracy: 0.4619\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.2981 - accuracy: 0.5367 - val_loss: 1.5189 - val_accuracy: 0.4722\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.2931 - accuracy: 0.5391 - val_loss: 1.5237 - val_accuracy: 0.4693\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2952 - accuracy: 0.5357 - val_loss: 1.5278 - val_accuracy: 0.4737\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 1.5278 - accuracy: 0.4737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImUZ1DjC3nzI"
      },
      "source": [
        "YOUR ANWSER HERE: \n",
        "\n",
        "* item Optimizer: Adam \n",
        "* item Initialization parameters: Epochs = 50\n",
        "* item Learning rate: Defaults to 0.001, this learning rate however is tweaked by the adam optimizer.\n",
        "* Is early stopping convenientin this model?: \n",
        "In the results we can see that after  around epoch 35 we don't really improve anymore on the val_acc and val_loss.  we do however keep slightly improving on the test_acc and test_loss. this is however not desirable since it could result that overfits. So early stopping could be usefull here.\n",
        "* Discuss result: There isn't much difference between this model and the earlier one with one less hidden layer. we can see that the val_acc is ~ 3% higher, but the biggest noticable difference is between the normal acc which is arround ~ 7% higher. There is a possibility that we need to add reguralization so that we can better learn unseen data  to improve the more important metric of val_acc, or the models will only slightly improve until more hidden layers make it such that the data from test is perfectly learned and we get overfitting. \n"
      ],
      "id": "ImUZ1DjC3nzI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39efca29"
      },
      "source": [
        "model3 = models.Sequential([\n",
        "    layers.Flatten(), # Input layer doesn't count\n",
        "    layers.Dense(512, activation='relu'), # Hidden layer 1\n",
        "    layers.Dense(256, activation='relu'), # Hidden layer 2\n",
        "    layers.Dense(10, activation='softmax') # end with softmax for classification also the final layer isn't part of a hidden layer\n",
        "])"
      ],
      "id": "39efca29",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3edc6c3c"
      },
      "source": [
        "model3.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "id": "3edc6c3c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0339ea9f",
        "outputId": "d173e642-5b38-4938-c9c5-e2732fcebac7"
      },
      "source": [
        "history3 = model3.fit(train_images, train_labels, epochs=50,\n",
        "                    validation_data=(test_images, test_labels))\n",
        "\n",
        "test_loss3, test_acc3 = model3.evaluate(test_images, test_labels)"
      ],
      "id": "0339ea9f",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 14s 8ms/step - loss: 1.8565 - accuracy: 0.3307 - val_loss: 1.7119 - val_accuracy: 0.3808\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.6696 - accuracy: 0.4035 - val_loss: 1.6338 - val_accuracy: 0.4186\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.5896 - accuracy: 0.4336 - val_loss: 1.5865 - val_accuracy: 0.4352\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.5400 - accuracy: 0.4492 - val_loss: 1.5165 - val_accuracy: 0.4566\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.5016 - accuracy: 0.4627 - val_loss: 1.5279 - val_accuracy: 0.4545\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.4734 - accuracy: 0.4741 - val_loss: 1.4916 - val_accuracy: 0.4682\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.4389 - accuracy: 0.4849 - val_loss: 1.5019 - val_accuracy: 0.4693\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.4127 - accuracy: 0.4938 - val_loss: 1.5061 - val_accuracy: 0.4643\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3989 - accuracy: 0.4997 - val_loss: 1.4653 - val_accuracy: 0.4782\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3804 - accuracy: 0.5054 - val_loss: 1.4377 - val_accuracy: 0.4887\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3621 - accuracy: 0.5136 - val_loss: 1.4496 - val_accuracy: 0.4849\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3425 - accuracy: 0.5173 - val_loss: 1.4525 - val_accuracy: 0.4800\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3307 - accuracy: 0.5244 - val_loss: 1.4869 - val_accuracy: 0.4769\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3192 - accuracy: 0.5258 - val_loss: 1.4638 - val_accuracy: 0.4790\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.3092 - accuracy: 0.5308 - val_loss: 1.4595 - val_accuracy: 0.4862\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2947 - accuracy: 0.5364 - val_loss: 1.4584 - val_accuracy: 0.4975\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2886 - accuracy: 0.5376 - val_loss: 1.4502 - val_accuracy: 0.4961\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.2768 - accuracy: 0.5425 - val_loss: 1.4359 - val_accuracy: 0.4971\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2688 - accuracy: 0.5450 - val_loss: 1.4475 - val_accuracy: 0.4964\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2598 - accuracy: 0.5471 - val_loss: 1.4458 - val_accuracy: 0.4944\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.2485 - accuracy: 0.5527 - val_loss: 1.4472 - val_accuracy: 0.4996\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2431 - accuracy: 0.5530 - val_loss: 1.4546 - val_accuracy: 0.4931\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.2305 - accuracy: 0.5593 - val_loss: 1.4550 - val_accuracy: 0.4983\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.2255 - accuracy: 0.5607 - val_loss: 1.4627 - val_accuracy: 0.4920\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.2203 - accuracy: 0.5632 - val_loss: 1.5038 - val_accuracy: 0.4919\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.2112 - accuracy: 0.5646 - val_loss: 1.4781 - val_accuracy: 0.4946\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.2118 - accuracy: 0.5648 - val_loss: 1.4504 - val_accuracy: 0.4995\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1993 - accuracy: 0.5700 - val_loss: 1.4648 - val_accuracy: 0.4959\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1965 - accuracy: 0.5730 - val_loss: 1.4783 - val_accuracy: 0.4973\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1893 - accuracy: 0.5726 - val_loss: 1.4904 - val_accuracy: 0.4900\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1846 - accuracy: 0.5753 - val_loss: 1.4656 - val_accuracy: 0.4991\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1817 - accuracy: 0.5759 - val_loss: 1.4774 - val_accuracy: 0.5045\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1754 - accuracy: 0.5757 - val_loss: 1.4735 - val_accuracy: 0.4984\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1705 - accuracy: 0.5788 - val_loss: 1.4906 - val_accuracy: 0.4988\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1665 - accuracy: 0.5792 - val_loss: 1.4861 - val_accuracy: 0.4983\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1581 - accuracy: 0.5863 - val_loss: 1.4661 - val_accuracy: 0.4994\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1528 - accuracy: 0.5857 - val_loss: 1.4924 - val_accuracy: 0.5008\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.1493 - accuracy: 0.5863 - val_loss: 1.5153 - val_accuracy: 0.4983\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1488 - accuracy: 0.5868 - val_loss: 1.5187 - val_accuracy: 0.4909\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1398 - accuracy: 0.5888 - val_loss: 1.5137 - val_accuracy: 0.4977\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1383 - accuracy: 0.5922 - val_loss: 1.5129 - val_accuracy: 0.4984\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1282 - accuracy: 0.5935 - val_loss: 1.5290 - val_accuracy: 0.4899\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1272 - accuracy: 0.5949 - val_loss: 1.5335 - val_accuracy: 0.5011\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1239 - accuracy: 0.5937 - val_loss: 1.6201 - val_accuracy: 0.4851\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1245 - accuracy: 0.5953 - val_loss: 1.5491 - val_accuracy: 0.4929\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1176 - accuracy: 0.5989 - val_loss: 1.5552 - val_accuracy: 0.4949\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1147 - accuracy: 0.5990 - val_loss: 1.5605 - val_accuracy: 0.4904\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1111 - accuracy: 0.6014 - val_loss: 1.5262 - val_accuracy: 0.5022\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1066 - accuracy: 0.6026 - val_loss: 1.6075 - val_accuracy: 0.4831\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 1.1039 - accuracy: 0.6025 - val_loss: 1.5897 - val_accuracy: 0.4909\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 1.5897 - accuracy: 0.4909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39110700"
      },
      "source": [
        "2- Reuse the code from part 1 to build and run a MLP with one hidden layer as big a you can. \n",
        "Compare the performance of your design with the results appearing in Table 1 of [https://arxiv.org/pdf/1611.03530.pdf] for a MLP of 512 units in a single \n",
        "hidden layer. Report the best result found for a maximum of 1000 epochs or 2 hrs CPU running time.\n",
        "The best accuracy amongst all teams will be awarded extra points."
      ],
      "id": "39110700"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0OGQlHa5cC-"
      },
      "source": [
        "YOUR ANWSER HERE: \n",
        "\n",
        "* item Optimizer: Adam \n",
        "* item Initialization parameters: Epochs = 1000\n",
        "* item Learning rate: Defaults to 0.001, this learning rate however is tweaked by the adam optimizer.\n",
        "* Is early stopping convenientin this model?: We set a patience of 100, such that if the validation loss doesn't improve in 100 epochs we do an early stop.\n",
        "* Discuss result: After experimenting with different setups, we came to the conclusion that our best results lie around ~ 55% accuracy on the validation set(This was done in an earlier experiment, not the one shown below). The accuracy on the test set is of course way higher, but irrelevant.\n"
      ],
      "id": "q0OGQlHa5cC-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea1ebb9e"
      },
      "source": [
        "# Add dropout, batchnorm?, minibatches?, regurilazation and data augment!\n",
        "# Won't add data augment because other models didn't have that as well.\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
        "\n",
        "model_big = models.Sequential([\n",
        "    layers.Flatten(), # Input layer doesn't count\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(1024, activation=\"relu\", kernel_initializer='he_uniform'), # 1 Hidden layer\n",
        "    # layers.BatchNormalization(),\n",
        "    layers.Dense(10, activation='softmax') # end with softmax for classification also the final layer isn't part of a hidden layer\n",
        "])"
      ],
      "id": "ea1ebb9e",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bd5e2eb"
      },
      "source": [
        "model_big.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "id": "9bd5e2eb",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0999afd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42e25f64-2b8e-4db0-d32c-f205dbcba3e5"
      },
      "source": [
        "history_big = model_big.fit(train_images, train_labels, batch_size=512, epochs=1000, callbacks=[callback],\n",
        "                    validation_data=(test_images, test_labels))\n",
        "\n",
        "test_loss_big, test_acc_big = model_big.evaluate(test_images, test_labels)"
      ],
      "id": "0999afd1",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 2.6557 - accuracy: 0.2363 - val_loss: 1.9355 - val_accuracy: 0.3194\n",
            "Epoch 2/1000\n",
            "98/98 [==============================] - 14s 147ms/step - loss: 1.8901 - accuracy: 0.3297 - val_loss: 1.8243 - val_accuracy: 0.3577\n",
            "Epoch 3/1000\n",
            "98/98 [==============================] - 14s 148ms/step - loss: 1.8158 - accuracy: 0.3603 - val_loss: 1.7815 - val_accuracy: 0.3649\n",
            "Epoch 4/1000\n",
            "98/98 [==============================] - 15s 149ms/step - loss: 1.7632 - accuracy: 0.3777 - val_loss: 1.7126 - val_accuracy: 0.4001\n",
            "Epoch 5/1000\n",
            "98/98 [==============================] - 15s 149ms/step - loss: 1.7148 - accuracy: 0.3969 - val_loss: 1.6981 - val_accuracy: 0.4029\n",
            "Epoch 6/1000\n",
            "98/98 [==============================] - 15s 149ms/step - loss: 1.6829 - accuracy: 0.4074 - val_loss: 1.6424 - val_accuracy: 0.4192\n",
            "Epoch 7/1000\n",
            "98/98 [==============================] - 15s 149ms/step - loss: 1.6478 - accuracy: 0.4246 - val_loss: 1.6158 - val_accuracy: 0.4314\n",
            "Epoch 8/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.6281 - accuracy: 0.4263 - val_loss: 1.6001 - val_accuracy: 0.4361\n",
            "Epoch 9/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.6031 - accuracy: 0.4362 - val_loss: 1.5741 - val_accuracy: 0.4479\n",
            "Epoch 10/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.5807 - accuracy: 0.4455 - val_loss: 1.5553 - val_accuracy: 0.4494\n",
            "Epoch 11/1000\n",
            "98/98 [==============================] - 15s 149ms/step - loss: 1.5611 - accuracy: 0.4534 - val_loss: 1.5543 - val_accuracy: 0.4477\n",
            "Epoch 12/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.5555 - accuracy: 0.4518 - val_loss: 1.5643 - val_accuracy: 0.4469\n",
            "Epoch 13/1000\n",
            "98/98 [==============================] - 15s 149ms/step - loss: 1.5399 - accuracy: 0.4576 - val_loss: 1.5176 - val_accuracy: 0.4636\n",
            "Epoch 14/1000\n",
            "98/98 [==============================] - 15s 149ms/step - loss: 1.5231 - accuracy: 0.4645 - val_loss: 1.5231 - val_accuracy: 0.4583\n",
            "Epoch 15/1000\n",
            "98/98 [==============================] - 15s 149ms/step - loss: 1.5072 - accuracy: 0.4689 - val_loss: 1.4929 - val_accuracy: 0.4718\n",
            "Epoch 16/1000\n",
            "98/98 [==============================] - 15s 148ms/step - loss: 1.4941 - accuracy: 0.4741 - val_loss: 1.5008 - val_accuracy: 0.4669\n",
            "Epoch 17/1000\n",
            "98/98 [==============================] - 14s 147ms/step - loss: 1.4892 - accuracy: 0.4757 - val_loss: 1.4905 - val_accuracy: 0.4693\n",
            "Epoch 18/1000\n",
            "98/98 [==============================] - 14s 145ms/step - loss: 1.4790 - accuracy: 0.4780 - val_loss: 1.4817 - val_accuracy: 0.4707\n",
            "Epoch 19/1000\n",
            "98/98 [==============================] - 14s 148ms/step - loss: 1.4727 - accuracy: 0.4801 - val_loss: 1.4783 - val_accuracy: 0.4735\n",
            "Epoch 20/1000\n",
            "98/98 [==============================] - 14s 147ms/step - loss: 1.4625 - accuracy: 0.4826 - val_loss: 1.5015 - val_accuracy: 0.4646\n",
            "Epoch 21/1000\n",
            "98/98 [==============================] - 14s 148ms/step - loss: 1.4530 - accuracy: 0.4862 - val_loss: 1.4564 - val_accuracy: 0.4785\n",
            "Epoch 22/1000\n",
            "98/98 [==============================] - 14s 147ms/step - loss: 1.4388 - accuracy: 0.4923 - val_loss: 1.4570 - val_accuracy: 0.4840\n",
            "Epoch 23/1000\n",
            "98/98 [==============================] - 14s 146ms/step - loss: 1.4306 - accuracy: 0.4962 - val_loss: 1.4473 - val_accuracy: 0.4852\n",
            "Epoch 24/1000\n",
            "98/98 [==============================] - 14s 146ms/step - loss: 1.4224 - accuracy: 0.4971 - val_loss: 1.4702 - val_accuracy: 0.4865\n",
            "Epoch 25/1000\n",
            "98/98 [==============================] - 14s 146ms/step - loss: 1.4100 - accuracy: 0.5009 - val_loss: 1.4320 - val_accuracy: 0.4927\n",
            "Epoch 26/1000\n",
            "98/98 [==============================] - 14s 146ms/step - loss: 1.4101 - accuracy: 0.5018 - val_loss: 1.4386 - val_accuracy: 0.4871\n",
            "Epoch 27/1000\n",
            "98/98 [==============================] - 14s 147ms/step - loss: 1.4063 - accuracy: 0.5033 - val_loss: 1.4519 - val_accuracy: 0.4884\n",
            "Epoch 28/1000\n",
            "98/98 [==============================] - 15s 149ms/step - loss: 1.3986 - accuracy: 0.5047 - val_loss: 1.4168 - val_accuracy: 0.5020\n",
            "Epoch 29/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.3899 - accuracy: 0.5095 - val_loss: 1.4162 - val_accuracy: 0.5015\n",
            "Epoch 30/1000\n",
            "98/98 [==============================] - 15s 149ms/step - loss: 1.3752 - accuracy: 0.5156 - val_loss: 1.4494 - val_accuracy: 0.4884\n",
            "Epoch 31/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.3760 - accuracy: 0.5143 - val_loss: 1.4222 - val_accuracy: 0.4951\n",
            "Epoch 32/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.3729 - accuracy: 0.5167 - val_loss: 1.4085 - val_accuracy: 0.5017\n",
            "Epoch 33/1000\n",
            "98/98 [==============================] - 15s 149ms/step - loss: 1.3636 - accuracy: 0.5185 - val_loss: 1.3920 - val_accuracy: 0.5055\n",
            "Epoch 34/1000\n",
            "98/98 [==============================] - 15s 149ms/step - loss: 1.3547 - accuracy: 0.5217 - val_loss: 1.4199 - val_accuracy: 0.4979\n",
            "Epoch 35/1000\n",
            "98/98 [==============================] - 15s 148ms/step - loss: 1.3588 - accuracy: 0.5224 - val_loss: 1.4064 - val_accuracy: 0.5017\n",
            "Epoch 36/1000\n",
            "98/98 [==============================] - 14s 148ms/step - loss: 1.3481 - accuracy: 0.5228 - val_loss: 1.4001 - val_accuracy: 0.5034\n",
            "Epoch 37/1000\n",
            "98/98 [==============================] - 14s 148ms/step - loss: 1.3426 - accuracy: 0.5248 - val_loss: 1.4064 - val_accuracy: 0.5039\n",
            "Epoch 38/1000\n",
            "98/98 [==============================] - 15s 148ms/step - loss: 1.3369 - accuracy: 0.5272 - val_loss: 1.3986 - val_accuracy: 0.5047\n",
            "Epoch 39/1000\n",
            "98/98 [==============================] - 15s 148ms/step - loss: 1.3376 - accuracy: 0.5283 - val_loss: 1.4018 - val_accuracy: 0.5072\n",
            "Epoch 40/1000\n",
            "98/98 [==============================] - 15s 148ms/step - loss: 1.3311 - accuracy: 0.5310 - val_loss: 1.3890 - val_accuracy: 0.5084\n",
            "Epoch 41/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.3226 - accuracy: 0.5346 - val_loss: 1.3829 - val_accuracy: 0.5092\n",
            "Epoch 42/1000\n",
            "98/98 [==============================] - 15s 149ms/step - loss: 1.3209 - accuracy: 0.5342 - val_loss: 1.4032 - val_accuracy: 0.5033\n",
            "Epoch 43/1000\n",
            "98/98 [==============================] - 15s 149ms/step - loss: 1.3152 - accuracy: 0.5358 - val_loss: 1.3930 - val_accuracy: 0.5076\n",
            "Epoch 44/1000\n",
            "98/98 [==============================] - 15s 149ms/step - loss: 1.3133 - accuracy: 0.5382 - val_loss: 1.4140 - val_accuracy: 0.5027\n",
            "Epoch 45/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.3082 - accuracy: 0.5371 - val_loss: 1.3838 - val_accuracy: 0.5090\n",
            "Epoch 46/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.3006 - accuracy: 0.5399 - val_loss: 1.3923 - val_accuracy: 0.5088\n",
            "Epoch 47/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.3024 - accuracy: 0.5404 - val_loss: 1.3736 - val_accuracy: 0.5152\n",
            "Epoch 48/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.3041 - accuracy: 0.5380 - val_loss: 1.4207 - val_accuracy: 0.5086\n",
            "Epoch 49/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.3004 - accuracy: 0.5385 - val_loss: 1.3690 - val_accuracy: 0.5118\n",
            "Epoch 50/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.2919 - accuracy: 0.5443 - val_loss: 1.3783 - val_accuracy: 0.5143\n",
            "Epoch 51/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.2868 - accuracy: 0.5448 - val_loss: 1.3814 - val_accuracy: 0.5103\n",
            "Epoch 52/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.2879 - accuracy: 0.5433 - val_loss: 1.3863 - val_accuracy: 0.5121\n",
            "Epoch 53/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.2894 - accuracy: 0.5452 - val_loss: 1.3684 - val_accuracy: 0.5171\n",
            "Epoch 54/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.2820 - accuracy: 0.5465 - val_loss: 1.3813 - val_accuracy: 0.5112\n",
            "Epoch 55/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.2826 - accuracy: 0.5447 - val_loss: 1.3851 - val_accuracy: 0.5140\n",
            "Epoch 56/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.2749 - accuracy: 0.5491 - val_loss: 1.3944 - val_accuracy: 0.5118\n",
            "Epoch 57/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.2760 - accuracy: 0.5504 - val_loss: 1.3840 - val_accuracy: 0.5110\n",
            "Epoch 58/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.2659 - accuracy: 0.5546 - val_loss: 1.3602 - val_accuracy: 0.5238\n",
            "Epoch 59/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.2637 - accuracy: 0.5527 - val_loss: 1.3913 - val_accuracy: 0.5120\n",
            "Epoch 60/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.2625 - accuracy: 0.5526 - val_loss: 1.3750 - val_accuracy: 0.5162\n",
            "Epoch 61/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2621 - accuracy: 0.5531 - val_loss: 1.3820 - val_accuracy: 0.5215\n",
            "Epoch 62/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.2544 - accuracy: 0.5546 - val_loss: 1.3502 - val_accuracy: 0.5300\n",
            "Epoch 63/1000\n",
            "98/98 [==============================] - 15s 149ms/step - loss: 1.2531 - accuracy: 0.5552 - val_loss: 1.3644 - val_accuracy: 0.5183\n",
            "Epoch 64/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.2590 - accuracy: 0.5556 - val_loss: 1.3775 - val_accuracy: 0.5152\n",
            "Epoch 65/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.2591 - accuracy: 0.5548 - val_loss: 1.4084 - val_accuracy: 0.5096\n",
            "Epoch 66/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.2549 - accuracy: 0.5574 - val_loss: 1.3792 - val_accuracy: 0.5202\n",
            "Epoch 67/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.2437 - accuracy: 0.5596 - val_loss: 1.3681 - val_accuracy: 0.5184\n",
            "Epoch 68/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.2379 - accuracy: 0.5613 - val_loss: 1.3615 - val_accuracy: 0.5236\n",
            "Epoch 69/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.2359 - accuracy: 0.5624 - val_loss: 1.3788 - val_accuracy: 0.5154\n",
            "Epoch 70/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.2435 - accuracy: 0.5596 - val_loss: 1.3728 - val_accuracy: 0.5179\n",
            "Epoch 71/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.2352 - accuracy: 0.5638 - val_loss: 1.3697 - val_accuracy: 0.5186\n",
            "Epoch 72/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.2334 - accuracy: 0.5642 - val_loss: 1.3618 - val_accuracy: 0.5223\n",
            "Epoch 73/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.2279 - accuracy: 0.5659 - val_loss: 1.3976 - val_accuracy: 0.5140\n",
            "Epoch 74/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.2287 - accuracy: 0.5670 - val_loss: 1.3707 - val_accuracy: 0.5203\n",
            "Epoch 75/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.2235 - accuracy: 0.5677 - val_loss: 1.3742 - val_accuracy: 0.5202\n",
            "Epoch 76/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.2249 - accuracy: 0.5662 - val_loss: 1.3821 - val_accuracy: 0.5162\n",
            "Epoch 77/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.2215 - accuracy: 0.5660 - val_loss: 1.3687 - val_accuracy: 0.5236\n",
            "Epoch 78/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.2165 - accuracy: 0.5677 - val_loss: 1.3827 - val_accuracy: 0.5183\n",
            "Epoch 79/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.2217 - accuracy: 0.5666 - val_loss: 1.3842 - val_accuracy: 0.5135\n",
            "Epoch 80/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.2221 - accuracy: 0.5695 - val_loss: 1.3933 - val_accuracy: 0.5170\n",
            "Epoch 81/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.2131 - accuracy: 0.5693 - val_loss: 1.3704 - val_accuracy: 0.5199\n",
            "Epoch 82/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.2126 - accuracy: 0.5686 - val_loss: 1.3643 - val_accuracy: 0.5213\n",
            "Epoch 83/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.2025 - accuracy: 0.5763 - val_loss: 1.3945 - val_accuracy: 0.5145\n",
            "Epoch 84/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.2079 - accuracy: 0.5711 - val_loss: 1.3716 - val_accuracy: 0.5229\n",
            "Epoch 85/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1998 - accuracy: 0.5750 - val_loss: 1.3757 - val_accuracy: 0.5237\n",
            "Epoch 86/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.2008 - accuracy: 0.5743 - val_loss: 1.3774 - val_accuracy: 0.5189\n",
            "Epoch 87/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.2008 - accuracy: 0.5748 - val_loss: 1.3752 - val_accuracy: 0.5213\n",
            "Epoch 88/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.1978 - accuracy: 0.5745 - val_loss: 1.4078 - val_accuracy: 0.5134\n",
            "Epoch 89/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2000 - accuracy: 0.5747 - val_loss: 1.4039 - val_accuracy: 0.5086\n",
            "Epoch 90/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.1957 - accuracy: 0.5765 - val_loss: 1.3783 - val_accuracy: 0.5223\n",
            "Epoch 91/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.1873 - accuracy: 0.5807 - val_loss: 1.3755 - val_accuracy: 0.5152\n",
            "Epoch 92/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1937 - accuracy: 0.5768 - val_loss: 1.3691 - val_accuracy: 0.5239\n",
            "Epoch 93/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.1862 - accuracy: 0.5812 - val_loss: 1.3849 - val_accuracy: 0.5184\n",
            "Epoch 94/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1898 - accuracy: 0.5782 - val_loss: 1.3852 - val_accuracy: 0.5145\n",
            "Epoch 95/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1907 - accuracy: 0.5773 - val_loss: 1.3583 - val_accuracy: 0.5241\n",
            "Epoch 96/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1850 - accuracy: 0.5805 - val_loss: 1.3651 - val_accuracy: 0.5232\n",
            "Epoch 97/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1883 - accuracy: 0.5796 - val_loss: 1.3870 - val_accuracy: 0.5198\n",
            "Epoch 98/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1791 - accuracy: 0.5830 - val_loss: 1.3880 - val_accuracy: 0.5159\n",
            "Epoch 99/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1863 - accuracy: 0.5778 - val_loss: 1.3770 - val_accuracy: 0.5223\n",
            "Epoch 100/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.1753 - accuracy: 0.5833 - val_loss: 1.4018 - val_accuracy: 0.5126\n",
            "Epoch 101/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1811 - accuracy: 0.5830 - val_loss: 1.3776 - val_accuracy: 0.5160\n",
            "Epoch 102/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1715 - accuracy: 0.5860 - val_loss: 1.3913 - val_accuracy: 0.5123\n",
            "Epoch 103/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.1752 - accuracy: 0.5831 - val_loss: 1.3951 - val_accuracy: 0.5139\n",
            "Epoch 104/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1711 - accuracy: 0.5862 - val_loss: 1.4028 - val_accuracy: 0.5095\n",
            "Epoch 105/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.1717 - accuracy: 0.5848 - val_loss: 1.3728 - val_accuracy: 0.5194\n",
            "Epoch 106/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.1686 - accuracy: 0.5854 - val_loss: 1.3992 - val_accuracy: 0.5143\n",
            "Epoch 107/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.1695 - accuracy: 0.5840 - val_loss: 1.3800 - val_accuracy: 0.5231\n",
            "Epoch 108/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1720 - accuracy: 0.5821 - val_loss: 1.3924 - val_accuracy: 0.5131\n",
            "Epoch 109/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1716 - accuracy: 0.5853 - val_loss: 1.4213 - val_accuracy: 0.5067\n",
            "Epoch 110/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.1612 - accuracy: 0.5865 - val_loss: 1.3809 - val_accuracy: 0.5201\n",
            "Epoch 111/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.1655 - accuracy: 0.5870 - val_loss: 1.3794 - val_accuracy: 0.5227\n",
            "Epoch 112/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.1587 - accuracy: 0.5872 - val_loss: 1.3832 - val_accuracy: 0.5169\n",
            "Epoch 113/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.1610 - accuracy: 0.5875 - val_loss: 1.4028 - val_accuracy: 0.5188\n",
            "Epoch 114/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.1599 - accuracy: 0.5894 - val_loss: 1.3802 - val_accuracy: 0.5286\n",
            "Epoch 115/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.1529 - accuracy: 0.5896 - val_loss: 1.3735 - val_accuracy: 0.5220\n",
            "Epoch 116/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1604 - accuracy: 0.5877 - val_loss: 1.3963 - val_accuracy: 0.5159\n",
            "Epoch 117/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.1549 - accuracy: 0.5905 - val_loss: 1.3858 - val_accuracy: 0.5233\n",
            "Epoch 118/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1519 - accuracy: 0.5893 - val_loss: 1.3973 - val_accuracy: 0.5184\n",
            "Epoch 119/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.1532 - accuracy: 0.5915 - val_loss: 1.3873 - val_accuracy: 0.5176\n",
            "Epoch 120/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1505 - accuracy: 0.5904 - val_loss: 1.4030 - val_accuracy: 0.5179\n",
            "Epoch 121/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.1526 - accuracy: 0.5922 - val_loss: 1.3968 - val_accuracy: 0.5216\n",
            "Epoch 122/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.1530 - accuracy: 0.5911 - val_loss: 1.3852 - val_accuracy: 0.5203\n",
            "Epoch 123/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.1481 - accuracy: 0.5922 - val_loss: 1.3988 - val_accuracy: 0.5189\n",
            "Epoch 124/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.1501 - accuracy: 0.5914 - val_loss: 1.3808 - val_accuracy: 0.5252\n",
            "Epoch 125/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.1497 - accuracy: 0.5910 - val_loss: 1.3971 - val_accuracy: 0.5179\n",
            "Epoch 126/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1395 - accuracy: 0.5944 - val_loss: 1.3911 - val_accuracy: 0.5240\n",
            "Epoch 127/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.1460 - accuracy: 0.5947 - val_loss: 1.3942 - val_accuracy: 0.5213\n",
            "Epoch 128/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1458 - accuracy: 0.5941 - val_loss: 1.3830 - val_accuracy: 0.5285\n",
            "Epoch 129/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.1389 - accuracy: 0.5948 - val_loss: 1.3958 - val_accuracy: 0.5202\n",
            "Epoch 130/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1369 - accuracy: 0.5951 - val_loss: 1.4012 - val_accuracy: 0.5196\n",
            "Epoch 131/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1386 - accuracy: 0.5965 - val_loss: 1.4085 - val_accuracy: 0.5167\n",
            "Epoch 132/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1305 - accuracy: 0.5973 - val_loss: 1.3835 - val_accuracy: 0.5207\n",
            "Epoch 133/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1322 - accuracy: 0.5972 - val_loss: 1.3900 - val_accuracy: 0.5228\n",
            "Epoch 134/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.1384 - accuracy: 0.5959 - val_loss: 1.3980 - val_accuracy: 0.5197\n",
            "Epoch 135/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1306 - accuracy: 0.5984 - val_loss: 1.4042 - val_accuracy: 0.5186\n",
            "Epoch 136/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.1377 - accuracy: 0.5967 - val_loss: 1.4257 - val_accuracy: 0.5123\n",
            "Epoch 137/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.1367 - accuracy: 0.5961 - val_loss: 1.4037 - val_accuracy: 0.5170\n",
            "Epoch 138/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1337 - accuracy: 0.5965 - val_loss: 1.3900 - val_accuracy: 0.5244\n",
            "Epoch 139/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.1356 - accuracy: 0.5954 - val_loss: 1.3932 - val_accuracy: 0.5289\n",
            "Epoch 140/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1353 - accuracy: 0.5966 - val_loss: 1.4671 - val_accuracy: 0.5108\n",
            "Epoch 141/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1387 - accuracy: 0.5921 - val_loss: 1.4044 - val_accuracy: 0.5129\n",
            "Epoch 142/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1276 - accuracy: 0.6005 - val_loss: 1.4443 - val_accuracy: 0.5091\n",
            "Epoch 143/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1277 - accuracy: 0.5971 - val_loss: 1.3845 - val_accuracy: 0.5286\n",
            "Epoch 144/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1296 - accuracy: 0.5978 - val_loss: 1.3928 - val_accuracy: 0.5267\n",
            "Epoch 145/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1272 - accuracy: 0.5997 - val_loss: 1.4175 - val_accuracy: 0.5108\n",
            "Epoch 146/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1253 - accuracy: 0.5982 - val_loss: 1.4142 - val_accuracy: 0.5199\n",
            "Epoch 147/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.1336 - accuracy: 0.5960 - val_loss: 1.4106 - val_accuracy: 0.5181\n",
            "Epoch 148/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1284 - accuracy: 0.5977 - val_loss: 1.4093 - val_accuracy: 0.5162\n",
            "Epoch 149/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1249 - accuracy: 0.6003 - val_loss: 1.3962 - val_accuracy: 0.5203\n",
            "Epoch 150/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.1252 - accuracy: 0.5987 - val_loss: 1.3973 - val_accuracy: 0.5194\n",
            "Epoch 151/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.1193 - accuracy: 0.6007 - val_loss: 1.4033 - val_accuracy: 0.5227\n",
            "Epoch 152/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.1257 - accuracy: 0.5994 - val_loss: 1.4061 - val_accuracy: 0.5202\n",
            "Epoch 153/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1193 - accuracy: 0.6025 - val_loss: 1.3905 - val_accuracy: 0.5271\n",
            "Epoch 154/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1214 - accuracy: 0.6006 - val_loss: 1.3905 - val_accuracy: 0.5262\n",
            "Epoch 155/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1160 - accuracy: 0.6045 - val_loss: 1.4086 - val_accuracy: 0.5204\n",
            "Epoch 156/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1189 - accuracy: 0.6032 - val_loss: 1.4032 - val_accuracy: 0.5281\n",
            "Epoch 157/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1201 - accuracy: 0.6015 - val_loss: 1.3963 - val_accuracy: 0.5257\n",
            "Epoch 158/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1132 - accuracy: 0.6024 - val_loss: 1.4147 - val_accuracy: 0.5176\n",
            "Epoch 159/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.1189 - accuracy: 0.6006 - val_loss: 1.4106 - val_accuracy: 0.5199\n",
            "Epoch 160/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.1157 - accuracy: 0.6040 - val_loss: 1.4054 - val_accuracy: 0.5213\n",
            "Epoch 161/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.1138 - accuracy: 0.6044 - val_loss: 1.4200 - val_accuracy: 0.5172\n",
            "Epoch 162/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.1168 - accuracy: 0.6027 - val_loss: 1.4255 - val_accuracy: 0.5149\n",
            "313/313 [==============================] - 2s 8ms/step - loss: 1.4255 - accuracy: 0.5149\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NT7F9PNo9K7j",
        "outputId": "a197f094-47de-4772-9dfe-9b19150e2e09"
      },
      "source": [
        "maxAcc = np.amax(history_big.history['accuracy'])\n",
        "print('Max element from accuracy : ', maxAcc)\n",
        "\n",
        "maxVal_Acc = np.amax(history_big.history['val_accuracy'])\n",
        "print('Max element from validation accuracy : ', maxVal_Acc)"
      ],
      "id": "NT7F9PNo9K7j",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max element from accuracy :  0.6045399904251099\n",
            "Max element from validation accuracy :  0.5299999713897705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "756db38d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "92d5df41-308c-4274-fe1f-b2d703cbc4c3"
      },
      "source": [
        "plt.plot(history_big.history['accuracy'], label='accuracy')\n",
        "plt.plot(history_big.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.plot(history_big.history['loss'], label='loss')\n",
        "plt.plot(history_big.history['val_loss'], label = 'val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.3, 0.7])\n",
        "plt.legend(loc='lower right')"
      ],
      "id": "756db38d",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f65c4365410>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00e2e370-0bf4-4e9a-90eb-8f4104667fc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "9e7fb0d9-8d1a-4172-fff5-e8507fc21fbc"
      },
      "source": [
        "plt.plot(history_big.history['loss'], label='loss')\n",
        "plt.plot(history_big.history['val_loss'], label = 'val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim([1, 2])\n",
        "plt.legend(loc='lower right')"
      ],
      "id": "00e2e370-0bf4-4e9a-90eb-8f4104667fc7",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f65c42e64d0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDJQ2kf8N_vo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "741f84f8-b75d-420f-b706-e5a6fa2400d3"
      },
      "source": [
        "# Add dropout, batchnorm?, minibatches?, regurilazation and data augment!\n",
        "# Won't add data augment because other models didn't have that as well.\n",
        "\n",
        "# callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=200)\n",
        "\n",
        "model_big_overfit = models.Sequential([\n",
        "    layers.Flatten(), # Input layer doesn't count\n",
        "    layers.Dropout(0.4),\n",
        "    layers.Dense(1024, activation=\"relu\", kernel_initializer='he_uniform'), # 1 Hidden layer\n",
        "    # layers.BatchNormalization(),\n",
        "    layers.Dense(10, activation='softmax') # end with softmax for classification also the final layer isn't part of a hidden layer\n",
        "])\n",
        "\n",
        "model_big_overfit.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history_big_overfit = model_big_overfit.fit(train_images, train_labels, batch_size=512, epochs=1000,\n",
        "                    validation_data=(test_images, test_labels))\n",
        "\n",
        "test_loss_big, test_acc_big = model_big_overfit.evaluate(test_images, test_labels)\n",
        "\n",
        "plt.plot(history_big_overfit.history['accuracy'], label='accuracy')\n",
        "plt.plot(history_big_overfit.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.plot(history_big_overfit.history['loss'], label='loss')\n",
        "plt.plot(history_big_overfit.history['val_loss'], label = 'val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.3, 0.7])\n",
        "plt.legend(loc='lower right')"
      ],
      "id": "nDJQ2kf8N_vo",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "98/98 [==============================] - 16s 153ms/step - loss: 2.5478 - accuracy: 0.2293 - val_loss: 1.9099 - val_accuracy: 0.3146\n",
            "Epoch 2/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.8804 - accuracy: 0.3280 - val_loss: 1.8139 - val_accuracy: 0.3498\n",
            "Epoch 3/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.8098 - accuracy: 0.3543 - val_loss: 1.7622 - val_accuracy: 0.3695\n",
            "Epoch 4/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.7647 - accuracy: 0.3723 - val_loss: 1.7057 - val_accuracy: 0.4004\n",
            "Epoch 5/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.7371 - accuracy: 0.3823 - val_loss: 1.6795 - val_accuracy: 0.4089\n",
            "Epoch 6/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.7056 - accuracy: 0.3959 - val_loss: 1.6539 - val_accuracy: 0.4159\n",
            "Epoch 7/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.6856 - accuracy: 0.4022 - val_loss: 1.6200 - val_accuracy: 0.4321\n",
            "Epoch 8/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.6616 - accuracy: 0.4133 - val_loss: 1.6128 - val_accuracy: 0.4379\n",
            "Epoch 9/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.6485 - accuracy: 0.4159 - val_loss: 1.5951 - val_accuracy: 0.4382\n",
            "Epoch 10/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.6311 - accuracy: 0.4224 - val_loss: 1.5915 - val_accuracy: 0.4383\n",
            "Epoch 11/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.6200 - accuracy: 0.4280 - val_loss: 1.5752 - val_accuracy: 0.4471\n",
            "Epoch 12/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.6062 - accuracy: 0.4300 - val_loss: 1.5560 - val_accuracy: 0.4550\n",
            "Epoch 13/1000\n",
            "98/98 [==============================] - 15s 150ms/step - loss: 1.5976 - accuracy: 0.4335 - val_loss: 1.5593 - val_accuracy: 0.4579\n",
            "Epoch 14/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.5882 - accuracy: 0.4381 - val_loss: 1.5365 - val_accuracy: 0.4620\n",
            "Epoch 15/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.5719 - accuracy: 0.4443 - val_loss: 1.5269 - val_accuracy: 0.4639\n",
            "Epoch 16/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.5708 - accuracy: 0.4440 - val_loss: 1.5270 - val_accuracy: 0.4634\n",
            "Epoch 17/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.5611 - accuracy: 0.4453 - val_loss: 1.5191 - val_accuracy: 0.4616\n",
            "Epoch 18/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.5542 - accuracy: 0.4487 - val_loss: 1.5342 - val_accuracy: 0.4556\n",
            "Epoch 19/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.5495 - accuracy: 0.4506 - val_loss: 1.5057 - val_accuracy: 0.4704\n",
            "Epoch 20/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.5415 - accuracy: 0.4526 - val_loss: 1.5056 - val_accuracy: 0.4713\n",
            "Epoch 21/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.5382 - accuracy: 0.4554 - val_loss: 1.4986 - val_accuracy: 0.4776\n",
            "Epoch 22/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.5298 - accuracy: 0.4561 - val_loss: 1.4913 - val_accuracy: 0.4752\n",
            "Epoch 23/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.5202 - accuracy: 0.4605 - val_loss: 1.4783 - val_accuracy: 0.4800\n",
            "Epoch 24/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.5213 - accuracy: 0.4629 - val_loss: 1.4804 - val_accuracy: 0.4775\n",
            "Epoch 25/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.5062 - accuracy: 0.4655 - val_loss: 1.4839 - val_accuracy: 0.4810\n",
            "Epoch 26/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.5048 - accuracy: 0.4673 - val_loss: 1.4748 - val_accuracy: 0.4802\n",
            "Epoch 27/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.5044 - accuracy: 0.4652 - val_loss: 1.4872 - val_accuracy: 0.4752\n",
            "Epoch 28/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.5001 - accuracy: 0.4665 - val_loss: 1.4789 - val_accuracy: 0.4776\n",
            "Epoch 29/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.4915 - accuracy: 0.4718 - val_loss: 1.4911 - val_accuracy: 0.4736\n",
            "Epoch 30/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.4900 - accuracy: 0.4737 - val_loss: 1.4560 - val_accuracy: 0.4904\n",
            "Epoch 31/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.4837 - accuracy: 0.4715 - val_loss: 1.4688 - val_accuracy: 0.4817\n",
            "Epoch 32/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.4786 - accuracy: 0.4754 - val_loss: 1.4531 - val_accuracy: 0.4863\n",
            "Epoch 33/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.4759 - accuracy: 0.4735 - val_loss: 1.4668 - val_accuracy: 0.4767\n",
            "Epoch 34/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.4749 - accuracy: 0.4753 - val_loss: 1.4712 - val_accuracy: 0.4831\n",
            "Epoch 35/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.4741 - accuracy: 0.4774 - val_loss: 1.4663 - val_accuracy: 0.4780\n",
            "Epoch 36/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.4598 - accuracy: 0.4826 - val_loss: 1.4329 - val_accuracy: 0.4950\n",
            "Epoch 37/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.4575 - accuracy: 0.4835 - val_loss: 1.4634 - val_accuracy: 0.4783\n",
            "Epoch 38/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.4535 - accuracy: 0.4821 - val_loss: 1.4369 - val_accuracy: 0.4963\n",
            "Epoch 39/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.4486 - accuracy: 0.4842 - val_loss: 1.4507 - val_accuracy: 0.4913\n",
            "Epoch 40/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.4502 - accuracy: 0.4831 - val_loss: 1.4346 - val_accuracy: 0.4963\n",
            "Epoch 41/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.4425 - accuracy: 0.4869 - val_loss: 1.4360 - val_accuracy: 0.4929\n",
            "Epoch 42/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.4415 - accuracy: 0.4858 - val_loss: 1.4317 - val_accuracy: 0.4947\n",
            "Epoch 43/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.4407 - accuracy: 0.4898 - val_loss: 1.4249 - val_accuracy: 0.4990\n",
            "Epoch 44/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.4361 - accuracy: 0.4880 - val_loss: 1.4389 - val_accuracy: 0.4942\n",
            "Epoch 45/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.4353 - accuracy: 0.4897 - val_loss: 1.4354 - val_accuracy: 0.4888\n",
            "Epoch 46/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.4313 - accuracy: 0.4894 - val_loss: 1.4224 - val_accuracy: 0.5013\n",
            "Epoch 47/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.4262 - accuracy: 0.4921 - val_loss: 1.4317 - val_accuracy: 0.4912\n",
            "Epoch 48/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.4230 - accuracy: 0.4961 - val_loss: 1.4199 - val_accuracy: 0.4987\n",
            "Epoch 49/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.4212 - accuracy: 0.4964 - val_loss: 1.4181 - val_accuracy: 0.4983\n",
            "Epoch 50/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.4179 - accuracy: 0.4952 - val_loss: 1.4222 - val_accuracy: 0.4966\n",
            "Epoch 51/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.4158 - accuracy: 0.4966 - val_loss: 1.4226 - val_accuracy: 0.4981\n",
            "Epoch 52/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.4097 - accuracy: 0.4973 - val_loss: 1.4256 - val_accuracy: 0.4979\n",
            "Epoch 53/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.4109 - accuracy: 0.4981 - val_loss: 1.4291 - val_accuracy: 0.5007\n",
            "Epoch 54/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.4070 - accuracy: 0.4994 - val_loss: 1.4079 - val_accuracy: 0.5038\n",
            "Epoch 55/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.4113 - accuracy: 0.5007 - val_loss: 1.4159 - val_accuracy: 0.4979\n",
            "Epoch 56/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.4062 - accuracy: 0.4979 - val_loss: 1.4130 - val_accuracy: 0.5016\n",
            "Epoch 57/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.4043 - accuracy: 0.5003 - val_loss: 1.4214 - val_accuracy: 0.4972\n",
            "Epoch 58/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.4030 - accuracy: 0.5004 - val_loss: 1.4010 - val_accuracy: 0.5020\n",
            "Epoch 59/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.4028 - accuracy: 0.5014 - val_loss: 1.4089 - val_accuracy: 0.5005\n",
            "Epoch 60/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.4017 - accuracy: 0.5019 - val_loss: 1.4056 - val_accuracy: 0.5029\n",
            "Epoch 61/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3991 - accuracy: 0.5049 - val_loss: 1.3973 - val_accuracy: 0.5062\n",
            "Epoch 62/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3911 - accuracy: 0.5043 - val_loss: 1.4016 - val_accuracy: 0.5034\n",
            "Epoch 63/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3857 - accuracy: 0.5068 - val_loss: 1.4088 - val_accuracy: 0.5035\n",
            "Epoch 64/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3897 - accuracy: 0.5058 - val_loss: 1.3922 - val_accuracy: 0.5147\n",
            "Epoch 65/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3837 - accuracy: 0.5081 - val_loss: 1.3990 - val_accuracy: 0.5064\n",
            "Epoch 66/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3887 - accuracy: 0.5059 - val_loss: 1.3968 - val_accuracy: 0.5072\n",
            "Epoch 67/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3862 - accuracy: 0.5068 - val_loss: 1.3900 - val_accuracy: 0.5116\n",
            "Epoch 68/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3857 - accuracy: 0.5056 - val_loss: 1.4040 - val_accuracy: 0.5060\n",
            "Epoch 69/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3803 - accuracy: 0.5059 - val_loss: 1.3911 - val_accuracy: 0.5060\n",
            "Epoch 70/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3800 - accuracy: 0.5078 - val_loss: 1.3894 - val_accuracy: 0.5149\n",
            "Epoch 71/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3775 - accuracy: 0.5094 - val_loss: 1.4048 - val_accuracy: 0.5045\n",
            "Epoch 72/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3766 - accuracy: 0.5118 - val_loss: 1.4231 - val_accuracy: 0.4982\n",
            "Epoch 73/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3702 - accuracy: 0.5145 - val_loss: 1.4129 - val_accuracy: 0.5046\n",
            "Epoch 74/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3749 - accuracy: 0.5112 - val_loss: 1.4043 - val_accuracy: 0.5016\n",
            "Epoch 75/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3656 - accuracy: 0.5111 - val_loss: 1.3869 - val_accuracy: 0.5104\n",
            "Epoch 76/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3671 - accuracy: 0.5141 - val_loss: 1.3856 - val_accuracy: 0.5142\n",
            "Epoch 77/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3691 - accuracy: 0.5143 - val_loss: 1.4050 - val_accuracy: 0.5012\n",
            "Epoch 78/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3694 - accuracy: 0.5114 - val_loss: 1.3893 - val_accuracy: 0.5102\n",
            "Epoch 79/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3645 - accuracy: 0.5140 - val_loss: 1.4179 - val_accuracy: 0.5019\n",
            "Epoch 80/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3614 - accuracy: 0.5164 - val_loss: 1.3987 - val_accuracy: 0.5066\n",
            "Epoch 81/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3616 - accuracy: 0.5157 - val_loss: 1.4026 - val_accuracy: 0.5011\n",
            "Epoch 82/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3591 - accuracy: 0.5145 - val_loss: 1.4029 - val_accuracy: 0.5006\n",
            "Epoch 83/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3604 - accuracy: 0.5149 - val_loss: 1.4014 - val_accuracy: 0.5024\n",
            "Epoch 84/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3544 - accuracy: 0.5183 - val_loss: 1.3947 - val_accuracy: 0.5108\n",
            "Epoch 85/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3491 - accuracy: 0.5204 - val_loss: 1.3854 - val_accuracy: 0.5125\n",
            "Epoch 86/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3578 - accuracy: 0.5158 - val_loss: 1.3831 - val_accuracy: 0.5097\n",
            "Epoch 87/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3552 - accuracy: 0.5175 - val_loss: 1.3909 - val_accuracy: 0.5123\n",
            "Epoch 88/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3499 - accuracy: 0.5156 - val_loss: 1.4062 - val_accuracy: 0.5018\n",
            "Epoch 89/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3525 - accuracy: 0.5180 - val_loss: 1.4037 - val_accuracy: 0.5035\n",
            "Epoch 90/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3474 - accuracy: 0.5194 - val_loss: 1.3844 - val_accuracy: 0.5170\n",
            "Epoch 91/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.3491 - accuracy: 0.5183 - val_loss: 1.3983 - val_accuracy: 0.5075\n",
            "Epoch 92/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3427 - accuracy: 0.5226 - val_loss: 1.3886 - val_accuracy: 0.5097\n",
            "Epoch 93/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.3512 - accuracy: 0.5175 - val_loss: 1.3979 - val_accuracy: 0.5053\n",
            "Epoch 94/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3443 - accuracy: 0.5193 - val_loss: 1.3984 - val_accuracy: 0.5089\n",
            "Epoch 95/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3409 - accuracy: 0.5248 - val_loss: 1.3952 - val_accuracy: 0.5078\n",
            "Epoch 96/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3464 - accuracy: 0.5203 - val_loss: 1.3955 - val_accuracy: 0.5095\n",
            "Epoch 97/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3425 - accuracy: 0.5227 - val_loss: 1.3996 - val_accuracy: 0.5082\n",
            "Epoch 98/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3385 - accuracy: 0.5214 - val_loss: 1.3909 - val_accuracy: 0.5043\n",
            "Epoch 99/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3356 - accuracy: 0.5238 - val_loss: 1.3916 - val_accuracy: 0.5083\n",
            "Epoch 100/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3396 - accuracy: 0.5238 - val_loss: 1.3851 - val_accuracy: 0.5107\n",
            "Epoch 101/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3318 - accuracy: 0.5254 - val_loss: 1.4155 - val_accuracy: 0.4969\n",
            "Epoch 102/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3361 - accuracy: 0.5248 - val_loss: 1.3903 - val_accuracy: 0.5123\n",
            "Epoch 103/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3374 - accuracy: 0.5216 - val_loss: 1.3904 - val_accuracy: 0.5083\n",
            "Epoch 104/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3335 - accuracy: 0.5249 - val_loss: 1.3936 - val_accuracy: 0.5086\n",
            "Epoch 105/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3315 - accuracy: 0.5264 - val_loss: 1.3944 - val_accuracy: 0.5088\n",
            "Epoch 106/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3284 - accuracy: 0.5264 - val_loss: 1.3948 - val_accuracy: 0.5073\n",
            "Epoch 107/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3350 - accuracy: 0.5266 - val_loss: 1.4044 - val_accuracy: 0.5098\n",
            "Epoch 108/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3377 - accuracy: 0.5216 - val_loss: 1.4064 - val_accuracy: 0.5036\n",
            "Epoch 109/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3270 - accuracy: 0.5256 - val_loss: 1.3932 - val_accuracy: 0.5105\n",
            "Epoch 110/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.3269 - accuracy: 0.5278 - val_loss: 1.3906 - val_accuracy: 0.5165\n",
            "Epoch 111/1000\n",
            "98/98 [==============================] - 15s 151ms/step - loss: 1.3235 - accuracy: 0.5292 - val_loss: 1.3862 - val_accuracy: 0.5126\n",
            "Epoch 112/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3285 - accuracy: 0.5251 - val_loss: 1.3845 - val_accuracy: 0.5149\n",
            "Epoch 113/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3220 - accuracy: 0.5295 - val_loss: 1.3799 - val_accuracy: 0.5176\n",
            "Epoch 114/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.3217 - accuracy: 0.5286 - val_loss: 1.3785 - val_accuracy: 0.5150\n",
            "Epoch 115/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3287 - accuracy: 0.5258 - val_loss: 1.4003 - val_accuracy: 0.5051\n",
            "Epoch 116/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.3220 - accuracy: 0.5296 - val_loss: 1.3923 - val_accuracy: 0.4990\n",
            "Epoch 117/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.3170 - accuracy: 0.5293 - val_loss: 1.3828 - val_accuracy: 0.5149\n",
            "Epoch 118/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3207 - accuracy: 0.5312 - val_loss: 1.3937 - val_accuracy: 0.5140\n",
            "Epoch 119/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3180 - accuracy: 0.5291 - val_loss: 1.3775 - val_accuracy: 0.5139\n",
            "Epoch 120/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3167 - accuracy: 0.5322 - val_loss: 1.3985 - val_accuracy: 0.5093\n",
            "Epoch 121/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3138 - accuracy: 0.5300 - val_loss: 1.3948 - val_accuracy: 0.5095\n",
            "Epoch 122/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3159 - accuracy: 0.5312 - val_loss: 1.3829 - val_accuracy: 0.5100\n",
            "Epoch 123/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3127 - accuracy: 0.5339 - val_loss: 1.4095 - val_accuracy: 0.5061\n",
            "Epoch 124/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3096 - accuracy: 0.5315 - val_loss: 1.3939 - val_accuracy: 0.5086\n",
            "Epoch 125/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3155 - accuracy: 0.5301 - val_loss: 1.3949 - val_accuracy: 0.5141\n",
            "Epoch 126/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3172 - accuracy: 0.5340 - val_loss: 1.4103 - val_accuracy: 0.5022\n",
            "Epoch 127/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3114 - accuracy: 0.5327 - val_loss: 1.4094 - val_accuracy: 0.5076\n",
            "Epoch 128/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.3143 - accuracy: 0.5342 - val_loss: 1.3975 - val_accuracy: 0.5102\n",
            "Epoch 129/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3143 - accuracy: 0.5314 - val_loss: 1.4058 - val_accuracy: 0.5064\n",
            "Epoch 130/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3057 - accuracy: 0.5363 - val_loss: 1.3902 - val_accuracy: 0.5133\n",
            "Epoch 131/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3090 - accuracy: 0.5336 - val_loss: 1.3877 - val_accuracy: 0.5101\n",
            "Epoch 132/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.3081 - accuracy: 0.5367 - val_loss: 1.4192 - val_accuracy: 0.4998\n",
            "Epoch 133/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3059 - accuracy: 0.5367 - val_loss: 1.4057 - val_accuracy: 0.5049\n",
            "Epoch 134/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.3058 - accuracy: 0.5327 - val_loss: 1.3853 - val_accuracy: 0.5176\n",
            "Epoch 135/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3082 - accuracy: 0.5356 - val_loss: 1.3963 - val_accuracy: 0.5085\n",
            "Epoch 136/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3028 - accuracy: 0.5344 - val_loss: 1.3871 - val_accuracy: 0.5182\n",
            "Epoch 137/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3039 - accuracy: 0.5349 - val_loss: 1.3873 - val_accuracy: 0.5124\n",
            "Epoch 138/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3029 - accuracy: 0.5361 - val_loss: 1.3896 - val_accuracy: 0.5128\n",
            "Epoch 139/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3008 - accuracy: 0.5358 - val_loss: 1.4204 - val_accuracy: 0.5008\n",
            "Epoch 140/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.3026 - accuracy: 0.5366 - val_loss: 1.3938 - val_accuracy: 0.5106\n",
            "Epoch 141/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3044 - accuracy: 0.5364 - val_loss: 1.3839 - val_accuracy: 0.5119\n",
            "Epoch 142/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3005 - accuracy: 0.5371 - val_loss: 1.3873 - val_accuracy: 0.5109\n",
            "Epoch 143/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.3016 - accuracy: 0.5361 - val_loss: 1.3954 - val_accuracy: 0.5147\n",
            "Epoch 144/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.3019 - accuracy: 0.5367 - val_loss: 1.3932 - val_accuracy: 0.5139\n",
            "Epoch 145/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2926 - accuracy: 0.5396 - val_loss: 1.3893 - val_accuracy: 0.5124\n",
            "Epoch 146/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2968 - accuracy: 0.5360 - val_loss: 1.3866 - val_accuracy: 0.5147\n",
            "Epoch 147/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2971 - accuracy: 0.5374 - val_loss: 1.4044 - val_accuracy: 0.5094\n",
            "Epoch 148/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.3010 - accuracy: 0.5369 - val_loss: 1.4034 - val_accuracy: 0.5111\n",
            "Epoch 149/1000\n",
            "98/98 [==============================] - 16s 158ms/step - loss: 1.2922 - accuracy: 0.5388 - val_loss: 1.3923 - val_accuracy: 0.5133\n",
            "Epoch 150/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2918 - accuracy: 0.5395 - val_loss: 1.4159 - val_accuracy: 0.5118\n",
            "Epoch 151/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2902 - accuracy: 0.5395 - val_loss: 1.3872 - val_accuracy: 0.5206\n",
            "Epoch 152/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2885 - accuracy: 0.5420 - val_loss: 1.3982 - val_accuracy: 0.5136\n",
            "Epoch 153/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2914 - accuracy: 0.5393 - val_loss: 1.4012 - val_accuracy: 0.5108\n",
            "Epoch 154/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2897 - accuracy: 0.5404 - val_loss: 1.3966 - val_accuracy: 0.5155\n",
            "Epoch 155/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2863 - accuracy: 0.5413 - val_loss: 1.4014 - val_accuracy: 0.5147\n",
            "Epoch 156/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2846 - accuracy: 0.5422 - val_loss: 1.3916 - val_accuracy: 0.5136\n",
            "Epoch 157/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2860 - accuracy: 0.5410 - val_loss: 1.3883 - val_accuracy: 0.5158\n",
            "Epoch 158/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2837 - accuracy: 0.5413 - val_loss: 1.3843 - val_accuracy: 0.5138\n",
            "Epoch 159/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2811 - accuracy: 0.5427 - val_loss: 1.4023 - val_accuracy: 0.5074\n",
            "Epoch 160/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2847 - accuracy: 0.5438 - val_loss: 1.4002 - val_accuracy: 0.5101\n",
            "Epoch 161/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2842 - accuracy: 0.5418 - val_loss: 1.3910 - val_accuracy: 0.5159\n",
            "Epoch 162/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2829 - accuracy: 0.5426 - val_loss: 1.3893 - val_accuracy: 0.5187\n",
            "Epoch 163/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2820 - accuracy: 0.5424 - val_loss: 1.3964 - val_accuracy: 0.5122\n",
            "Epoch 164/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2818 - accuracy: 0.5440 - val_loss: 1.3925 - val_accuracy: 0.5120\n",
            "Epoch 165/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2812 - accuracy: 0.5449 - val_loss: 1.4001 - val_accuracy: 0.5154\n",
            "Epoch 166/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.2863 - accuracy: 0.5407 - val_loss: 1.3934 - val_accuracy: 0.5142\n",
            "Epoch 167/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2830 - accuracy: 0.5441 - val_loss: 1.3963 - val_accuracy: 0.5161\n",
            "Epoch 168/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2853 - accuracy: 0.5434 - val_loss: 1.3818 - val_accuracy: 0.5216\n",
            "Epoch 169/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2800 - accuracy: 0.5432 - val_loss: 1.4069 - val_accuracy: 0.5105\n",
            "Epoch 170/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2849 - accuracy: 0.5414 - val_loss: 1.3886 - val_accuracy: 0.5208\n",
            "Epoch 171/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2826 - accuracy: 0.5460 - val_loss: 1.3840 - val_accuracy: 0.5206\n",
            "Epoch 172/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2809 - accuracy: 0.5432 - val_loss: 1.4000 - val_accuracy: 0.5096\n",
            "Epoch 173/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2827 - accuracy: 0.5452 - val_loss: 1.3986 - val_accuracy: 0.5144\n",
            "Epoch 174/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2754 - accuracy: 0.5454 - val_loss: 1.4040 - val_accuracy: 0.5089\n",
            "Epoch 175/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2748 - accuracy: 0.5453 - val_loss: 1.4011 - val_accuracy: 0.5109\n",
            "Epoch 176/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2798 - accuracy: 0.5436 - val_loss: 1.4021 - val_accuracy: 0.5144\n",
            "Epoch 177/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2797 - accuracy: 0.5438 - val_loss: 1.3825 - val_accuracy: 0.5198\n",
            "Epoch 178/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2820 - accuracy: 0.5427 - val_loss: 1.3956 - val_accuracy: 0.5196\n",
            "Epoch 179/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2780 - accuracy: 0.5449 - val_loss: 1.3995 - val_accuracy: 0.5078\n",
            "Epoch 180/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.2757 - accuracy: 0.5456 - val_loss: 1.4026 - val_accuracy: 0.5130\n",
            "Epoch 181/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2720 - accuracy: 0.5488 - val_loss: 1.4009 - val_accuracy: 0.5104\n",
            "Epoch 182/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2769 - accuracy: 0.5458 - val_loss: 1.3941 - val_accuracy: 0.5110\n",
            "Epoch 183/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2723 - accuracy: 0.5467 - val_loss: 1.3951 - val_accuracy: 0.5182\n",
            "Epoch 184/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2777 - accuracy: 0.5437 - val_loss: 1.4108 - val_accuracy: 0.5063\n",
            "Epoch 185/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2743 - accuracy: 0.5462 - val_loss: 1.3907 - val_accuracy: 0.5148\n",
            "Epoch 186/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2710 - accuracy: 0.5469 - val_loss: 1.3969 - val_accuracy: 0.5193\n",
            "Epoch 187/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2692 - accuracy: 0.5454 - val_loss: 1.3923 - val_accuracy: 0.5158\n",
            "Epoch 188/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2676 - accuracy: 0.5480 - val_loss: 1.4366 - val_accuracy: 0.5010\n",
            "Epoch 189/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2700 - accuracy: 0.5502 - val_loss: 1.4008 - val_accuracy: 0.5189\n",
            "Epoch 190/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2751 - accuracy: 0.5459 - val_loss: 1.3856 - val_accuracy: 0.5205\n",
            "Epoch 191/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2719 - accuracy: 0.5460 - val_loss: 1.4024 - val_accuracy: 0.5172\n",
            "Epoch 192/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2664 - accuracy: 0.5515 - val_loss: 1.3955 - val_accuracy: 0.5227\n",
            "Epoch 193/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.2777 - accuracy: 0.5439 - val_loss: 1.4155 - val_accuracy: 0.5152\n",
            "Epoch 194/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2717 - accuracy: 0.5472 - val_loss: 1.3952 - val_accuracy: 0.5176\n",
            "Epoch 195/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2681 - accuracy: 0.5473 - val_loss: 1.4108 - val_accuracy: 0.5094\n",
            "Epoch 196/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2712 - accuracy: 0.5483 - val_loss: 1.3978 - val_accuracy: 0.5178\n",
            "Epoch 197/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2684 - accuracy: 0.5508 - val_loss: 1.3853 - val_accuracy: 0.5215\n",
            "Epoch 198/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2665 - accuracy: 0.5481 - val_loss: 1.4008 - val_accuracy: 0.5168\n",
            "Epoch 199/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2634 - accuracy: 0.5507 - val_loss: 1.3951 - val_accuracy: 0.5174\n",
            "Epoch 200/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2645 - accuracy: 0.5485 - val_loss: 1.3934 - val_accuracy: 0.5209\n",
            "Epoch 201/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2685 - accuracy: 0.5472 - val_loss: 1.4068 - val_accuracy: 0.5106\n",
            "Epoch 202/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2621 - accuracy: 0.5483 - val_loss: 1.4078 - val_accuracy: 0.5103\n",
            "Epoch 203/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2608 - accuracy: 0.5507 - val_loss: 1.4093 - val_accuracy: 0.5130\n",
            "Epoch 204/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2625 - accuracy: 0.5498 - val_loss: 1.3981 - val_accuracy: 0.5170\n",
            "Epoch 205/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2578 - accuracy: 0.5532 - val_loss: 1.3873 - val_accuracy: 0.5240\n",
            "Epoch 206/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2655 - accuracy: 0.5508 - val_loss: 1.4004 - val_accuracy: 0.5178\n",
            "Epoch 207/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.2644 - accuracy: 0.5481 - val_loss: 1.4164 - val_accuracy: 0.5063\n",
            "Epoch 208/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2594 - accuracy: 0.5531 - val_loss: 1.3997 - val_accuracy: 0.5152\n",
            "Epoch 209/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2582 - accuracy: 0.5520 - val_loss: 1.4096 - val_accuracy: 0.5132\n",
            "Epoch 210/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2622 - accuracy: 0.5503 - val_loss: 1.3999 - val_accuracy: 0.5157\n",
            "Epoch 211/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2667 - accuracy: 0.5483 - val_loss: 1.4122 - val_accuracy: 0.5131\n",
            "Epoch 212/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2653 - accuracy: 0.5513 - val_loss: 1.3858 - val_accuracy: 0.5219\n",
            "Epoch 213/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2608 - accuracy: 0.5527 - val_loss: 1.4055 - val_accuracy: 0.5179\n",
            "Epoch 214/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2650 - accuracy: 0.5482 - val_loss: 1.4113 - val_accuracy: 0.5181\n",
            "Epoch 215/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2638 - accuracy: 0.5513 - val_loss: 1.3948 - val_accuracy: 0.5198\n",
            "Epoch 216/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2575 - accuracy: 0.5531 - val_loss: 1.4052 - val_accuracy: 0.5148\n",
            "Epoch 217/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2564 - accuracy: 0.5528 - val_loss: 1.3964 - val_accuracy: 0.5202\n",
            "Epoch 218/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2552 - accuracy: 0.5527 - val_loss: 1.4313 - val_accuracy: 0.5087\n",
            "Epoch 219/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2557 - accuracy: 0.5517 - val_loss: 1.4043 - val_accuracy: 0.5186\n",
            "Epoch 220/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2551 - accuracy: 0.5536 - val_loss: 1.3953 - val_accuracy: 0.5193\n",
            "Epoch 221/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2567 - accuracy: 0.5535 - val_loss: 1.3904 - val_accuracy: 0.5243\n",
            "Epoch 222/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2548 - accuracy: 0.5510 - val_loss: 1.3930 - val_accuracy: 0.5238\n",
            "Epoch 223/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2533 - accuracy: 0.5534 - val_loss: 1.3934 - val_accuracy: 0.5253\n",
            "Epoch 224/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2621 - accuracy: 0.5489 - val_loss: 1.4071 - val_accuracy: 0.5155\n",
            "Epoch 225/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2590 - accuracy: 0.5526 - val_loss: 1.3909 - val_accuracy: 0.5193\n",
            "Epoch 226/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2558 - accuracy: 0.5518 - val_loss: 1.4118 - val_accuracy: 0.5146\n",
            "Epoch 227/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2588 - accuracy: 0.5519 - val_loss: 1.3980 - val_accuracy: 0.5179\n",
            "Epoch 228/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2535 - accuracy: 0.5538 - val_loss: 1.4135 - val_accuracy: 0.5130\n",
            "Epoch 229/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2564 - accuracy: 0.5518 - val_loss: 1.4114 - val_accuracy: 0.5153\n",
            "Epoch 230/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2570 - accuracy: 0.5524 - val_loss: 1.4058 - val_accuracy: 0.5139\n",
            "Epoch 231/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2509 - accuracy: 0.5572 - val_loss: 1.4024 - val_accuracy: 0.5220\n",
            "Epoch 232/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2507 - accuracy: 0.5531 - val_loss: 1.3967 - val_accuracy: 0.5187\n",
            "Epoch 233/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2530 - accuracy: 0.5545 - val_loss: 1.4069 - val_accuracy: 0.5190\n",
            "Epoch 234/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2605 - accuracy: 0.5513 - val_loss: 1.4212 - val_accuracy: 0.5084\n",
            "Epoch 235/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2493 - accuracy: 0.5553 - val_loss: 1.4146 - val_accuracy: 0.5153\n",
            "Epoch 236/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2574 - accuracy: 0.5525 - val_loss: 1.4034 - val_accuracy: 0.5170\n",
            "Epoch 237/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2565 - accuracy: 0.5529 - val_loss: 1.3979 - val_accuracy: 0.5179\n",
            "Epoch 238/1000\n",
            "98/98 [==============================] - 15s 152ms/step - loss: 1.2508 - accuracy: 0.5562 - val_loss: 1.4074 - val_accuracy: 0.5153\n",
            "Epoch 239/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2516 - accuracy: 0.5545 - val_loss: 1.3928 - val_accuracy: 0.5205\n",
            "Epoch 240/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2470 - accuracy: 0.5563 - val_loss: 1.4161 - val_accuracy: 0.5125\n",
            "Epoch 241/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2514 - accuracy: 0.5530 - val_loss: 1.3953 - val_accuracy: 0.5211\n",
            "Epoch 242/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2461 - accuracy: 0.5541 - val_loss: 1.4146 - val_accuracy: 0.5140\n",
            "Epoch 243/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2531 - accuracy: 0.5531 - val_loss: 1.3994 - val_accuracy: 0.5241\n",
            "Epoch 244/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2481 - accuracy: 0.5556 - val_loss: 1.4006 - val_accuracy: 0.5154\n",
            "Epoch 245/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2466 - accuracy: 0.5549 - val_loss: 1.4089 - val_accuracy: 0.5138\n",
            "Epoch 246/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2432 - accuracy: 0.5572 - val_loss: 1.4096 - val_accuracy: 0.5193\n",
            "Epoch 247/1000\n",
            "98/98 [==============================] - 15s 153ms/step - loss: 1.2524 - accuracy: 0.5549 - val_loss: 1.4203 - val_accuracy: 0.5205\n",
            "Epoch 248/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2499 - accuracy: 0.5529 - val_loss: 1.4197 - val_accuracy: 0.5145\n",
            "Epoch 249/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2485 - accuracy: 0.5561 - val_loss: 1.3966 - val_accuracy: 0.5249\n",
            "Epoch 250/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2489 - accuracy: 0.5537 - val_loss: 1.4169 - val_accuracy: 0.5101\n",
            "Epoch 251/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2492 - accuracy: 0.5552 - val_loss: 1.4064 - val_accuracy: 0.5199\n",
            "Epoch 252/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2513 - accuracy: 0.5538 - val_loss: 1.4040 - val_accuracy: 0.5151\n",
            "Epoch 253/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2454 - accuracy: 0.5540 - val_loss: 1.4193 - val_accuracy: 0.5125\n",
            "Epoch 254/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2418 - accuracy: 0.5575 - val_loss: 1.4001 - val_accuracy: 0.5203\n",
            "Epoch 255/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2459 - accuracy: 0.5554 - val_loss: 1.4116 - val_accuracy: 0.5165\n",
            "Epoch 256/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2436 - accuracy: 0.5544 - val_loss: 1.4137 - val_accuracy: 0.5141\n",
            "Epoch 257/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2429 - accuracy: 0.5585 - val_loss: 1.4168 - val_accuracy: 0.5111\n",
            "Epoch 258/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2482 - accuracy: 0.5566 - val_loss: 1.4139 - val_accuracy: 0.5116\n",
            "Epoch 259/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2466 - accuracy: 0.5562 - val_loss: 1.4477 - val_accuracy: 0.5124\n",
            "Epoch 260/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2424 - accuracy: 0.5550 - val_loss: 1.4190 - val_accuracy: 0.5138\n",
            "Epoch 261/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2446 - accuracy: 0.5571 - val_loss: 1.4149 - val_accuracy: 0.5156\n",
            "Epoch 262/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2453 - accuracy: 0.5565 - val_loss: 1.3904 - val_accuracy: 0.5299\n",
            "Epoch 263/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2444 - accuracy: 0.5573 - val_loss: 1.4099 - val_accuracy: 0.5131\n",
            "Epoch 264/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2449 - accuracy: 0.5576 - val_loss: 1.4241 - val_accuracy: 0.5145\n",
            "Epoch 265/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2503 - accuracy: 0.5528 - val_loss: 1.3993 - val_accuracy: 0.5244\n",
            "Epoch 266/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2443 - accuracy: 0.5545 - val_loss: 1.3952 - val_accuracy: 0.5260\n",
            "Epoch 267/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2434 - accuracy: 0.5545 - val_loss: 1.3974 - val_accuracy: 0.5196\n",
            "Epoch 268/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2486 - accuracy: 0.5525 - val_loss: 1.3994 - val_accuracy: 0.5237\n",
            "Epoch 269/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2364 - accuracy: 0.5611 - val_loss: 1.3931 - val_accuracy: 0.5279\n",
            "Epoch 270/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2430 - accuracy: 0.5547 - val_loss: 1.4014 - val_accuracy: 0.5200\n",
            "Epoch 271/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2391 - accuracy: 0.5575 - val_loss: 1.4124 - val_accuracy: 0.5185\n",
            "Epoch 272/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2456 - accuracy: 0.5555 - val_loss: 1.3965 - val_accuracy: 0.5213\n",
            "Epoch 273/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2437 - accuracy: 0.5556 - val_loss: 1.4135 - val_accuracy: 0.5141\n",
            "Epoch 274/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2433 - accuracy: 0.5551 - val_loss: 1.3990 - val_accuracy: 0.5220\n",
            "Epoch 275/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2421 - accuracy: 0.5568 - val_loss: 1.3986 - val_accuracy: 0.5233\n",
            "Epoch 276/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2402 - accuracy: 0.5574 - val_loss: 1.4131 - val_accuracy: 0.5177\n",
            "Epoch 277/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2386 - accuracy: 0.5575 - val_loss: 1.4035 - val_accuracy: 0.5201\n",
            "Epoch 278/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2420 - accuracy: 0.5563 - val_loss: 1.4094 - val_accuracy: 0.5228\n",
            "Epoch 279/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2411 - accuracy: 0.5572 - val_loss: 1.4240 - val_accuracy: 0.5113\n",
            "Epoch 280/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2416 - accuracy: 0.5589 - val_loss: 1.4008 - val_accuracy: 0.5213\n",
            "Epoch 281/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2393 - accuracy: 0.5602 - val_loss: 1.4113 - val_accuracy: 0.5179\n",
            "Epoch 282/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2402 - accuracy: 0.5565 - val_loss: 1.4092 - val_accuracy: 0.5200\n",
            "Epoch 283/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2365 - accuracy: 0.5596 - val_loss: 1.4010 - val_accuracy: 0.5189\n",
            "Epoch 284/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2407 - accuracy: 0.5588 - val_loss: 1.4064 - val_accuracy: 0.5189\n",
            "Epoch 285/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2325 - accuracy: 0.5594 - val_loss: 1.3964 - val_accuracy: 0.5202\n",
            "Epoch 286/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2375 - accuracy: 0.5560 - val_loss: 1.4060 - val_accuracy: 0.5179\n",
            "Epoch 287/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2392 - accuracy: 0.5553 - val_loss: 1.3965 - val_accuracy: 0.5227\n",
            "Epoch 288/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2400 - accuracy: 0.5579 - val_loss: 1.4142 - val_accuracy: 0.5148\n",
            "Epoch 289/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2354 - accuracy: 0.5581 - val_loss: 1.4122 - val_accuracy: 0.5204\n",
            "Epoch 290/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2317 - accuracy: 0.5615 - val_loss: 1.4060 - val_accuracy: 0.5209\n",
            "Epoch 291/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2395 - accuracy: 0.5595 - val_loss: 1.4080 - val_accuracy: 0.5184\n",
            "Epoch 292/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2393 - accuracy: 0.5564 - val_loss: 1.3958 - val_accuracy: 0.5238\n",
            "Epoch 293/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2344 - accuracy: 0.5613 - val_loss: 1.4025 - val_accuracy: 0.5233\n",
            "Epoch 294/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2351 - accuracy: 0.5599 - val_loss: 1.4151 - val_accuracy: 0.5184\n",
            "Epoch 295/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2342 - accuracy: 0.5606 - val_loss: 1.3949 - val_accuracy: 0.5246\n",
            "Epoch 296/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2325 - accuracy: 0.5590 - val_loss: 1.4059 - val_accuracy: 0.5202\n",
            "Epoch 297/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2373 - accuracy: 0.5578 - val_loss: 1.4137 - val_accuracy: 0.5171\n",
            "Epoch 298/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2376 - accuracy: 0.5599 - val_loss: 1.4111 - val_accuracy: 0.5226\n",
            "Epoch 299/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2425 - accuracy: 0.5549 - val_loss: 1.4121 - val_accuracy: 0.5216\n",
            "Epoch 300/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2348 - accuracy: 0.5623 - val_loss: 1.4145 - val_accuracy: 0.5182\n",
            "Epoch 301/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2375 - accuracy: 0.5589 - val_loss: 1.4154 - val_accuracy: 0.5162\n",
            "Epoch 302/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2358 - accuracy: 0.5627 - val_loss: 1.4342 - val_accuracy: 0.5164\n",
            "Epoch 303/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2398 - accuracy: 0.5575 - val_loss: 1.4024 - val_accuracy: 0.5240\n",
            "Epoch 304/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2328 - accuracy: 0.5597 - val_loss: 1.4083 - val_accuracy: 0.5227\n",
            "Epoch 305/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2296 - accuracy: 0.5603 - val_loss: 1.4238 - val_accuracy: 0.5176\n",
            "Epoch 306/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2328 - accuracy: 0.5611 - val_loss: 1.4106 - val_accuracy: 0.5151\n",
            "Epoch 307/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2339 - accuracy: 0.5570 - val_loss: 1.4153 - val_accuracy: 0.5188\n",
            "Epoch 308/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2359 - accuracy: 0.5568 - val_loss: 1.4099 - val_accuracy: 0.5167\n",
            "Epoch 309/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2292 - accuracy: 0.5626 - val_loss: 1.4085 - val_accuracy: 0.5204\n",
            "Epoch 310/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2292 - accuracy: 0.5609 - val_loss: 1.4178 - val_accuracy: 0.5137\n",
            "Epoch 311/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2336 - accuracy: 0.5620 - val_loss: 1.4104 - val_accuracy: 0.5207\n",
            "Epoch 312/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2314 - accuracy: 0.5591 - val_loss: 1.4082 - val_accuracy: 0.5228\n",
            "Epoch 313/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2321 - accuracy: 0.5610 - val_loss: 1.4067 - val_accuracy: 0.5222\n",
            "Epoch 314/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2296 - accuracy: 0.5622 - val_loss: 1.4159 - val_accuracy: 0.5163\n",
            "Epoch 315/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2282 - accuracy: 0.5610 - val_loss: 1.4109 - val_accuracy: 0.5182\n",
            "Epoch 316/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2315 - accuracy: 0.5616 - val_loss: 1.4254 - val_accuracy: 0.5193\n",
            "Epoch 317/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2320 - accuracy: 0.5593 - val_loss: 1.4107 - val_accuracy: 0.5205\n",
            "Epoch 318/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2333 - accuracy: 0.5595 - val_loss: 1.4069 - val_accuracy: 0.5225\n",
            "Epoch 319/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2352 - accuracy: 0.5603 - val_loss: 1.4104 - val_accuracy: 0.5235\n",
            "Epoch 320/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2355 - accuracy: 0.5602 - val_loss: 1.4364 - val_accuracy: 0.5083\n",
            "Epoch 321/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2335 - accuracy: 0.5606 - val_loss: 1.4209 - val_accuracy: 0.5158\n",
            "Epoch 322/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2299 - accuracy: 0.5582 - val_loss: 1.4151 - val_accuracy: 0.5184\n",
            "Epoch 323/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2273 - accuracy: 0.5624 - val_loss: 1.4215 - val_accuracy: 0.5109\n",
            "Epoch 324/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2223 - accuracy: 0.5648 - val_loss: 1.4324 - val_accuracy: 0.5110\n",
            "Epoch 325/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2273 - accuracy: 0.5616 - val_loss: 1.4190 - val_accuracy: 0.5197\n",
            "Epoch 326/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2254 - accuracy: 0.5610 - val_loss: 1.4027 - val_accuracy: 0.5232\n",
            "Epoch 327/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2264 - accuracy: 0.5618 - val_loss: 1.4101 - val_accuracy: 0.5181\n",
            "Epoch 328/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2281 - accuracy: 0.5613 - val_loss: 1.4112 - val_accuracy: 0.5206\n",
            "Epoch 329/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2262 - accuracy: 0.5643 - val_loss: 1.4282 - val_accuracy: 0.5160\n",
            "Epoch 330/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2338 - accuracy: 0.5626 - val_loss: 1.4351 - val_accuracy: 0.5125\n",
            "Epoch 331/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2303 - accuracy: 0.5608 - val_loss: 1.4104 - val_accuracy: 0.5243\n",
            "Epoch 332/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2281 - accuracy: 0.5626 - val_loss: 1.4237 - val_accuracy: 0.5171\n",
            "Epoch 333/1000\n",
            "98/98 [==============================] - 15s 154ms/step - loss: 1.2281 - accuracy: 0.5633 - val_loss: 1.4136 - val_accuracy: 0.5187\n",
            "Epoch 334/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2255 - accuracy: 0.5608 - val_loss: 1.4118 - val_accuracy: 0.5184\n",
            "Epoch 335/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2229 - accuracy: 0.5620 - val_loss: 1.4140 - val_accuracy: 0.5193\n",
            "Epoch 336/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2274 - accuracy: 0.5620 - val_loss: 1.4085 - val_accuracy: 0.5186\n",
            "Epoch 337/1000\n",
            "98/98 [==============================] - 15s 155ms/step - loss: 1.2233 - accuracy: 0.5648 - val_loss: 1.4107 - val_accuracy: 0.5176\n",
            "Epoch 338/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2324 - accuracy: 0.5608 - val_loss: 1.4278 - val_accuracy: 0.5134\n",
            "Epoch 339/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2239 - accuracy: 0.5618 - val_loss: 1.4230 - val_accuracy: 0.5142\n",
            "Epoch 340/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2285 - accuracy: 0.5610 - val_loss: 1.4159 - val_accuracy: 0.5210\n",
            "Epoch 341/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2228 - accuracy: 0.5642 - val_loss: 1.4021 - val_accuracy: 0.5237\n",
            "Epoch 342/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2304 - accuracy: 0.5609 - val_loss: 1.4361 - val_accuracy: 0.5135\n",
            "Epoch 343/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2293 - accuracy: 0.5621 - val_loss: 1.4228 - val_accuracy: 0.5182\n",
            "Epoch 344/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2257 - accuracy: 0.5632 - val_loss: 1.4200 - val_accuracy: 0.5168\n",
            "Epoch 345/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2270 - accuracy: 0.5631 - val_loss: 1.4200 - val_accuracy: 0.5158\n",
            "Epoch 346/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2230 - accuracy: 0.5619 - val_loss: 1.4224 - val_accuracy: 0.5171\n",
            "Epoch 347/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2254 - accuracy: 0.5634 - val_loss: 1.4184 - val_accuracy: 0.5197\n",
            "Epoch 348/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2241 - accuracy: 0.5642 - val_loss: 1.4178 - val_accuracy: 0.5175\n",
            "Epoch 349/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2224 - accuracy: 0.5651 - val_loss: 1.4150 - val_accuracy: 0.5184\n",
            "Epoch 350/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2236 - accuracy: 0.5647 - val_loss: 1.4138 - val_accuracy: 0.5189\n",
            "Epoch 351/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2288 - accuracy: 0.5605 - val_loss: 1.4268 - val_accuracy: 0.5177\n",
            "Epoch 352/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2249 - accuracy: 0.5608 - val_loss: 1.4241 - val_accuracy: 0.5184\n",
            "Epoch 353/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2224 - accuracy: 0.5625 - val_loss: 1.4218 - val_accuracy: 0.5136\n",
            "Epoch 354/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2231 - accuracy: 0.5637 - val_loss: 1.4275 - val_accuracy: 0.5185\n",
            "Epoch 355/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2263 - accuracy: 0.5630 - val_loss: 1.4100 - val_accuracy: 0.5228\n",
            "Epoch 356/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2262 - accuracy: 0.5588 - val_loss: 1.4181 - val_accuracy: 0.5160\n",
            "Epoch 357/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2247 - accuracy: 0.5657 - val_loss: 1.4043 - val_accuracy: 0.5252\n",
            "Epoch 358/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2188 - accuracy: 0.5649 - val_loss: 1.4136 - val_accuracy: 0.5196\n",
            "Epoch 359/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2231 - accuracy: 0.5642 - val_loss: 1.4633 - val_accuracy: 0.5115\n",
            "Epoch 360/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2251 - accuracy: 0.5616 - val_loss: 1.4182 - val_accuracy: 0.5200\n",
            "Epoch 361/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2241 - accuracy: 0.5643 - val_loss: 1.4117 - val_accuracy: 0.5163\n",
            "Epoch 362/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2219 - accuracy: 0.5655 - val_loss: 1.4082 - val_accuracy: 0.5206\n",
            "Epoch 363/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2206 - accuracy: 0.5614 - val_loss: 1.4188 - val_accuracy: 0.5174\n",
            "Epoch 364/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2211 - accuracy: 0.5657 - val_loss: 1.4269 - val_accuracy: 0.5202\n",
            "Epoch 365/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2263 - accuracy: 0.5632 - val_loss: 1.4208 - val_accuracy: 0.5205\n",
            "Epoch 366/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2226 - accuracy: 0.5634 - val_loss: 1.4240 - val_accuracy: 0.5152\n",
            "Epoch 367/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2212 - accuracy: 0.5654 - val_loss: 1.4249 - val_accuracy: 0.5173\n",
            "Epoch 368/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2278 - accuracy: 0.5603 - val_loss: 1.4266 - val_accuracy: 0.5162\n",
            "Epoch 369/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2139 - accuracy: 0.5679 - val_loss: 1.4221 - val_accuracy: 0.5162\n",
            "Epoch 370/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2179 - accuracy: 0.5654 - val_loss: 1.4243 - val_accuracy: 0.5190\n",
            "Epoch 371/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2223 - accuracy: 0.5638 - val_loss: 1.4164 - val_accuracy: 0.5235\n",
            "Epoch 372/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2195 - accuracy: 0.5647 - val_loss: 1.4275 - val_accuracy: 0.5148\n",
            "Epoch 373/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2173 - accuracy: 0.5648 - val_loss: 1.4251 - val_accuracy: 0.5129\n",
            "Epoch 374/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2263 - accuracy: 0.5617 - val_loss: 1.4224 - val_accuracy: 0.5191\n",
            "Epoch 375/1000\n",
            "98/98 [==============================] - 16s 158ms/step - loss: 1.2175 - accuracy: 0.5634 - val_loss: 1.4425 - val_accuracy: 0.5084\n",
            "Epoch 376/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2184 - accuracy: 0.5650 - val_loss: 1.4205 - val_accuracy: 0.5196\n",
            "Epoch 377/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2209 - accuracy: 0.5646 - val_loss: 1.4307 - val_accuracy: 0.5146\n",
            "Epoch 378/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2207 - accuracy: 0.5664 - val_loss: 1.4102 - val_accuracy: 0.5222\n",
            "Epoch 379/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2235 - accuracy: 0.5635 - val_loss: 1.4170 - val_accuracy: 0.5232\n",
            "Epoch 380/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2177 - accuracy: 0.5659 - val_loss: 1.4084 - val_accuracy: 0.5230\n",
            "Epoch 381/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2151 - accuracy: 0.5664 - val_loss: 1.4095 - val_accuracy: 0.5249\n",
            "Epoch 382/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2171 - accuracy: 0.5655 - val_loss: 1.4394 - val_accuracy: 0.5117\n",
            "Epoch 383/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2144 - accuracy: 0.5668 - val_loss: 1.4130 - val_accuracy: 0.5240\n",
            "Epoch 384/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2199 - accuracy: 0.5646 - val_loss: 1.4153 - val_accuracy: 0.5244\n",
            "Epoch 385/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2195 - accuracy: 0.5666 - val_loss: 1.4132 - val_accuracy: 0.5189\n",
            "Epoch 386/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2229 - accuracy: 0.5636 - val_loss: 1.4189 - val_accuracy: 0.5178\n",
            "Epoch 387/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2191 - accuracy: 0.5611 - val_loss: 1.4189 - val_accuracy: 0.5160\n",
            "Epoch 388/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2180 - accuracy: 0.5658 - val_loss: 1.4440 - val_accuracy: 0.5166\n",
            "Epoch 389/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2201 - accuracy: 0.5649 - val_loss: 1.4263 - val_accuracy: 0.5099\n",
            "Epoch 390/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2159 - accuracy: 0.5658 - val_loss: 1.4163 - val_accuracy: 0.5215\n",
            "Epoch 391/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2113 - accuracy: 0.5663 - val_loss: 1.4228 - val_accuracy: 0.5232\n",
            "Epoch 392/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2157 - accuracy: 0.5653 - val_loss: 1.4571 - val_accuracy: 0.5122\n",
            "Epoch 393/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2202 - accuracy: 0.5645 - val_loss: 1.4219 - val_accuracy: 0.5197\n",
            "Epoch 394/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2177 - accuracy: 0.5660 - val_loss: 1.4155 - val_accuracy: 0.5191\n",
            "Epoch 395/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2196 - accuracy: 0.5650 - val_loss: 1.4349 - val_accuracy: 0.5058\n",
            "Epoch 396/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2200 - accuracy: 0.5640 - val_loss: 1.4323 - val_accuracy: 0.5172\n",
            "Epoch 397/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2115 - accuracy: 0.5688 - val_loss: 1.4267 - val_accuracy: 0.5150\n",
            "Epoch 398/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2179 - accuracy: 0.5639 - val_loss: 1.4292 - val_accuracy: 0.5187\n",
            "Epoch 399/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2161 - accuracy: 0.5671 - val_loss: 1.4236 - val_accuracy: 0.5168\n",
            "Epoch 400/1000\n",
            "98/98 [==============================] - 16s 158ms/step - loss: 1.2121 - accuracy: 0.5680 - val_loss: 1.4151 - val_accuracy: 0.5169\n",
            "Epoch 401/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2180 - accuracy: 0.5654 - val_loss: 1.4213 - val_accuracy: 0.5193\n",
            "Epoch 402/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2186 - accuracy: 0.5659 - val_loss: 1.4135 - val_accuracy: 0.5183\n",
            "Epoch 403/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2147 - accuracy: 0.5689 - val_loss: 1.4119 - val_accuracy: 0.5189\n",
            "Epoch 404/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2105 - accuracy: 0.5674 - val_loss: 1.4160 - val_accuracy: 0.5255\n",
            "Epoch 405/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2169 - accuracy: 0.5673 - val_loss: 1.4111 - val_accuracy: 0.5222\n",
            "Epoch 406/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2156 - accuracy: 0.5668 - val_loss: 1.4323 - val_accuracy: 0.5130\n",
            "Epoch 407/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2139 - accuracy: 0.5654 - val_loss: 1.4143 - val_accuracy: 0.5184\n",
            "Epoch 408/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2194 - accuracy: 0.5658 - val_loss: 1.4071 - val_accuracy: 0.5226\n",
            "Epoch 409/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2157 - accuracy: 0.5658 - val_loss: 1.4097 - val_accuracy: 0.5209\n",
            "Epoch 410/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2199 - accuracy: 0.5625 - val_loss: 1.4059 - val_accuracy: 0.5261\n",
            "Epoch 411/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2164 - accuracy: 0.5646 - val_loss: 1.4270 - val_accuracy: 0.5189\n",
            "Epoch 412/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2155 - accuracy: 0.5654 - val_loss: 1.4264 - val_accuracy: 0.5172\n",
            "Epoch 413/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2134 - accuracy: 0.5677 - val_loss: 1.4271 - val_accuracy: 0.5191\n",
            "Epoch 414/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2142 - accuracy: 0.5666 - val_loss: 1.4446 - val_accuracy: 0.5115\n",
            "Epoch 415/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2126 - accuracy: 0.5666 - val_loss: 1.4168 - val_accuracy: 0.5215\n",
            "Epoch 416/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2190 - accuracy: 0.5654 - val_loss: 1.4134 - val_accuracy: 0.5226\n",
            "Epoch 417/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2158 - accuracy: 0.5656 - val_loss: 1.4220 - val_accuracy: 0.5186\n",
            "Epoch 418/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2190 - accuracy: 0.5612 - val_loss: 1.4291 - val_accuracy: 0.5183\n",
            "Epoch 419/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2106 - accuracy: 0.5658 - val_loss: 1.4141 - val_accuracy: 0.5185\n",
            "Epoch 420/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2097 - accuracy: 0.5683 - val_loss: 1.4112 - val_accuracy: 0.5199\n",
            "Epoch 421/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2204 - accuracy: 0.5662 - val_loss: 1.4219 - val_accuracy: 0.5199\n",
            "Epoch 422/1000\n",
            "98/98 [==============================] - 15s 156ms/step - loss: 1.2140 - accuracy: 0.5687 - val_loss: 1.4219 - val_accuracy: 0.5159\n",
            "Epoch 423/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2134 - accuracy: 0.5670 - val_loss: 1.4238 - val_accuracy: 0.5161\n",
            "Epoch 424/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2129 - accuracy: 0.5675 - val_loss: 1.4187 - val_accuracy: 0.5208\n",
            "Epoch 425/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2076 - accuracy: 0.5704 - val_loss: 1.4276 - val_accuracy: 0.5164\n",
            "Epoch 426/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2174 - accuracy: 0.5664 - val_loss: 1.4078 - val_accuracy: 0.5237\n",
            "Epoch 427/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2141 - accuracy: 0.5665 - val_loss: 1.4014 - val_accuracy: 0.5286\n",
            "Epoch 428/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2139 - accuracy: 0.5669 - val_loss: 1.4376 - val_accuracy: 0.5128\n",
            "Epoch 429/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2101 - accuracy: 0.5663 - val_loss: 1.4197 - val_accuracy: 0.5204\n",
            "Epoch 430/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2089 - accuracy: 0.5677 - val_loss: 1.4280 - val_accuracy: 0.5120\n",
            "Epoch 431/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2161 - accuracy: 0.5648 - val_loss: 1.4195 - val_accuracy: 0.5184\n",
            "Epoch 432/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2121 - accuracy: 0.5679 - val_loss: 1.4325 - val_accuracy: 0.5158\n",
            "Epoch 433/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2132 - accuracy: 0.5653 - val_loss: 1.4187 - val_accuracy: 0.5171\n",
            "Epoch 434/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2212 - accuracy: 0.5623 - val_loss: 1.4090 - val_accuracy: 0.5204\n",
            "Epoch 435/1000\n",
            "98/98 [==============================] - 15s 157ms/step - loss: 1.2154 - accuracy: 0.5657 - val_loss: 1.4214 - val_accuracy: 0.5185\n",
            "Epoch 436/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2125 - accuracy: 0.5667 - val_loss: 1.4154 - val_accuracy: 0.5173\n",
            "Epoch 437/1000\n",
            "98/98 [==============================] - 16s 158ms/step - loss: 1.2089 - accuracy: 0.5691 - val_loss: 1.4322 - val_accuracy: 0.5150\n",
            "Epoch 438/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2196 - accuracy: 0.5667 - val_loss: 1.4220 - val_accuracy: 0.5190\n",
            "Epoch 439/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.2106 - accuracy: 0.5677 - val_loss: 1.4253 - val_accuracy: 0.5155\n",
            "Epoch 440/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2149 - accuracy: 0.5681 - val_loss: 1.4210 - val_accuracy: 0.5144\n",
            "Epoch 441/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2102 - accuracy: 0.5673 - val_loss: 1.4108 - val_accuracy: 0.5209\n",
            "Epoch 442/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2116 - accuracy: 0.5666 - val_loss: 1.4065 - val_accuracy: 0.5235\n",
            "Epoch 443/1000\n",
            "98/98 [==============================] - 16s 158ms/step - loss: 1.2093 - accuracy: 0.5688 - val_loss: 1.4108 - val_accuracy: 0.5248\n",
            "Epoch 444/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2074 - accuracy: 0.5682 - val_loss: 1.4340 - val_accuracy: 0.5107\n",
            "Epoch 445/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2090 - accuracy: 0.5668 - val_loss: 1.4207 - val_accuracy: 0.5221\n",
            "Epoch 446/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2098 - accuracy: 0.5670 - val_loss: 1.4260 - val_accuracy: 0.5186\n",
            "Epoch 447/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2092 - accuracy: 0.5673 - val_loss: 1.4201 - val_accuracy: 0.5187\n",
            "Epoch 448/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2061 - accuracy: 0.5694 - val_loss: 1.4270 - val_accuracy: 0.5209\n",
            "Epoch 449/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2175 - accuracy: 0.5652 - val_loss: 1.4144 - val_accuracy: 0.5211\n",
            "Epoch 450/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2105 - accuracy: 0.5646 - val_loss: 1.4249 - val_accuracy: 0.5247\n",
            "Epoch 451/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2165 - accuracy: 0.5653 - val_loss: 1.4334 - val_accuracy: 0.5118\n",
            "Epoch 452/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2150 - accuracy: 0.5668 - val_loss: 1.4173 - val_accuracy: 0.5205\n",
            "Epoch 453/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2094 - accuracy: 0.5669 - val_loss: 1.4211 - val_accuracy: 0.5165\n",
            "Epoch 454/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2114 - accuracy: 0.5700 - val_loss: 1.4386 - val_accuracy: 0.5153\n",
            "Epoch 455/1000\n",
            "98/98 [==============================] - 16s 158ms/step - loss: 1.2101 - accuracy: 0.5671 - val_loss: 1.4228 - val_accuracy: 0.5138\n",
            "Epoch 456/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2104 - accuracy: 0.5674 - val_loss: 1.4324 - val_accuracy: 0.5149\n",
            "Epoch 457/1000\n",
            "98/98 [==============================] - 16s 158ms/step - loss: 1.2112 - accuracy: 0.5656 - val_loss: 1.4274 - val_accuracy: 0.5157\n",
            "Epoch 458/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2072 - accuracy: 0.5685 - val_loss: 1.4333 - val_accuracy: 0.5175\n",
            "Epoch 459/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2059 - accuracy: 0.5695 - val_loss: 1.4323 - val_accuracy: 0.5167\n",
            "Epoch 460/1000\n",
            "98/98 [==============================] - 16s 158ms/step - loss: 1.2091 - accuracy: 0.5655 - val_loss: 1.4194 - val_accuracy: 0.5170\n",
            "Epoch 461/1000\n",
            "98/98 [==============================] - 16s 158ms/step - loss: 1.2082 - accuracy: 0.5695 - val_loss: 1.4506 - val_accuracy: 0.5120\n",
            "Epoch 462/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2118 - accuracy: 0.5674 - val_loss: 1.4145 - val_accuracy: 0.5253\n",
            "Epoch 463/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2077 - accuracy: 0.5688 - val_loss: 1.4104 - val_accuracy: 0.5234\n",
            "Epoch 464/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2115 - accuracy: 0.5674 - val_loss: 1.4147 - val_accuracy: 0.5200\n",
            "Epoch 465/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2061 - accuracy: 0.5701 - val_loss: 1.4319 - val_accuracy: 0.5202\n",
            "Epoch 466/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2108 - accuracy: 0.5667 - val_loss: 1.4287 - val_accuracy: 0.5110\n",
            "Epoch 467/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2140 - accuracy: 0.5679 - val_loss: 1.4091 - val_accuracy: 0.5233\n",
            "Epoch 468/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2082 - accuracy: 0.5659 - val_loss: 1.4220 - val_accuracy: 0.5158\n",
            "Epoch 469/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2100 - accuracy: 0.5696 - val_loss: 1.4238 - val_accuracy: 0.5162\n",
            "Epoch 470/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2069 - accuracy: 0.5691 - val_loss: 1.4405 - val_accuracy: 0.5140\n",
            "Epoch 471/1000\n",
            "98/98 [==============================] - 16s 158ms/step - loss: 1.2046 - accuracy: 0.5692 - val_loss: 1.4145 - val_accuracy: 0.5212\n",
            "Epoch 472/1000\n",
            "98/98 [==============================] - 16s 158ms/step - loss: 1.2085 - accuracy: 0.5686 - val_loss: 1.4402 - val_accuracy: 0.5106\n",
            "Epoch 473/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2076 - accuracy: 0.5697 - val_loss: 1.4147 - val_accuracy: 0.5204\n",
            "Epoch 474/1000\n",
            "98/98 [==============================] - 16s 158ms/step - loss: 1.2026 - accuracy: 0.5709 - val_loss: 1.4216 - val_accuracy: 0.5173\n",
            "Epoch 475/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2105 - accuracy: 0.5648 - val_loss: 1.4219 - val_accuracy: 0.5193\n",
            "Epoch 476/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2065 - accuracy: 0.5694 - val_loss: 1.4138 - val_accuracy: 0.5222\n",
            "Epoch 477/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2092 - accuracy: 0.5684 - val_loss: 1.4216 - val_accuracy: 0.5207\n",
            "Epoch 478/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2075 - accuracy: 0.5677 - val_loss: 1.4211 - val_accuracy: 0.5166\n",
            "Epoch 479/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2105 - accuracy: 0.5685 - val_loss: 1.4271 - val_accuracy: 0.5142\n",
            "Epoch 480/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2085 - accuracy: 0.5712 - val_loss: 1.4237 - val_accuracy: 0.5188\n",
            "Epoch 481/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2036 - accuracy: 0.5719 - val_loss: 1.4207 - val_accuracy: 0.5164\n",
            "Epoch 482/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2094 - accuracy: 0.5661 - val_loss: 1.4262 - val_accuracy: 0.5192\n",
            "Epoch 483/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2077 - accuracy: 0.5678 - val_loss: 1.4119 - val_accuracy: 0.5210\n",
            "Epoch 484/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2101 - accuracy: 0.5692 - val_loss: 1.4358 - val_accuracy: 0.5161\n",
            "Epoch 485/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2090 - accuracy: 0.5688 - val_loss: 1.4159 - val_accuracy: 0.5193\n",
            "Epoch 486/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2104 - accuracy: 0.5691 - val_loss: 1.4165 - val_accuracy: 0.5192\n",
            "Epoch 487/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2057 - accuracy: 0.5669 - val_loss: 1.4271 - val_accuracy: 0.5140\n",
            "Epoch 488/1000\n",
            "98/98 [==============================] - 16s 158ms/step - loss: 1.2076 - accuracy: 0.5705 - val_loss: 1.4301 - val_accuracy: 0.5160\n",
            "Epoch 489/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2041 - accuracy: 0.5693 - val_loss: 1.4231 - val_accuracy: 0.5220\n",
            "Epoch 490/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2094 - accuracy: 0.5674 - val_loss: 1.4059 - val_accuracy: 0.5241\n",
            "Epoch 491/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2110 - accuracy: 0.5665 - val_loss: 1.4364 - val_accuracy: 0.5149\n",
            "Epoch 492/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2040 - accuracy: 0.5689 - val_loss: 1.4093 - val_accuracy: 0.5252\n",
            "Epoch 493/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2064 - accuracy: 0.5692 - val_loss: 1.4145 - val_accuracy: 0.5199\n",
            "Epoch 494/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2081 - accuracy: 0.5687 - val_loss: 1.4274 - val_accuracy: 0.5133\n",
            "Epoch 495/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2085 - accuracy: 0.5693 - val_loss: 1.4200 - val_accuracy: 0.5183\n",
            "Epoch 496/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2052 - accuracy: 0.5689 - val_loss: 1.4179 - val_accuracy: 0.5214\n",
            "Epoch 497/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2066 - accuracy: 0.5675 - val_loss: 1.4165 - val_accuracy: 0.5199\n",
            "Epoch 498/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2053 - accuracy: 0.5695 - val_loss: 1.4398 - val_accuracy: 0.5144\n",
            "Epoch 499/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2058 - accuracy: 0.5680 - val_loss: 1.4384 - val_accuracy: 0.5120\n",
            "Epoch 500/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2095 - accuracy: 0.5672 - val_loss: 1.4273 - val_accuracy: 0.5181\n",
            "Epoch 501/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2081 - accuracy: 0.5678 - val_loss: 1.4413 - val_accuracy: 0.5134\n",
            "Epoch 502/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2096 - accuracy: 0.5698 - val_loss: 1.4377 - val_accuracy: 0.5143\n",
            "Epoch 503/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2078 - accuracy: 0.5676 - val_loss: 1.4304 - val_accuracy: 0.5177\n",
            "Epoch 504/1000\n",
            "98/98 [==============================] - 16s 158ms/step - loss: 1.2014 - accuracy: 0.5708 - val_loss: 1.4350 - val_accuracy: 0.5148\n",
            "Epoch 505/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2024 - accuracy: 0.5707 - val_loss: 1.4209 - val_accuracy: 0.5209\n",
            "Epoch 506/1000\n",
            "98/98 [==============================] - 16s 158ms/step - loss: 1.2031 - accuracy: 0.5704 - val_loss: 1.4192 - val_accuracy: 0.5258\n",
            "Epoch 507/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2005 - accuracy: 0.5702 - val_loss: 1.4455 - val_accuracy: 0.5128\n",
            "Epoch 508/1000\n",
            "98/98 [==============================] - 15s 158ms/step - loss: 1.2014 - accuracy: 0.5706 - val_loss: 1.4209 - val_accuracy: 0.5205\n",
            "Epoch 509/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2037 - accuracy: 0.5690 - val_loss: 1.4262 - val_accuracy: 0.5159\n",
            "Epoch 510/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2032 - accuracy: 0.5705 - val_loss: 1.4458 - val_accuracy: 0.5059\n",
            "Epoch 511/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2043 - accuracy: 0.5689 - val_loss: 1.4460 - val_accuracy: 0.5109\n",
            "Epoch 512/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2028 - accuracy: 0.5714 - val_loss: 1.4144 - val_accuracy: 0.5224\n",
            "Epoch 513/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.2032 - accuracy: 0.5696 - val_loss: 1.4373 - val_accuracy: 0.5108\n",
            "Epoch 514/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.2038 - accuracy: 0.5710 - val_loss: 1.4165 - val_accuracy: 0.5231\n",
            "Epoch 515/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.2014 - accuracy: 0.5723 - val_loss: 1.4228 - val_accuracy: 0.5198\n",
            "Epoch 516/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.2033 - accuracy: 0.5692 - val_loss: 1.4367 - val_accuracy: 0.5097\n",
            "Epoch 517/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.2026 - accuracy: 0.5700 - val_loss: 1.4396 - val_accuracy: 0.5203\n",
            "Epoch 518/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.2013 - accuracy: 0.5703 - val_loss: 1.4154 - val_accuracy: 0.5201\n",
            "Epoch 519/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2044 - accuracy: 0.5683 - val_loss: 1.4252 - val_accuracy: 0.5142\n",
            "Epoch 520/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2050 - accuracy: 0.5692 - val_loss: 1.4168 - val_accuracy: 0.5218\n",
            "Epoch 521/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2036 - accuracy: 0.5722 - val_loss: 1.4298 - val_accuracy: 0.5107\n",
            "Epoch 522/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2060 - accuracy: 0.5694 - val_loss: 1.4572 - val_accuracy: 0.5004\n",
            "Epoch 523/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2012 - accuracy: 0.5728 - val_loss: 1.4120 - val_accuracy: 0.5266\n",
            "Epoch 524/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2020 - accuracy: 0.5694 - val_loss: 1.4280 - val_accuracy: 0.5153\n",
            "Epoch 525/1000\n",
            "98/98 [==============================] - 16s 158ms/step - loss: 1.2066 - accuracy: 0.5701 - val_loss: 1.4223 - val_accuracy: 0.5187\n",
            "Epoch 526/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2069 - accuracy: 0.5686 - val_loss: 1.4205 - val_accuracy: 0.5194\n",
            "Epoch 527/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2034 - accuracy: 0.5701 - val_loss: 1.4340 - val_accuracy: 0.5180\n",
            "Epoch 528/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2073 - accuracy: 0.5693 - val_loss: 1.4352 - val_accuracy: 0.5162\n",
            "Epoch 529/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2062 - accuracy: 0.5709 - val_loss: 1.4460 - val_accuracy: 0.5116\n",
            "Epoch 530/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2046 - accuracy: 0.5695 - val_loss: 1.4341 - val_accuracy: 0.5174\n",
            "Epoch 531/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2019 - accuracy: 0.5701 - val_loss: 1.4413 - val_accuracy: 0.5165\n",
            "Epoch 532/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2060 - accuracy: 0.5688 - val_loss: 1.4254 - val_accuracy: 0.5227\n",
            "Epoch 533/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2049 - accuracy: 0.5690 - val_loss: 1.4371 - val_accuracy: 0.5182\n",
            "Epoch 534/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2027 - accuracy: 0.5729 - val_loss: 1.4420 - val_accuracy: 0.5189\n",
            "Epoch 535/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2025 - accuracy: 0.5702 - val_loss: 1.4175 - val_accuracy: 0.5244\n",
            "Epoch 536/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.1999 - accuracy: 0.5711 - val_loss: 1.4312 - val_accuracy: 0.5176\n",
            "Epoch 537/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2039 - accuracy: 0.5692 - val_loss: 1.4308 - val_accuracy: 0.5152\n",
            "Epoch 538/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2059 - accuracy: 0.5687 - val_loss: 1.4158 - val_accuracy: 0.5230\n",
            "Epoch 539/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2010 - accuracy: 0.5699 - val_loss: 1.4147 - val_accuracy: 0.5216\n",
            "Epoch 540/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2018 - accuracy: 0.5706 - val_loss: 1.4324 - val_accuracy: 0.5158\n",
            "Epoch 541/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1999 - accuracy: 0.5723 - val_loss: 1.4248 - val_accuracy: 0.5169\n",
            "Epoch 542/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.1985 - accuracy: 0.5709 - val_loss: 1.4169 - val_accuracy: 0.5225\n",
            "Epoch 543/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2038 - accuracy: 0.5666 - val_loss: 1.4247 - val_accuracy: 0.5177\n",
            "Epoch 544/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.1988 - accuracy: 0.5729 - val_loss: 1.4321 - val_accuracy: 0.5150\n",
            "Epoch 545/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1988 - accuracy: 0.5705 - val_loss: 1.4241 - val_accuracy: 0.5198\n",
            "Epoch 546/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.1989 - accuracy: 0.5692 - val_loss: 1.4297 - val_accuracy: 0.5197\n",
            "Epoch 547/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1979 - accuracy: 0.5719 - val_loss: 1.4231 - val_accuracy: 0.5188\n",
            "Epoch 548/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.2018 - accuracy: 0.5710 - val_loss: 1.4218 - val_accuracy: 0.5232\n",
            "Epoch 549/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1997 - accuracy: 0.5709 - val_loss: 1.4326 - val_accuracy: 0.5141\n",
            "Epoch 550/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1998 - accuracy: 0.5727 - val_loss: 1.4247 - val_accuracy: 0.5181\n",
            "Epoch 551/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2046 - accuracy: 0.5707 - val_loss: 1.4351 - val_accuracy: 0.5181\n",
            "Epoch 552/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.2034 - accuracy: 0.5704 - val_loss: 1.4468 - val_accuracy: 0.5143\n",
            "Epoch 553/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.2016 - accuracy: 0.5714 - val_loss: 1.4322 - val_accuracy: 0.5165\n",
            "Epoch 554/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.2004 - accuracy: 0.5695 - val_loss: 1.4399 - val_accuracy: 0.5122\n",
            "Epoch 555/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.2030 - accuracy: 0.5718 - val_loss: 1.4272 - val_accuracy: 0.5178\n",
            "Epoch 556/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.2056 - accuracy: 0.5701 - val_loss: 1.4407 - val_accuracy: 0.5156\n",
            "Epoch 557/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1960 - accuracy: 0.5752 - val_loss: 1.4240 - val_accuracy: 0.5184\n",
            "Epoch 558/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.2004 - accuracy: 0.5692 - val_loss: 1.4403 - val_accuracy: 0.5132\n",
            "Epoch 559/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.2025 - accuracy: 0.5702 - val_loss: 1.4288 - val_accuracy: 0.5245\n",
            "Epoch 560/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1979 - accuracy: 0.5726 - val_loss: 1.4312 - val_accuracy: 0.5174\n",
            "Epoch 561/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.2010 - accuracy: 0.5703 - val_loss: 1.4322 - val_accuracy: 0.5172\n",
            "Epoch 562/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1994 - accuracy: 0.5700 - val_loss: 1.4240 - val_accuracy: 0.5175\n",
            "Epoch 563/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.2028 - accuracy: 0.5724 - val_loss: 1.4500 - val_accuracy: 0.5119\n",
            "Epoch 564/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1974 - accuracy: 0.5712 - val_loss: 1.4268 - val_accuracy: 0.5172\n",
            "Epoch 565/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1965 - accuracy: 0.5734 - val_loss: 1.4262 - val_accuracy: 0.5180\n",
            "Epoch 566/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1989 - accuracy: 0.5699 - val_loss: 1.4206 - val_accuracy: 0.5222\n",
            "Epoch 567/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2032 - accuracy: 0.5714 - val_loss: 1.4221 - val_accuracy: 0.5177\n",
            "Epoch 568/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2045 - accuracy: 0.5681 - val_loss: 1.4239 - val_accuracy: 0.5170\n",
            "Epoch 569/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1983 - accuracy: 0.5728 - val_loss: 1.4249 - val_accuracy: 0.5164\n",
            "Epoch 570/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2027 - accuracy: 0.5699 - val_loss: 1.4395 - val_accuracy: 0.5169\n",
            "Epoch 571/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1971 - accuracy: 0.5721 - val_loss: 1.4196 - val_accuracy: 0.5203\n",
            "Epoch 572/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1974 - accuracy: 0.5726 - val_loss: 1.4335 - val_accuracy: 0.5103\n",
            "Epoch 573/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1985 - accuracy: 0.5737 - val_loss: 1.4371 - val_accuracy: 0.5168\n",
            "Epoch 574/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2026 - accuracy: 0.5709 - val_loss: 1.4452 - val_accuracy: 0.5137\n",
            "Epoch 575/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1937 - accuracy: 0.5765 - val_loss: 1.4249 - val_accuracy: 0.5255\n",
            "Epoch 576/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1959 - accuracy: 0.5725 - val_loss: 1.4356 - val_accuracy: 0.5133\n",
            "Epoch 577/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1986 - accuracy: 0.5703 - val_loss: 1.4441 - val_accuracy: 0.5136\n",
            "Epoch 578/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1931 - accuracy: 0.5741 - val_loss: 1.4434 - val_accuracy: 0.5161\n",
            "Epoch 579/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.2010 - accuracy: 0.5696 - val_loss: 1.4184 - val_accuracy: 0.5210\n",
            "Epoch 580/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.1964 - accuracy: 0.5717 - val_loss: 1.4225 - val_accuracy: 0.5187\n",
            "Epoch 581/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1981 - accuracy: 0.5708 - val_loss: 1.4390 - val_accuracy: 0.5162\n",
            "Epoch 582/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1977 - accuracy: 0.5728 - val_loss: 1.4401 - val_accuracy: 0.5128\n",
            "Epoch 583/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.2015 - accuracy: 0.5699 - val_loss: 1.4226 - val_accuracy: 0.5193\n",
            "Epoch 584/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1973 - accuracy: 0.5729 - val_loss: 1.4309 - val_accuracy: 0.5158\n",
            "Epoch 585/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1936 - accuracy: 0.5699 - val_loss: 1.4229 - val_accuracy: 0.5201\n",
            "Epoch 586/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1911 - accuracy: 0.5725 - val_loss: 1.4429 - val_accuracy: 0.5113\n",
            "Epoch 587/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1965 - accuracy: 0.5725 - val_loss: 1.4620 - val_accuracy: 0.5047\n",
            "Epoch 588/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.2005 - accuracy: 0.5701 - val_loss: 1.4481 - val_accuracy: 0.5132\n",
            "Epoch 589/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.2006 - accuracy: 0.5741 - val_loss: 1.4346 - val_accuracy: 0.5239\n",
            "Epoch 590/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1976 - accuracy: 0.5726 - val_loss: 1.4199 - val_accuracy: 0.5212\n",
            "Epoch 591/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1956 - accuracy: 0.5726 - val_loss: 1.4115 - val_accuracy: 0.5261\n",
            "Epoch 592/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1961 - accuracy: 0.5726 - val_loss: 1.4305 - val_accuracy: 0.5193\n",
            "Epoch 593/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2004 - accuracy: 0.5703 - val_loss: 1.4221 - val_accuracy: 0.5217\n",
            "Epoch 594/1000\n",
            "98/98 [==============================] - 16s 159ms/step - loss: 1.1975 - accuracy: 0.5703 - val_loss: 1.4281 - val_accuracy: 0.5190\n",
            "Epoch 595/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2007 - accuracy: 0.5700 - val_loss: 1.4335 - val_accuracy: 0.5164\n",
            "Epoch 596/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2005 - accuracy: 0.5707 - val_loss: 1.4271 - val_accuracy: 0.5164\n",
            "Epoch 597/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1961 - accuracy: 0.5722 - val_loss: 1.4361 - val_accuracy: 0.5140\n",
            "Epoch 598/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1948 - accuracy: 0.5708 - val_loss: 1.4228 - val_accuracy: 0.5184\n",
            "Epoch 599/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1926 - accuracy: 0.5744 - val_loss: 1.4337 - val_accuracy: 0.5183\n",
            "Epoch 600/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1937 - accuracy: 0.5738 - val_loss: 1.4313 - val_accuracy: 0.5160\n",
            "Epoch 601/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1985 - accuracy: 0.5718 - val_loss: 1.4331 - val_accuracy: 0.5189\n",
            "Epoch 602/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1974 - accuracy: 0.5725 - val_loss: 1.4297 - val_accuracy: 0.5199\n",
            "Epoch 603/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1967 - accuracy: 0.5725 - val_loss: 1.4570 - val_accuracy: 0.5065\n",
            "Epoch 604/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2059 - accuracy: 0.5695 - val_loss: 1.4345 - val_accuracy: 0.5193\n",
            "Epoch 605/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2008 - accuracy: 0.5704 - val_loss: 1.4449 - val_accuracy: 0.5116\n",
            "Epoch 606/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1917 - accuracy: 0.5733 - val_loss: 1.4196 - val_accuracy: 0.5205\n",
            "Epoch 607/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1955 - accuracy: 0.5736 - val_loss: 1.4262 - val_accuracy: 0.5159\n",
            "Epoch 608/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1947 - accuracy: 0.5712 - val_loss: 1.4382 - val_accuracy: 0.5180\n",
            "Epoch 609/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2034 - accuracy: 0.5698 - val_loss: 1.4182 - val_accuracy: 0.5223\n",
            "Epoch 610/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1950 - accuracy: 0.5735 - val_loss: 1.4246 - val_accuracy: 0.5155\n",
            "Epoch 611/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1978 - accuracy: 0.5729 - val_loss: 1.4300 - val_accuracy: 0.5190\n",
            "Epoch 612/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1947 - accuracy: 0.5746 - val_loss: 1.4458 - val_accuracy: 0.5144\n",
            "Epoch 613/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1937 - accuracy: 0.5736 - val_loss: 1.4233 - val_accuracy: 0.5225\n",
            "Epoch 614/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1971 - accuracy: 0.5716 - val_loss: 1.4314 - val_accuracy: 0.5134\n",
            "Epoch 615/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2003 - accuracy: 0.5702 - val_loss: 1.4453 - val_accuracy: 0.5105\n",
            "Epoch 616/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1952 - accuracy: 0.5734 - val_loss: 1.4237 - val_accuracy: 0.5206\n",
            "Epoch 617/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1923 - accuracy: 0.5751 - val_loss: 1.4285 - val_accuracy: 0.5187\n",
            "Epoch 618/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1925 - accuracy: 0.5725 - val_loss: 1.4283 - val_accuracy: 0.5212\n",
            "Epoch 619/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1911 - accuracy: 0.5739 - val_loss: 1.4353 - val_accuracy: 0.5148\n",
            "Epoch 620/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1959 - accuracy: 0.5745 - val_loss: 1.4280 - val_accuracy: 0.5178\n",
            "Epoch 621/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.2001 - accuracy: 0.5720 - val_loss: 1.4508 - val_accuracy: 0.5140\n",
            "Epoch 622/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1948 - accuracy: 0.5702 - val_loss: 1.4405 - val_accuracy: 0.5175\n",
            "Epoch 623/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.2019 - accuracy: 0.5714 - val_loss: 1.4321 - val_accuracy: 0.5142\n",
            "Epoch 624/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1988 - accuracy: 0.5734 - val_loss: 1.4463 - val_accuracy: 0.5189\n",
            "Epoch 625/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1960 - accuracy: 0.5735 - val_loss: 1.4493 - val_accuracy: 0.5103\n",
            "Epoch 626/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1961 - accuracy: 0.5729 - val_loss: 1.4319 - val_accuracy: 0.5151\n",
            "Epoch 627/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1954 - accuracy: 0.5726 - val_loss: 1.4345 - val_accuracy: 0.5170\n",
            "Epoch 628/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1872 - accuracy: 0.5750 - val_loss: 1.4249 - val_accuracy: 0.5215\n",
            "Epoch 629/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1923 - accuracy: 0.5718 - val_loss: 1.4386 - val_accuracy: 0.5141\n",
            "Epoch 630/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1895 - accuracy: 0.5751 - val_loss: 1.4311 - val_accuracy: 0.5169\n",
            "Epoch 631/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1952 - accuracy: 0.5763 - val_loss: 1.4341 - val_accuracy: 0.5193\n",
            "Epoch 632/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1902 - accuracy: 0.5727 - val_loss: 1.4490 - val_accuracy: 0.5131\n",
            "Epoch 633/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1921 - accuracy: 0.5730 - val_loss: 1.4419 - val_accuracy: 0.5122\n",
            "Epoch 634/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1920 - accuracy: 0.5737 - val_loss: 1.4424 - val_accuracy: 0.5152\n",
            "Epoch 635/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1978 - accuracy: 0.5704 - val_loss: 1.4294 - val_accuracy: 0.5193\n",
            "Epoch 636/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1931 - accuracy: 0.5758 - val_loss: 1.4322 - val_accuracy: 0.5180\n",
            "Epoch 637/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1923 - accuracy: 0.5745 - val_loss: 1.4367 - val_accuracy: 0.5162\n",
            "Epoch 638/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1928 - accuracy: 0.5744 - val_loss: 1.4228 - val_accuracy: 0.5223\n",
            "Epoch 639/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1944 - accuracy: 0.5728 - val_loss: 1.4343 - val_accuracy: 0.5184\n",
            "Epoch 640/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1953 - accuracy: 0.5735 - val_loss: 1.4331 - val_accuracy: 0.5211\n",
            "Epoch 641/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1934 - accuracy: 0.5705 - val_loss: 1.4373 - val_accuracy: 0.5156\n",
            "Epoch 642/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1964 - accuracy: 0.5744 - val_loss: 1.4288 - val_accuracy: 0.5244\n",
            "Epoch 643/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1975 - accuracy: 0.5722 - val_loss: 1.4346 - val_accuracy: 0.5180\n",
            "Epoch 644/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1946 - accuracy: 0.5738 - val_loss: 1.4259 - val_accuracy: 0.5205\n",
            "Epoch 645/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1963 - accuracy: 0.5728 - val_loss: 1.4251 - val_accuracy: 0.5212\n",
            "Epoch 646/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1950 - accuracy: 0.5734 - val_loss: 1.4454 - val_accuracy: 0.5144\n",
            "Epoch 647/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1920 - accuracy: 0.5742 - val_loss: 1.4279 - val_accuracy: 0.5168\n",
            "Epoch 648/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1893 - accuracy: 0.5744 - val_loss: 1.4259 - val_accuracy: 0.5193\n",
            "Epoch 649/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1902 - accuracy: 0.5754 - val_loss: 1.4623 - val_accuracy: 0.5076\n",
            "Epoch 650/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1925 - accuracy: 0.5756 - val_loss: 1.4431 - val_accuracy: 0.5075\n",
            "Epoch 651/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1969 - accuracy: 0.5696 - val_loss: 1.4296 - val_accuracy: 0.5222\n",
            "Epoch 652/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1948 - accuracy: 0.5724 - val_loss: 1.4285 - val_accuracy: 0.5189\n",
            "Epoch 653/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1876 - accuracy: 0.5751 - val_loss: 1.4391 - val_accuracy: 0.5168\n",
            "Epoch 654/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1952 - accuracy: 0.5718 - val_loss: 1.4243 - val_accuracy: 0.5215\n",
            "Epoch 655/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1905 - accuracy: 0.5760 - val_loss: 1.4412 - val_accuracy: 0.5200\n",
            "Epoch 656/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1912 - accuracy: 0.5746 - val_loss: 1.4290 - val_accuracy: 0.5217\n",
            "Epoch 657/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1906 - accuracy: 0.5747 - val_loss: 1.4433 - val_accuracy: 0.5135\n",
            "Epoch 658/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1938 - accuracy: 0.5739 - val_loss: 1.4403 - val_accuracy: 0.5165\n",
            "Epoch 659/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1965 - accuracy: 0.5733 - val_loss: 1.4358 - val_accuracy: 0.5206\n",
            "Epoch 660/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1953 - accuracy: 0.5756 - val_loss: 1.4381 - val_accuracy: 0.5207\n",
            "Epoch 661/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1887 - accuracy: 0.5761 - val_loss: 1.4294 - val_accuracy: 0.5185\n",
            "Epoch 662/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1913 - accuracy: 0.5743 - val_loss: 1.4282 - val_accuracy: 0.5213\n",
            "Epoch 663/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1917 - accuracy: 0.5728 - val_loss: 1.4411 - val_accuracy: 0.5150\n",
            "Epoch 664/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1942 - accuracy: 0.5741 - val_loss: 1.4248 - val_accuracy: 0.5237\n",
            "Epoch 665/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1915 - accuracy: 0.5775 - val_loss: 1.4300 - val_accuracy: 0.5221\n",
            "Epoch 666/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1892 - accuracy: 0.5735 - val_loss: 1.4446 - val_accuracy: 0.5133\n",
            "Epoch 667/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1954 - accuracy: 0.5731 - val_loss: 1.4372 - val_accuracy: 0.5190\n",
            "Epoch 668/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1862 - accuracy: 0.5749 - val_loss: 1.4337 - val_accuracy: 0.5193\n",
            "Epoch 669/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1918 - accuracy: 0.5745 - val_loss: 1.4344 - val_accuracy: 0.5219\n",
            "Epoch 670/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1910 - accuracy: 0.5753 - val_loss: 1.4313 - val_accuracy: 0.5166\n",
            "Epoch 671/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1892 - accuracy: 0.5783 - val_loss: 1.4238 - val_accuracy: 0.5239\n",
            "Epoch 672/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1898 - accuracy: 0.5759 - val_loss: 1.4364 - val_accuracy: 0.5182\n",
            "Epoch 673/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1894 - accuracy: 0.5749 - val_loss: 1.4308 - val_accuracy: 0.5194\n",
            "Epoch 674/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1900 - accuracy: 0.5742 - val_loss: 1.4255 - val_accuracy: 0.5226\n",
            "Epoch 675/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1960 - accuracy: 0.5732 - val_loss: 1.4458 - val_accuracy: 0.5184\n",
            "Epoch 676/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1890 - accuracy: 0.5756 - val_loss: 1.4450 - val_accuracy: 0.5154\n",
            "Epoch 677/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1941 - accuracy: 0.5737 - val_loss: 1.4305 - val_accuracy: 0.5150\n",
            "Epoch 678/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1942 - accuracy: 0.5746 - val_loss: 1.4304 - val_accuracy: 0.5166\n",
            "Epoch 679/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1933 - accuracy: 0.5755 - val_loss: 1.4179 - val_accuracy: 0.5229\n",
            "Epoch 680/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1903 - accuracy: 0.5730 - val_loss: 1.4471 - val_accuracy: 0.5093\n",
            "Epoch 681/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1882 - accuracy: 0.5761 - val_loss: 1.4454 - val_accuracy: 0.5132\n",
            "Epoch 682/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1933 - accuracy: 0.5721 - val_loss: 1.4432 - val_accuracy: 0.5164\n",
            "Epoch 683/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1925 - accuracy: 0.5729 - val_loss: 1.4455 - val_accuracy: 0.5103\n",
            "Epoch 684/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1908 - accuracy: 0.5741 - val_loss: 1.4593 - val_accuracy: 0.5100\n",
            "Epoch 685/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1960 - accuracy: 0.5723 - val_loss: 1.4486 - val_accuracy: 0.5132\n",
            "Epoch 686/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1868 - accuracy: 0.5745 - val_loss: 1.4480 - val_accuracy: 0.5176\n",
            "Epoch 687/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1895 - accuracy: 0.5761 - val_loss: 1.4307 - val_accuracy: 0.5234\n",
            "Epoch 688/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1873 - accuracy: 0.5765 - val_loss: 1.4177 - val_accuracy: 0.5191\n",
            "Epoch 689/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1977 - accuracy: 0.5712 - val_loss: 1.4545 - val_accuracy: 0.5160\n",
            "Epoch 690/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1991 - accuracy: 0.5692 - val_loss: 1.4345 - val_accuracy: 0.5192\n",
            "Epoch 691/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1889 - accuracy: 0.5748 - val_loss: 1.4266 - val_accuracy: 0.5225\n",
            "Epoch 692/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1945 - accuracy: 0.5735 - val_loss: 1.4273 - val_accuracy: 0.5187\n",
            "Epoch 693/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1960 - accuracy: 0.5701 - val_loss: 1.4308 - val_accuracy: 0.5165\n",
            "Epoch 694/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1831 - accuracy: 0.5768 - val_loss: 1.4316 - val_accuracy: 0.5165\n",
            "Epoch 695/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1885 - accuracy: 0.5756 - val_loss: 1.4225 - val_accuracy: 0.5187\n",
            "Epoch 696/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1882 - accuracy: 0.5749 - val_loss: 1.4271 - val_accuracy: 0.5186\n",
            "Epoch 697/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1884 - accuracy: 0.5779 - val_loss: 1.4316 - val_accuracy: 0.5227\n",
            "Epoch 698/1000\n",
            "98/98 [==============================] - 16s 160ms/step - loss: 1.1916 - accuracy: 0.5745 - val_loss: 1.4379 - val_accuracy: 0.5164\n",
            "Epoch 699/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1862 - accuracy: 0.5746 - val_loss: 1.4399 - val_accuracy: 0.5117\n",
            "Epoch 700/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1933 - accuracy: 0.5721 - val_loss: 1.4383 - val_accuracy: 0.5180\n",
            "Epoch 701/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1913 - accuracy: 0.5732 - val_loss: 1.4289 - val_accuracy: 0.5187\n",
            "Epoch 702/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1890 - accuracy: 0.5751 - val_loss: 1.4350 - val_accuracy: 0.5132\n",
            "Epoch 703/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1883 - accuracy: 0.5739 - val_loss: 1.4256 - val_accuracy: 0.5233\n",
            "Epoch 704/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1962 - accuracy: 0.5723 - val_loss: 1.4553 - val_accuracy: 0.5089\n",
            "Epoch 705/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1917 - accuracy: 0.5741 - val_loss: 1.4270 - val_accuracy: 0.5218\n",
            "Epoch 706/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1924 - accuracy: 0.5734 - val_loss: 1.4292 - val_accuracy: 0.5170\n",
            "Epoch 707/1000\n",
            "98/98 [==============================] - 17s 178ms/step - loss: 1.1869 - accuracy: 0.5748 - val_loss: 1.4308 - val_accuracy: 0.5165\n",
            "Epoch 708/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1878 - accuracy: 0.5763 - val_loss: 1.4257 - val_accuracy: 0.5227\n",
            "Epoch 709/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1898 - accuracy: 0.5746 - val_loss: 1.4260 - val_accuracy: 0.5225\n",
            "Epoch 710/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1869 - accuracy: 0.5762 - val_loss: 1.4352 - val_accuracy: 0.5146\n",
            "Epoch 711/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1912 - accuracy: 0.5751 - val_loss: 1.4475 - val_accuracy: 0.5137\n",
            "Epoch 712/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1875 - accuracy: 0.5749 - val_loss: 1.4326 - val_accuracy: 0.5205\n",
            "Epoch 713/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1904 - accuracy: 0.5743 - val_loss: 1.4316 - val_accuracy: 0.5205\n",
            "Epoch 714/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1957 - accuracy: 0.5736 - val_loss: 1.4263 - val_accuracy: 0.5230\n",
            "Epoch 715/1000\n",
            "98/98 [==============================] - 16s 161ms/step - loss: 1.1883 - accuracy: 0.5784 - val_loss: 1.4274 - val_accuracy: 0.5205\n",
            "Epoch 716/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1908 - accuracy: 0.5748 - val_loss: 1.4273 - val_accuracy: 0.5200\n",
            "Epoch 717/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1860 - accuracy: 0.5749 - val_loss: 1.4456 - val_accuracy: 0.5091\n",
            "Epoch 718/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1919 - accuracy: 0.5722 - val_loss: 1.4363 - val_accuracy: 0.5197\n",
            "Epoch 719/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1919 - accuracy: 0.5739 - val_loss: 1.4284 - val_accuracy: 0.5210\n",
            "Epoch 720/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1864 - accuracy: 0.5769 - val_loss: 1.4401 - val_accuracy: 0.5165\n",
            "Epoch 721/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1884 - accuracy: 0.5737 - val_loss: 1.4268 - val_accuracy: 0.5216\n",
            "Epoch 722/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1944 - accuracy: 0.5747 - val_loss: 1.4450 - val_accuracy: 0.5169\n",
            "Epoch 723/1000\n",
            "98/98 [==============================] - 16s 167ms/step - loss: 1.1886 - accuracy: 0.5766 - val_loss: 1.4214 - val_accuracy: 0.5202\n",
            "Epoch 724/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1886 - accuracy: 0.5762 - val_loss: 1.4494 - val_accuracy: 0.5125\n",
            "Epoch 725/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1924 - accuracy: 0.5738 - val_loss: 1.4376 - val_accuracy: 0.5167\n",
            "Epoch 726/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1924 - accuracy: 0.5750 - val_loss: 1.4318 - val_accuracy: 0.5187\n",
            "Epoch 727/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1877 - accuracy: 0.5767 - val_loss: 1.4630 - val_accuracy: 0.5061\n",
            "Epoch 728/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1946 - accuracy: 0.5737 - val_loss: 1.4316 - val_accuracy: 0.5187\n",
            "Epoch 729/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1937 - accuracy: 0.5752 - val_loss: 1.4225 - val_accuracy: 0.5231\n",
            "Epoch 730/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1860 - accuracy: 0.5740 - val_loss: 1.4343 - val_accuracy: 0.5195\n",
            "Epoch 731/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1864 - accuracy: 0.5723 - val_loss: 1.4454 - val_accuracy: 0.5121\n",
            "Epoch 732/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1918 - accuracy: 0.5751 - val_loss: 1.4409 - val_accuracy: 0.5130\n",
            "Epoch 733/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1893 - accuracy: 0.5744 - val_loss: 1.4395 - val_accuracy: 0.5169\n",
            "Epoch 734/1000\n",
            "98/98 [==============================] - 16s 167ms/step - loss: 1.1922 - accuracy: 0.5758 - val_loss: 1.4202 - val_accuracy: 0.5233\n",
            "Epoch 735/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1895 - accuracy: 0.5740 - val_loss: 1.4505 - val_accuracy: 0.5160\n",
            "Epoch 736/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1892 - accuracy: 0.5764 - val_loss: 1.4381 - val_accuracy: 0.5192\n",
            "Epoch 737/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1877 - accuracy: 0.5769 - val_loss: 1.4466 - val_accuracy: 0.5183\n",
            "Epoch 738/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1827 - accuracy: 0.5765 - val_loss: 1.4743 - val_accuracy: 0.5062\n",
            "Epoch 739/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1860 - accuracy: 0.5755 - val_loss: 1.4449 - val_accuracy: 0.5146\n",
            "Epoch 740/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1913 - accuracy: 0.5750 - val_loss: 1.4451 - val_accuracy: 0.5104\n",
            "Epoch 741/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1829 - accuracy: 0.5773 - val_loss: 1.4381 - val_accuracy: 0.5159\n",
            "Epoch 742/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1850 - accuracy: 0.5754 - val_loss: 1.4306 - val_accuracy: 0.5217\n",
            "Epoch 743/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1856 - accuracy: 0.5774 - val_loss: 1.4277 - val_accuracy: 0.5228\n",
            "Epoch 744/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1893 - accuracy: 0.5756 - val_loss: 1.4357 - val_accuracy: 0.5214\n",
            "Epoch 745/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1898 - accuracy: 0.5754 - val_loss: 1.4397 - val_accuracy: 0.5182\n",
            "Epoch 746/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1790 - accuracy: 0.5799 - val_loss: 1.4456 - val_accuracy: 0.5170\n",
            "Epoch 747/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1868 - accuracy: 0.5773 - val_loss: 1.4210 - val_accuracy: 0.5204\n",
            "Epoch 748/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1918 - accuracy: 0.5762 - val_loss: 1.4287 - val_accuracy: 0.5176\n",
            "Epoch 749/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1830 - accuracy: 0.5784 - val_loss: 1.4550 - val_accuracy: 0.5102\n",
            "Epoch 750/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1883 - accuracy: 0.5776 - val_loss: 1.4604 - val_accuracy: 0.5104\n",
            "Epoch 751/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1855 - accuracy: 0.5749 - val_loss: 1.4490 - val_accuracy: 0.5143\n",
            "Epoch 752/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1913 - accuracy: 0.5742 - val_loss: 1.4450 - val_accuracy: 0.5182\n",
            "Epoch 753/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1870 - accuracy: 0.5767 - val_loss: 1.4467 - val_accuracy: 0.5202\n",
            "Epoch 754/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1832 - accuracy: 0.5785 - val_loss: 1.4364 - val_accuracy: 0.5199\n",
            "Epoch 755/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1891 - accuracy: 0.5754 - val_loss: 1.4351 - val_accuracy: 0.5129\n",
            "Epoch 756/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1836 - accuracy: 0.5752 - val_loss: 1.4428 - val_accuracy: 0.5155\n",
            "Epoch 757/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1908 - accuracy: 0.5749 - val_loss: 1.4365 - val_accuracy: 0.5189\n",
            "Epoch 758/1000\n",
            "98/98 [==============================] - 17s 170ms/step - loss: 1.1953 - accuracy: 0.5714 - val_loss: 1.4500 - val_accuracy: 0.5143\n",
            "Epoch 759/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1888 - accuracy: 0.5726 - val_loss: 1.4410 - val_accuracy: 0.5186\n",
            "Epoch 760/1000\n",
            "98/98 [==============================] - 17s 169ms/step - loss: 1.1877 - accuracy: 0.5748 - val_loss: 1.4464 - val_accuracy: 0.5159\n",
            "Epoch 761/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1842 - accuracy: 0.5774 - val_loss: 1.4263 - val_accuracy: 0.5237\n",
            "Epoch 762/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1942 - accuracy: 0.5709 - val_loss: 1.4340 - val_accuracy: 0.5182\n",
            "Epoch 763/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1869 - accuracy: 0.5743 - val_loss: 1.4331 - val_accuracy: 0.5167\n",
            "Epoch 764/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1879 - accuracy: 0.5774 - val_loss: 1.4398 - val_accuracy: 0.5153\n",
            "Epoch 765/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1847 - accuracy: 0.5773 - val_loss: 1.4421 - val_accuracy: 0.5166\n",
            "Epoch 766/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1850 - accuracy: 0.5768 - val_loss: 1.4375 - val_accuracy: 0.5162\n",
            "Epoch 767/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1861 - accuracy: 0.5774 - val_loss: 1.4383 - val_accuracy: 0.5170\n",
            "Epoch 768/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1898 - accuracy: 0.5756 - val_loss: 1.4161 - val_accuracy: 0.5248\n",
            "Epoch 769/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1881 - accuracy: 0.5737 - val_loss: 1.4373 - val_accuracy: 0.5166\n",
            "Epoch 770/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1910 - accuracy: 0.5749 - val_loss: 1.4445 - val_accuracy: 0.5150\n",
            "Epoch 771/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1859 - accuracy: 0.5758 - val_loss: 1.4296 - val_accuracy: 0.5230\n",
            "Epoch 772/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1848 - accuracy: 0.5777 - val_loss: 1.4436 - val_accuracy: 0.5127\n",
            "Epoch 773/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1880 - accuracy: 0.5747 - val_loss: 1.4358 - val_accuracy: 0.5136\n",
            "Epoch 774/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1880 - accuracy: 0.5738 - val_loss: 1.4347 - val_accuracy: 0.5163\n",
            "Epoch 775/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1882 - accuracy: 0.5754 - val_loss: 1.4252 - val_accuracy: 0.5179\n",
            "Epoch 776/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1863 - accuracy: 0.5760 - val_loss: 1.4437 - val_accuracy: 0.5196\n",
            "Epoch 777/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1896 - accuracy: 0.5757 - val_loss: 1.4371 - val_accuracy: 0.5145\n",
            "Epoch 778/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1893 - accuracy: 0.5754 - val_loss: 1.4349 - val_accuracy: 0.5152\n",
            "Epoch 779/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1908 - accuracy: 0.5744 - val_loss: 1.4404 - val_accuracy: 0.5160\n",
            "Epoch 780/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1888 - accuracy: 0.5762 - val_loss: 1.4278 - val_accuracy: 0.5160\n",
            "Epoch 781/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1858 - accuracy: 0.5749 - val_loss: 1.4349 - val_accuracy: 0.5217\n",
            "Epoch 782/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1889 - accuracy: 0.5748 - val_loss: 1.4331 - val_accuracy: 0.5174\n",
            "Epoch 783/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1871 - accuracy: 0.5763 - val_loss: 1.4385 - val_accuracy: 0.5159\n",
            "Epoch 784/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1892 - accuracy: 0.5722 - val_loss: 1.4245 - val_accuracy: 0.5205\n",
            "Epoch 785/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1818 - accuracy: 0.5779 - val_loss: 1.4511 - val_accuracy: 0.5128\n",
            "Epoch 786/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1857 - accuracy: 0.5769 - val_loss: 1.4347 - val_accuracy: 0.5219\n",
            "Epoch 787/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1877 - accuracy: 0.5755 - val_loss: 1.4293 - val_accuracy: 0.5196\n",
            "Epoch 788/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1860 - accuracy: 0.5781 - val_loss: 1.4512 - val_accuracy: 0.5137\n",
            "Epoch 789/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1856 - accuracy: 0.5767 - val_loss: 1.4247 - val_accuracy: 0.5231\n",
            "Epoch 790/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1840 - accuracy: 0.5757 - val_loss: 1.4360 - val_accuracy: 0.5195\n",
            "Epoch 791/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1819 - accuracy: 0.5771 - val_loss: 1.4371 - val_accuracy: 0.5163\n",
            "Epoch 792/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1849 - accuracy: 0.5760 - val_loss: 1.4482 - val_accuracy: 0.5131\n",
            "Epoch 793/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1842 - accuracy: 0.5762 - val_loss: 1.4414 - val_accuracy: 0.5206\n",
            "Epoch 794/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1829 - accuracy: 0.5788 - val_loss: 1.4287 - val_accuracy: 0.5220\n",
            "Epoch 795/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1905 - accuracy: 0.5755 - val_loss: 1.4484 - val_accuracy: 0.5167\n",
            "Epoch 796/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1882 - accuracy: 0.5744 - val_loss: 1.4523 - val_accuracy: 0.5140\n",
            "Epoch 797/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1842 - accuracy: 0.5776 - val_loss: 1.4265 - val_accuracy: 0.5197\n",
            "Epoch 798/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1858 - accuracy: 0.5766 - val_loss: 1.4477 - val_accuracy: 0.5146\n",
            "Epoch 799/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1830 - accuracy: 0.5793 - val_loss: 1.4418 - val_accuracy: 0.5173\n",
            "Epoch 800/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1858 - accuracy: 0.5752 - val_loss: 1.4389 - val_accuracy: 0.5170\n",
            "Epoch 801/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1845 - accuracy: 0.5767 - val_loss: 1.4381 - val_accuracy: 0.5176\n",
            "Epoch 802/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1851 - accuracy: 0.5759 - val_loss: 1.4303 - val_accuracy: 0.5223\n",
            "Epoch 803/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1821 - accuracy: 0.5783 - val_loss: 1.4236 - val_accuracy: 0.5225\n",
            "Epoch 804/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1906 - accuracy: 0.5738 - val_loss: 1.4420 - val_accuracy: 0.5231\n",
            "Epoch 805/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1820 - accuracy: 0.5769 - val_loss: 1.4388 - val_accuracy: 0.5165\n",
            "Epoch 806/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1824 - accuracy: 0.5776 - val_loss: 1.4305 - val_accuracy: 0.5181\n",
            "Epoch 807/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1804 - accuracy: 0.5783 - val_loss: 1.4394 - val_accuracy: 0.5189\n",
            "Epoch 808/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1808 - accuracy: 0.5779 - val_loss: 1.4409 - val_accuracy: 0.5177\n",
            "Epoch 809/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1861 - accuracy: 0.5740 - val_loss: 1.4298 - val_accuracy: 0.5215\n",
            "Epoch 810/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1840 - accuracy: 0.5754 - val_loss: 1.4427 - val_accuracy: 0.5104\n",
            "Epoch 811/1000\n",
            "98/98 [==============================] - 16s 162ms/step - loss: 1.1840 - accuracy: 0.5782 - val_loss: 1.4371 - val_accuracy: 0.5155\n",
            "Epoch 812/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1902 - accuracy: 0.5742 - val_loss: 1.4436 - val_accuracy: 0.5134\n",
            "Epoch 813/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1859 - accuracy: 0.5746 - val_loss: 1.4392 - val_accuracy: 0.5147\n",
            "Epoch 814/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1868 - accuracy: 0.5756 - val_loss: 1.4496 - val_accuracy: 0.5113\n",
            "Epoch 815/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1908 - accuracy: 0.5762 - val_loss: 1.4483 - val_accuracy: 0.5165\n",
            "Epoch 816/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1834 - accuracy: 0.5782 - val_loss: 1.4227 - val_accuracy: 0.5258\n",
            "Epoch 817/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1879 - accuracy: 0.5758 - val_loss: 1.4407 - val_accuracy: 0.5170\n",
            "Epoch 818/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1857 - accuracy: 0.5770 - val_loss: 1.4382 - val_accuracy: 0.5132\n",
            "Epoch 819/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1850 - accuracy: 0.5762 - val_loss: 1.4410 - val_accuracy: 0.5172\n",
            "Epoch 820/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1936 - accuracy: 0.5719 - val_loss: 1.4243 - val_accuracy: 0.5251\n",
            "Epoch 821/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1803 - accuracy: 0.5801 - val_loss: 1.4436 - val_accuracy: 0.5165\n",
            "Epoch 822/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1865 - accuracy: 0.5754 - val_loss: 1.4405 - val_accuracy: 0.5156\n",
            "Epoch 823/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1814 - accuracy: 0.5778 - val_loss: 1.4504 - val_accuracy: 0.5116\n",
            "Epoch 824/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1814 - accuracy: 0.5765 - val_loss: 1.4363 - val_accuracy: 0.5192\n",
            "Epoch 825/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1809 - accuracy: 0.5784 - val_loss: 1.4213 - val_accuracy: 0.5218\n",
            "Epoch 826/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1833 - accuracy: 0.5796 - val_loss: 1.4476 - val_accuracy: 0.5143\n",
            "Epoch 827/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1862 - accuracy: 0.5763 - val_loss: 1.4453 - val_accuracy: 0.5175\n",
            "Epoch 828/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1832 - accuracy: 0.5783 - val_loss: 1.4350 - val_accuracy: 0.5229\n",
            "Epoch 829/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1863 - accuracy: 0.5764 - val_loss: 1.4401 - val_accuracy: 0.5155\n",
            "Epoch 830/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1861 - accuracy: 0.5735 - val_loss: 1.4588 - val_accuracy: 0.5128\n",
            "Epoch 831/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1862 - accuracy: 0.5745 - val_loss: 1.4341 - val_accuracy: 0.5163\n",
            "Epoch 832/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1851 - accuracy: 0.5757 - val_loss: 1.4313 - val_accuracy: 0.5248\n",
            "Epoch 833/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1842 - accuracy: 0.5767 - val_loss: 1.4473 - val_accuracy: 0.5142\n",
            "Epoch 834/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1826 - accuracy: 0.5766 - val_loss: 1.4315 - val_accuracy: 0.5179\n",
            "Epoch 835/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1861 - accuracy: 0.5753 - val_loss: 1.4256 - val_accuracy: 0.5220\n",
            "Epoch 836/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1826 - accuracy: 0.5770 - val_loss: 1.4569 - val_accuracy: 0.5112\n",
            "Epoch 837/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1840 - accuracy: 0.5802 - val_loss: 1.4333 - val_accuracy: 0.5225\n",
            "Epoch 838/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1799 - accuracy: 0.5801 - val_loss: 1.4273 - val_accuracy: 0.5201\n",
            "Epoch 839/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1820 - accuracy: 0.5783 - val_loss: 1.4433 - val_accuracy: 0.5190\n",
            "Epoch 840/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1838 - accuracy: 0.5751 - val_loss: 1.4367 - val_accuracy: 0.5164\n",
            "Epoch 841/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1822 - accuracy: 0.5799 - val_loss: 1.4393 - val_accuracy: 0.5202\n",
            "Epoch 842/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1818 - accuracy: 0.5778 - val_loss: 1.4453 - val_accuracy: 0.5201\n",
            "Epoch 843/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1841 - accuracy: 0.5783 - val_loss: 1.4677 - val_accuracy: 0.5078\n",
            "Epoch 844/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1847 - accuracy: 0.5748 - val_loss: 1.4345 - val_accuracy: 0.5199\n",
            "Epoch 845/1000\n",
            "98/98 [==============================] - 16s 168ms/step - loss: 1.1817 - accuracy: 0.5755 - val_loss: 1.4444 - val_accuracy: 0.5147\n",
            "Epoch 846/1000\n",
            "98/98 [==============================] - 16s 168ms/step - loss: 1.1850 - accuracy: 0.5755 - val_loss: 1.4302 - val_accuracy: 0.5250\n",
            "Epoch 847/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1754 - accuracy: 0.5809 - val_loss: 1.4420 - val_accuracy: 0.5169\n",
            "Epoch 848/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1816 - accuracy: 0.5781 - val_loss: 1.4350 - val_accuracy: 0.5178\n",
            "Epoch 849/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1853 - accuracy: 0.5758 - val_loss: 1.4211 - val_accuracy: 0.5255\n",
            "Epoch 850/1000\n",
            "98/98 [==============================] - 16s 167ms/step - loss: 1.1815 - accuracy: 0.5771 - val_loss: 1.4272 - val_accuracy: 0.5237\n",
            "Epoch 851/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1830 - accuracy: 0.5751 - val_loss: 1.4390 - val_accuracy: 0.5161\n",
            "Epoch 852/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1799 - accuracy: 0.5782 - val_loss: 1.4364 - val_accuracy: 0.5179\n",
            "Epoch 853/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1764 - accuracy: 0.5793 - val_loss: 1.4363 - val_accuracy: 0.5211\n",
            "Epoch 854/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1809 - accuracy: 0.5796 - val_loss: 1.4446 - val_accuracy: 0.5200\n",
            "Epoch 855/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1843 - accuracy: 0.5768 - val_loss: 1.4317 - val_accuracy: 0.5205\n",
            "Epoch 856/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1832 - accuracy: 0.5794 - val_loss: 1.4445 - val_accuracy: 0.5155\n",
            "Epoch 857/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1860 - accuracy: 0.5751 - val_loss: 1.4393 - val_accuracy: 0.5205\n",
            "Epoch 858/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1835 - accuracy: 0.5778 - val_loss: 1.4294 - val_accuracy: 0.5193\n",
            "Epoch 859/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1773 - accuracy: 0.5788 - val_loss: 1.4468 - val_accuracy: 0.5141\n",
            "Epoch 860/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1839 - accuracy: 0.5772 - val_loss: 1.4361 - val_accuracy: 0.5166\n",
            "Epoch 861/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1828 - accuracy: 0.5766 - val_loss: 1.4253 - val_accuracy: 0.5234\n",
            "Epoch 862/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1818 - accuracy: 0.5792 - val_loss: 1.4434 - val_accuracy: 0.5165\n",
            "Epoch 863/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1769 - accuracy: 0.5798 - val_loss: 1.4435 - val_accuracy: 0.5209\n",
            "Epoch 864/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1794 - accuracy: 0.5785 - val_loss: 1.4342 - val_accuracy: 0.5215\n",
            "Epoch 865/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1827 - accuracy: 0.5758 - val_loss: 1.4595 - val_accuracy: 0.5130\n",
            "Epoch 866/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1785 - accuracy: 0.5786 - val_loss: 1.4480 - val_accuracy: 0.5149\n",
            "Epoch 867/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1813 - accuracy: 0.5780 - val_loss: 1.4383 - val_accuracy: 0.5195\n",
            "Epoch 868/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1828 - accuracy: 0.5741 - val_loss: 1.4440 - val_accuracy: 0.5194\n",
            "Epoch 869/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1786 - accuracy: 0.5765 - val_loss: 1.4410 - val_accuracy: 0.5213\n",
            "Epoch 870/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1792 - accuracy: 0.5774 - val_loss: 1.4287 - val_accuracy: 0.5168\n",
            "Epoch 871/1000\n",
            "98/98 [==============================] - 16s 167ms/step - loss: 1.1854 - accuracy: 0.5773 - val_loss: 1.4364 - val_accuracy: 0.5217\n",
            "Epoch 872/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1824 - accuracy: 0.5763 - val_loss: 1.4429 - val_accuracy: 0.5159\n",
            "Epoch 873/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1780 - accuracy: 0.5793 - val_loss: 1.4496 - val_accuracy: 0.5143\n",
            "Epoch 874/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1783 - accuracy: 0.5777 - val_loss: 1.4429 - val_accuracy: 0.5107\n",
            "Epoch 875/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1815 - accuracy: 0.5774 - val_loss: 1.4435 - val_accuracy: 0.5157\n",
            "Epoch 876/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1803 - accuracy: 0.5782 - val_loss: 1.4357 - val_accuracy: 0.5183\n",
            "Epoch 877/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1863 - accuracy: 0.5764 - val_loss: 1.4189 - val_accuracy: 0.5206\n",
            "Epoch 878/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1774 - accuracy: 0.5827 - val_loss: 1.4515 - val_accuracy: 0.5125\n",
            "Epoch 879/1000\n",
            "98/98 [==============================] - 16s 163ms/step - loss: 1.1831 - accuracy: 0.5785 - val_loss: 1.4287 - val_accuracy: 0.5211\n",
            "Epoch 880/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1792 - accuracy: 0.5803 - val_loss: 1.4254 - val_accuracy: 0.5275\n",
            "Epoch 881/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1790 - accuracy: 0.5808 - val_loss: 1.4305 - val_accuracy: 0.5167\n",
            "Epoch 882/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1809 - accuracy: 0.5759 - val_loss: 1.4381 - val_accuracy: 0.5148\n",
            "Epoch 883/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1854 - accuracy: 0.5744 - val_loss: 1.4288 - val_accuracy: 0.5203\n",
            "Epoch 884/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1852 - accuracy: 0.5747 - val_loss: 1.4348 - val_accuracy: 0.5192\n",
            "Epoch 885/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1786 - accuracy: 0.5775 - val_loss: 1.4394 - val_accuracy: 0.5163\n",
            "Epoch 886/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1820 - accuracy: 0.5774 - val_loss: 1.4197 - val_accuracy: 0.5246\n",
            "Epoch 887/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1776 - accuracy: 0.5780 - val_loss: 1.4523 - val_accuracy: 0.5100\n",
            "Epoch 888/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1839 - accuracy: 0.5783 - val_loss: 1.4239 - val_accuracy: 0.5222\n",
            "Epoch 889/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1819 - accuracy: 0.5752 - val_loss: 1.4394 - val_accuracy: 0.5181\n",
            "Epoch 890/1000\n",
            "98/98 [==============================] - 16s 167ms/step - loss: 1.1806 - accuracy: 0.5773 - val_loss: 1.4295 - val_accuracy: 0.5238\n",
            "Epoch 891/1000\n",
            "98/98 [==============================] - 16s 167ms/step - loss: 1.1754 - accuracy: 0.5783 - val_loss: 1.4518 - val_accuracy: 0.5174\n",
            "Epoch 892/1000\n",
            "98/98 [==============================] - 16s 167ms/step - loss: 1.1815 - accuracy: 0.5777 - val_loss: 1.4379 - val_accuracy: 0.5182\n",
            "Epoch 893/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1812 - accuracy: 0.5756 - val_loss: 1.4315 - val_accuracy: 0.5189\n",
            "Epoch 894/1000\n",
            "98/98 [==============================] - 16s 167ms/step - loss: 1.1794 - accuracy: 0.5778 - val_loss: 1.4439 - val_accuracy: 0.5146\n",
            "Epoch 895/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1839 - accuracy: 0.5774 - val_loss: 1.4265 - val_accuracy: 0.5246\n",
            "Epoch 896/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1758 - accuracy: 0.5808 - val_loss: 1.4400 - val_accuracy: 0.5214\n",
            "Epoch 897/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1827 - accuracy: 0.5764 - val_loss: 1.4491 - val_accuracy: 0.5133\n",
            "Epoch 898/1000\n",
            "98/98 [==============================] - 16s 168ms/step - loss: 1.1820 - accuracy: 0.5773 - val_loss: 1.4351 - val_accuracy: 0.5242\n",
            "Epoch 899/1000\n",
            "98/98 [==============================] - 16s 168ms/step - loss: 1.1839 - accuracy: 0.5766 - val_loss: 1.4486 - val_accuracy: 0.5151\n",
            "Epoch 900/1000\n",
            "98/98 [==============================] - 16s 168ms/step - loss: 1.1780 - accuracy: 0.5776 - val_loss: 1.4196 - val_accuracy: 0.5242\n",
            "Epoch 901/1000\n",
            "98/98 [==============================] - 17s 169ms/step - loss: 1.1806 - accuracy: 0.5760 - val_loss: 1.4344 - val_accuracy: 0.5209\n",
            "Epoch 902/1000\n",
            "98/98 [==============================] - 16s 167ms/step - loss: 1.1786 - accuracy: 0.5793 - val_loss: 1.4514 - val_accuracy: 0.5149\n",
            "Epoch 903/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1792 - accuracy: 0.5793 - val_loss: 1.4522 - val_accuracy: 0.5137\n",
            "Epoch 904/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1872 - accuracy: 0.5751 - val_loss: 1.4438 - val_accuracy: 0.5145\n",
            "Epoch 905/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1823 - accuracy: 0.5784 - val_loss: 1.4355 - val_accuracy: 0.5160\n",
            "Epoch 906/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1807 - accuracy: 0.5771 - val_loss: 1.4391 - val_accuracy: 0.5155\n",
            "Epoch 907/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1741 - accuracy: 0.5811 - val_loss: 1.4299 - val_accuracy: 0.5212\n",
            "Epoch 908/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1780 - accuracy: 0.5778 - val_loss: 1.4395 - val_accuracy: 0.5208\n",
            "Epoch 909/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1746 - accuracy: 0.5802 - val_loss: 1.4438 - val_accuracy: 0.5148\n",
            "Epoch 910/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1833 - accuracy: 0.5772 - val_loss: 1.4282 - val_accuracy: 0.5214\n",
            "Epoch 911/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1847 - accuracy: 0.5732 - val_loss: 1.4349 - val_accuracy: 0.5208\n",
            "Epoch 912/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1811 - accuracy: 0.5752 - val_loss: 1.4653 - val_accuracy: 0.5111\n",
            "Epoch 913/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1769 - accuracy: 0.5815 - val_loss: 1.4469 - val_accuracy: 0.5143\n",
            "Epoch 914/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1809 - accuracy: 0.5795 - val_loss: 1.4688 - val_accuracy: 0.5090\n",
            "Epoch 915/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1803 - accuracy: 0.5773 - val_loss: 1.4344 - val_accuracy: 0.5180\n",
            "Epoch 916/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1800 - accuracy: 0.5789 - val_loss: 1.4392 - val_accuracy: 0.5184\n",
            "Epoch 917/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1831 - accuracy: 0.5747 - val_loss: 1.4466 - val_accuracy: 0.5158\n",
            "Epoch 918/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1800 - accuracy: 0.5800 - val_loss: 1.4381 - val_accuracy: 0.5231\n",
            "Epoch 919/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1801 - accuracy: 0.5780 - val_loss: 1.4461 - val_accuracy: 0.5190\n",
            "Epoch 920/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1809 - accuracy: 0.5780 - val_loss: 1.4262 - val_accuracy: 0.5238\n",
            "Epoch 921/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1804 - accuracy: 0.5783 - val_loss: 1.4449 - val_accuracy: 0.5152\n",
            "Epoch 922/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1763 - accuracy: 0.5783 - val_loss: 1.4278 - val_accuracy: 0.5171\n",
            "Epoch 923/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1777 - accuracy: 0.5761 - val_loss: 1.4394 - val_accuracy: 0.5210\n",
            "Epoch 924/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1772 - accuracy: 0.5796 - val_loss: 1.4363 - val_accuracy: 0.5225\n",
            "Epoch 925/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1850 - accuracy: 0.5768 - val_loss: 1.4402 - val_accuracy: 0.5190\n",
            "Epoch 926/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1824 - accuracy: 0.5776 - val_loss: 1.4452 - val_accuracy: 0.5140\n",
            "Epoch 927/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1805 - accuracy: 0.5794 - val_loss: 1.4414 - val_accuracy: 0.5213\n",
            "Epoch 928/1000\n",
            "98/98 [==============================] - 16s 168ms/step - loss: 1.1736 - accuracy: 0.5806 - val_loss: 1.4429 - val_accuracy: 0.5171\n",
            "Epoch 929/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1750 - accuracy: 0.5791 - val_loss: 1.4331 - val_accuracy: 0.5262\n",
            "Epoch 930/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1827 - accuracy: 0.5776 - val_loss: 1.4507 - val_accuracy: 0.5159\n",
            "Epoch 931/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1836 - accuracy: 0.5787 - val_loss: 1.4481 - val_accuracy: 0.5170\n",
            "Epoch 932/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1816 - accuracy: 0.5783 - val_loss: 1.4340 - val_accuracy: 0.5201\n",
            "Epoch 933/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1751 - accuracy: 0.5784 - val_loss: 1.4287 - val_accuracy: 0.5218\n",
            "Epoch 934/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1817 - accuracy: 0.5799 - val_loss: 1.4385 - val_accuracy: 0.5179\n",
            "Epoch 935/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1790 - accuracy: 0.5772 - val_loss: 1.4334 - val_accuracy: 0.5166\n",
            "Epoch 936/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1816 - accuracy: 0.5804 - val_loss: 1.4304 - val_accuracy: 0.5213\n",
            "Epoch 937/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1818 - accuracy: 0.5762 - val_loss: 1.4446 - val_accuracy: 0.5163\n",
            "Epoch 938/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1801 - accuracy: 0.5789 - val_loss: 1.4240 - val_accuracy: 0.5232\n",
            "Epoch 939/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1771 - accuracy: 0.5778 - val_loss: 1.4435 - val_accuracy: 0.5192\n",
            "Epoch 940/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1771 - accuracy: 0.5779 - val_loss: 1.4349 - val_accuracy: 0.5225\n",
            "Epoch 941/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1774 - accuracy: 0.5801 - val_loss: 1.4348 - val_accuracy: 0.5226\n",
            "Epoch 942/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1794 - accuracy: 0.5786 - val_loss: 1.4399 - val_accuracy: 0.5209\n",
            "Epoch 943/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1780 - accuracy: 0.5779 - val_loss: 1.4308 - val_accuracy: 0.5221\n",
            "Epoch 944/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1775 - accuracy: 0.5806 - val_loss: 1.4377 - val_accuracy: 0.5196\n",
            "Epoch 945/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1762 - accuracy: 0.5812 - val_loss: 1.4308 - val_accuracy: 0.5224\n",
            "Epoch 946/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1793 - accuracy: 0.5779 - val_loss: 1.4453 - val_accuracy: 0.5197\n",
            "Epoch 947/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1785 - accuracy: 0.5799 - val_loss: 1.4412 - val_accuracy: 0.5176\n",
            "Epoch 948/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1789 - accuracy: 0.5783 - val_loss: 1.4332 - val_accuracy: 0.5173\n",
            "Epoch 949/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1833 - accuracy: 0.5758 - val_loss: 1.4247 - val_accuracy: 0.5252\n",
            "Epoch 950/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1819 - accuracy: 0.5760 - val_loss: 1.4236 - val_accuracy: 0.5247\n",
            "Epoch 951/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1729 - accuracy: 0.5825 - val_loss: 1.4325 - val_accuracy: 0.5189\n",
            "Epoch 952/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1818 - accuracy: 0.5786 - val_loss: 1.4298 - val_accuracy: 0.5199\n",
            "Epoch 953/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1875 - accuracy: 0.5743 - val_loss: 1.4243 - val_accuracy: 0.5200\n",
            "Epoch 954/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1790 - accuracy: 0.5792 - val_loss: 1.4387 - val_accuracy: 0.5187\n",
            "Epoch 955/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1803 - accuracy: 0.5789 - val_loss: 1.4553 - val_accuracy: 0.5135\n",
            "Epoch 956/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1783 - accuracy: 0.5785 - val_loss: 1.4261 - val_accuracy: 0.5227\n",
            "Epoch 957/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1760 - accuracy: 0.5799 - val_loss: 1.4441 - val_accuracy: 0.5167\n",
            "Epoch 958/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1859 - accuracy: 0.5745 - val_loss: 1.4607 - val_accuracy: 0.5100\n",
            "Epoch 959/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1818 - accuracy: 0.5773 - val_loss: 1.4432 - val_accuracy: 0.5221\n",
            "Epoch 960/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1756 - accuracy: 0.5785 - val_loss: 1.4183 - val_accuracy: 0.5278\n",
            "Epoch 961/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1805 - accuracy: 0.5776 - val_loss: 1.4377 - val_accuracy: 0.5193\n",
            "Epoch 962/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1775 - accuracy: 0.5790 - val_loss: 1.4630 - val_accuracy: 0.5145\n",
            "Epoch 963/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1758 - accuracy: 0.5775 - val_loss: 1.4467 - val_accuracy: 0.5132\n",
            "Epoch 964/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1792 - accuracy: 0.5784 - val_loss: 1.4314 - val_accuracy: 0.5188\n",
            "Epoch 965/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1743 - accuracy: 0.5811 - val_loss: 1.4457 - val_accuracy: 0.5164\n",
            "Epoch 966/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1734 - accuracy: 0.5810 - val_loss: 1.4453 - val_accuracy: 0.5158\n",
            "Epoch 967/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1709 - accuracy: 0.5787 - val_loss: 1.4374 - val_accuracy: 0.5204\n",
            "Epoch 968/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1753 - accuracy: 0.5800 - val_loss: 1.4476 - val_accuracy: 0.5164\n",
            "Epoch 969/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1778 - accuracy: 0.5793 - val_loss: 1.4410 - val_accuracy: 0.5161\n",
            "Epoch 970/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1782 - accuracy: 0.5809 - val_loss: 1.4568 - val_accuracy: 0.5154\n",
            "Epoch 971/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1792 - accuracy: 0.5796 - val_loss: 1.4621 - val_accuracy: 0.5090\n",
            "Epoch 972/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1745 - accuracy: 0.5795 - val_loss: 1.4270 - val_accuracy: 0.5239\n",
            "Epoch 973/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1750 - accuracy: 0.5807 - val_loss: 1.4593 - val_accuracy: 0.5100\n",
            "Epoch 974/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1794 - accuracy: 0.5786 - val_loss: 1.4405 - val_accuracy: 0.5187\n",
            "Epoch 975/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1748 - accuracy: 0.5788 - val_loss: 1.4266 - val_accuracy: 0.5234\n",
            "Epoch 976/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1776 - accuracy: 0.5765 - val_loss: 1.4311 - val_accuracy: 0.5178\n",
            "Epoch 977/1000\n",
            "98/98 [==============================] - 16s 164ms/step - loss: 1.1787 - accuracy: 0.5770 - val_loss: 1.4552 - val_accuracy: 0.5114\n",
            "Epoch 978/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1738 - accuracy: 0.5790 - val_loss: 1.4366 - val_accuracy: 0.5245\n",
            "Epoch 979/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1838 - accuracy: 0.5767 - val_loss: 1.4239 - val_accuracy: 0.5215\n",
            "Epoch 980/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1765 - accuracy: 0.5786 - val_loss: 1.4369 - val_accuracy: 0.5200\n",
            "Epoch 981/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1772 - accuracy: 0.5817 - val_loss: 1.4419 - val_accuracy: 0.5176\n",
            "Epoch 982/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1778 - accuracy: 0.5811 - val_loss: 1.4281 - val_accuracy: 0.5243\n",
            "Epoch 983/1000\n",
            "98/98 [==============================] - 16s 167ms/step - loss: 1.1796 - accuracy: 0.5800 - val_loss: 1.4466 - val_accuracy: 0.5128\n",
            "Epoch 984/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1782 - accuracy: 0.5794 - val_loss: 1.4490 - val_accuracy: 0.5144\n",
            "Epoch 985/1000\n",
            "98/98 [==============================] - 16s 167ms/step - loss: 1.1797 - accuracy: 0.5769 - val_loss: 1.4440 - val_accuracy: 0.5158\n",
            "Epoch 986/1000\n",
            "98/98 [==============================] - 16s 168ms/step - loss: 1.1781 - accuracy: 0.5760 - val_loss: 1.4461 - val_accuracy: 0.5173\n",
            "Epoch 987/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1779 - accuracy: 0.5802 - val_loss: 1.4521 - val_accuracy: 0.5138\n",
            "Epoch 988/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1721 - accuracy: 0.5801 - val_loss: 1.4538 - val_accuracy: 0.5139\n",
            "Epoch 989/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1819 - accuracy: 0.5773 - val_loss: 1.4352 - val_accuracy: 0.5172\n",
            "Epoch 990/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1762 - accuracy: 0.5779 - val_loss: 1.4435 - val_accuracy: 0.5159\n",
            "Epoch 991/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1740 - accuracy: 0.5816 - val_loss: 1.4381 - val_accuracy: 0.5199\n",
            "Epoch 992/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1760 - accuracy: 0.5799 - val_loss: 1.4450 - val_accuracy: 0.5111\n",
            "Epoch 993/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1751 - accuracy: 0.5801 - val_loss: 1.4390 - val_accuracy: 0.5225\n",
            "Epoch 994/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1732 - accuracy: 0.5799 - val_loss: 1.4406 - val_accuracy: 0.5198\n",
            "Epoch 995/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1757 - accuracy: 0.5800 - val_loss: 1.4338 - val_accuracy: 0.5212\n",
            "Epoch 996/1000\n",
            "98/98 [==============================] - 16s 165ms/step - loss: 1.1734 - accuracy: 0.5821 - val_loss: 1.4364 - val_accuracy: 0.5176\n",
            "Epoch 997/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1753 - accuracy: 0.5816 - val_loss: 1.4503 - val_accuracy: 0.5140\n",
            "Epoch 998/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1771 - accuracy: 0.5797 - val_loss: 1.4519 - val_accuracy: 0.5121\n",
            "Epoch 999/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1740 - accuracy: 0.5796 - val_loss: 1.4331 - val_accuracy: 0.5211\n",
            "Epoch 1000/1000\n",
            "98/98 [==============================] - 16s 166ms/step - loss: 1.1763 - accuracy: 0.5787 - val_loss: 1.4431 - val_accuracy: 0.5155\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 1.4431 - accuracy: 0.5155\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f65c69d5c90>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STtoCwa1IsQz"
      },
      "source": [
        ""
      ],
      "id": "STtoCwa1IsQz",
      "execution_count": null,
      "outputs": []
    }
  ]
}
